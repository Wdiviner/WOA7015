{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet101\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import re\n",
        "import torchvision.models as models\n",
        "import random\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2FRA7QD5j1Kp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "DXeVw_2mkd3W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPrzIVYEjq1_"
      },
      "outputs": [],
      "source": [
        "# Pre-processing steps for the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def has_less_than_30_images(folder_path):\n",
        "    # Supported image file extensions\n",
        "    image_extensions = {'.jpg'}\n",
        "\n",
        "    # Count the number of image files\n",
        "    image_count = 0\n",
        "    for file in os.listdir(folder_path):\n",
        "        if any(file.endswith(ext) for ext in image_extensions):\n",
        "            image_count += 1\n",
        "\n",
        "            # If 30 or more images are found, return False\n",
        "            if image_count >= 30:\n",
        "                return False\n",
        "    # If fewer than 30 images, return True\n",
        "    return True\n",
        "\n",
        "# Extract the number from the file name\n",
        "def extract_numbers_from_filename(filename):\n",
        "    return [int(num) for num in re.findall(r'\\d+', filename)]\n",
        "\n",
        "\n",
        "def extract_frame(data_dir):\n",
        "    samples = []\n",
        "    # Loop over the folders in the dataset\n",
        "    folders = sorted(os.listdir(data_dir), key=extract_numbers_from_filename)\n",
        "\n",
        "    for image_folder in folders:\n",
        "\n",
        "        print(\"image_folder : \", image_folder)\n",
        "\n",
        "        if \"hand_flapping\" in image_folder:\n",
        "            label = 0 # hand flapping\n",
        "        elif \"arm_flapping\" in image_folder:\n",
        "            label = 1 # arm flapping\n",
        "\n",
        "        image_folder_path = os.path.join(data_dir, image_folder)\n",
        "        # print(\"image_folder_path : \", image_folder_path)\n",
        "\n",
        "        frames = []\n",
        "        for image_file in sorted(os.listdir(image_folder_path), key=extract_numbers_from_filename):\n",
        "            if has_less_than_30_images(image_folder_path):\n",
        "                continue\n",
        "\n",
        "            if image_file.endswith(('.png', '.jpg', '.jpeg')):  # check for image files\n",
        "                image_path = os.path.join(image_folder_path, image_file)\n",
        "                image = cv2.imread(image_path)\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                image = transform(image)\n",
        "\n",
        "                frames.append(image)\n",
        "\n",
        "        # Stack the frames into a tensor of shape (num_frames, 3, 112, 112)\n",
        "        frames_tensor = torch.stack(frames, dim=0)\n",
        "        samples.append((frames_tensor, label))\n",
        "\n",
        "    np.random.shuffle(samples)\n",
        "    return samples\n",
        "\n",
        "# Define the file path to your dataset\n",
        "train_data_dir = '/kaggle/input/7015-dataset/train/train'\n",
        "test_data_dir = '/kaggle/input/7015-dataset/test/test'\n",
        "\n",
        "train_samples = extract_frame(train_data_dir)\n",
        "test_samples = extract_frame(test_data_dir)\n",
        "\n",
        "train_features, train_labels = zip(*train_samples)\n",
        "test_features, test_labels = zip(*test_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "Db-xzA-Ukiva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# random seed\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "# Custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "class CNNLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes, timesteps):\n",
        "        super(CNNLSTM, self).__init__()\n",
        "        self.resnet = resnet101(pretrained=True)\n",
        "        self.resnet.fc = nn.Sequential(nn.Linear(self.resnet.fc.in_features, input_size))\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_size, num_classes)  # fully connection\n",
        "\n",
        "    def forward(self, x_3d):\n",
        "        batch_size, timesteps, channels, height, width = x_3d.size()\n",
        "        c_in = x_3d.view(batch_size * timesteps, channels, height, width)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            c_out = self.resnet(c_in)\n",
        "\n",
        "        r_in = c_out.view(batch_size, timesteps, -1)\n",
        "\n",
        "        hidden = None\n",
        "        out, hidden = self.lstm(r_in, hidden)\n",
        "\n",
        "        x = self.fc1(out[:, -1, :])\n",
        "        return x\n",
        "\n",
        "# parameter\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "input_size = 64\n",
        "hidden_size = 32\n",
        "num_layers = 2\n",
        "num_classes = 2\n",
        "timesteps = 30\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CNNLSTM(input_size, hidden_size, num_layers, num_classes,timesteps).to(device)\n",
        "\n",
        "# Define criterion and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Create dataset instances\n",
        "train_dataset = CustomDataset(train_features, train_labels)\n",
        "test_dataset = CustomDataset(test_features, test_labels)\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# train\n",
        "num_epochs = 30\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "test_predicted_probs = []\n",
        "\n",
        "# early_stopping\n",
        "early_stopping_patience = 10\n",
        "early_stopping_counter = 0\n",
        "best_test_loss = float('inf')\n",
        "\n",
        "# List used to store performance metric values\n",
        "accuracy_values = []\n",
        "f1_score_values = []\n",
        "auc_values = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    test_predicted_probs = []\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_total = 0\n",
        "    train_correct = 0\n",
        "    for batch_features, batch_labels in train_loader:\n",
        "        batch_features = batch_features.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_features)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        train_total += batch_labels.size(0)\n",
        "        train_correct += (predicted == batch_labels).sum().item()\n",
        "    train_loss /= len(train_loader)\n",
        "    train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "    # Testing loop\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    test_true_labels = []\n",
        "    test_predicted_labels = []\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    with torch.no_grad():\n",
        "        for batch_features, batch_labels in test_loader:\n",
        "            batch_features = batch_features.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            test_total += batch_labels.size(0)\n",
        "            test_correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "            test_true_labels.extend(batch_labels.cpu().numpy())\n",
        "            test_predicted_labels.extend(predicted.cpu().numpy())\n",
        "            test_predicted_probs.extend(outputs.cpu().numpy())\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100 * test_correct / test_total\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    # Convert the list to a numpy array for roc_auc_score\n",
        "    test_predicted_probs = np.array(test_predicted_probs)\n",
        "\n",
        "    # Compute AUC for each class\n",
        "    auc_scores = []\n",
        "    for class_idx in range(test_predicted_probs.shape[1]):\n",
        "        auc = roc_auc_score(np.array(test_true_labels) == class_idx, test_predicted_probs[:, class_idx])\n",
        "        auc_scores.append(auc)\n",
        "\n",
        "    avg_auc = np.mean(auc_scores)\n",
        "\n",
        "\n",
        "    accuracy_values.append(test_accuracy / 100)\n",
        "    f1_score_values.append(f1_score(test_true_labels, test_predicted_labels, average='weighted'))\n",
        "    auc_values.append(avg_auc)\n",
        "\n",
        "\n",
        "    # Print testing loss and accuracy\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "     # Early stopping check\n",
        "    if test_loss < best_test_loss:\n",
        "        best_test_loss = test_loss\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    # Check if early stopping criteria met\n",
        "    if early_stopping_counter >= early_stopping_patience:\n",
        "        print(\"Early stopping.\")\n",
        "        break\n",
        "\n",
        "# calculate accuracy、recall、F1score\n",
        "test_accuracy = accuracy_score(test_true_labels, test_predicted_labels)\n",
        "test_accuracy = test_accuracy*100\n",
        "test_recall = recall_score(test_true_labels, test_predicted_labels, average='weighted')\n",
        "test_f1 = f1_score(test_true_labels, test_predicted_labels, average='weighted')\n",
        "\n",
        "accuracy_variance = np.var(accuracy_values)\n",
        "f1_score_variance = np.var(f1_score_values)\n",
        "auc_variance = np.var(auc_values)\n",
        "\n",
        "print(f'Final Test Accuracy: {test_accuracy:.2f}%, Test Recall: {test_recall:.2f}, Test F1 Score: {test_f1:.2f}, Average AUC: {avg_auc:.4f}')\n",
        "print(\"Accuracy Variance:\", accuracy_variance)\n",
        "print(\"F1-score Variance:\", f1_score_variance)\n",
        "print(\"AUC Variance:\", auc_variance)\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training and testing losses\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.title('Training and Testing Loss per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training and testing accuracies\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(test_accuracies, label='Test Accuracy')\n",
        "plt.title('Training and Testing Accuracy per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ebctXPD4kkzn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}