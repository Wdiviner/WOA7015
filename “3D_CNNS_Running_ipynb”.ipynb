{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ie3yOUZSEsEI",
        "outputId": "76ffeb3a-79c5-4d0f-9681-ee322313c06c"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    127\u001b[0m   )\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxjrhZ0bMuyL"
      },
      "source": [
        "# **dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tw65jjqaCK4a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import classification_report, roc_curve, auc\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ASD_Isolated(Dataset):\n",
        "    def __init__(self, data_path, num_classes=2, transform=None):\n",
        "        super(ASD_Isolated, self).__init__()\n",
        "        self.data_path = data_path\n",
        "        self.num_classes = num_classes\n",
        "        self.transform = transform\n",
        "        # self.frames = 30  # 如果不同子文件夹内图像数量不同，请移除此属性\n",
        "        self.classes = sorted(os.listdir(data_path))  # 这里假设data_path直接是包含类别的路径\n",
        "        assert len(self.classes) == num_classes, f\"类别数量({len(self.classes)})与num_classes参数({num_classes})不匹配\"\n",
        "        self.data_info = self._get_data_info()\n",
        "\n",
        "    def _get_data_info(self):\n",
        "      data_info = []\n",
        "      for category in self.classes:\n",
        "          category_path = os.path.join(self.data_path, category)\n",
        "          # print(\"category_path\", category_path)\n",
        "          for instance_folder in sorted(os.listdir(category_path)):\n",
        "              instance_path = os.path.join(category_path, instance_folder)\n",
        "              # print(\"instance_path\", instance_path)\n",
        "              if os.path.isdir(instance_path):\n",
        "                  # 遍历实例文件夹中的子文件夹\n",
        "                  #sub_folders = [f for f in sorted(os.listdir(instance_path)) if os.path.isdir(os.path.join(instance_path, f))]\n",
        "                  # for sub_folder in sub_folders:\n",
        "                  #     print(\"instance_path：\", instance_path)\n",
        "                  #     sub_folder_path = os.path.join(instance_path, sub_folder)\n",
        "                  #     image_files = [img for img in sorted(os.listdir(sub_folder_path)) if img.endswith('.jpg')]\n",
        "                  #     # 现在不检查图像数量，直接添加信息\n",
        "                  #     data_info.append((sub_folder_path, category))\n",
        "                  image_files = [img for img in sorted(os.listdir(instance_path)) if img.endswith('.jpg')]\n",
        "\n",
        "                  # if len(image_files) != 30:\n",
        "                  #   print(\"instance_path:\", instance_path)\n",
        "                  # 现在不检查图像数量，直接添加信息\n",
        "                  data_info.append((instance_path, category))\n",
        "      # print(f\"Loaded {len(data_info)} samples.\")  # 调试打印语句\n",
        "      return data_info  # 注意这里，现在return语句在循环之外\n",
        "\n",
        "\n",
        "    def read_images(self, folder_path):\n",
        "        image_files = sorted([os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.jpg')])\n",
        "        images = [Image.open(file).convert('RGB') for file in image_files]\n",
        "        if self.transform is not None:\n",
        "            images = [self.transform(image) for image in images]\n",
        "        images = torch.stack(images, dim=0)\n",
        "        images = images.permute(1, 0, 2, 3)  # Adjust for the expected input dimensions of the CNN\n",
        "        return images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        folder_path, label = self.data_info[idx]\n",
        "        images = self.read_images(folder_path)\n",
        "        label_index = self.classes.index(label)\n",
        "        label_tensor = torch.tensor(label_index, dtype=torch.long)\n",
        "        return {'data': images, 'label': label_tensor}\n",
        "\n",
        "# 测试代码\n",
        "if __name__ == '__main__':\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize([128, 128]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # 您需要替换这里的路径为您数据集的实际路径\n",
        "    dataset_path = \"/content/drive/MyDrive/output_frames/test\"\n",
        "\n",
        "    dataset = ASD_Isolated(data_path=dataset_path, num_classes=2, transform=transform)\n",
        "    print(f\"Dataset size: {len(dataset)}\")\n",
        "    if len(dataset) > 0:  # 确保数据集不为空\n",
        "        sample = dataset[0]\n",
        "        print(f\"Sample image shape: {sample['data'].shape}, Label: {sample['label']}\")\n",
        "    else:\n",
        "        print(\"Dataset is empty!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbrHOQsoMeJW"
      },
      "source": [
        "# **train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vv932TY3A7XS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_epoch(model, criterion, optimizer, dataloader, device, epoch, logger, log_interval, writer):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    all_label = []\n",
        "    all_pred = []\n",
        "\n",
        "    for batch_idx, data in enumerate(dataloader):\n",
        "        # 获取输入和标签\n",
        "        inputs, labels = data['data'].to(device), data['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # 前向传播\n",
        "        outputs = model(inputs)\n",
        "        if isinstance(outputs, list):\n",
        "            outputs = outputs[0]\n",
        "\n",
        "        # 计算损失\n",
        "        loss = criterion(outputs, labels.squeeze())\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # 计算准确率\n",
        "        prediction = torch.max(outputs, 1)[1]\n",
        "        all_label.extend(labels.squeeze())\n",
        "        all_pred.extend(prediction)\n",
        "        score = accuracy_score(labels.squeeze().cpu().data.squeeze().numpy(), prediction.cpu().data.squeeze().numpy())\n",
        "\n",
        "        # 反向传播 & 优化\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # if (batch_idx + 1) % log_interval == 0:\n",
        "        #     # logger.info(\"epoch {:3d} | iteration {:5d} | Loss {:.6f} | Acc {:.2f}%\".format(epoch+1, batch_idx+1, loss.item(), score*100))\n",
        "\n",
        "    # 计算平均损失和准确率\n",
        "    training_loss = sum(losses)/len(losses)\n",
        "    all_label = torch.stack(all_label, dim=0)\n",
        "    all_pred = torch.stack(all_pred, dim=0)\n",
        "    training_acc = accuracy_score(all_label.squeeze().cpu().data.squeeze().numpy(), all_pred.cpu().data.squeeze().numpy())\n",
        "\n",
        "    print('Loss', {'train': training_loss}, epoch+1)\n",
        "    print('Accuracy', {'train': training_acc}, epoch+1)\n",
        "    print(\"第 {} 轮平均训练损失: {:.6f} | 准确率: {:.2f}%\".format(epoch+1, training_loss, training_acc*100))\n",
        "\n",
        "    return training_loss, training_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPCKwNCvNA3s"
      },
      "source": [
        "# **tool**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7s4Fqh9I-Zhy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision.utils as utils\n",
        "import cv2\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def get_label_and_pred(model, dataloader, device):\n",
        "    all_label = []\n",
        "    all_pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(test_loader):\n",
        "            # get the inputs and labels\n",
        "            inputs, labels = data['data'].to(device), data['label'].to(device)\n",
        "            # forward\n",
        "            outputs = model(inputs)\n",
        "            if isinstance(outputs, list):\n",
        "                outputs = outputs[0]\n",
        "            # collect labels & prediction\n",
        "            prediction = torch.max(outputs, 1)[1]\n",
        "            all_label.extend(labels.squeeze())\n",
        "            all_pred.extend(prediction)\n",
        "    # Compute accuracy\n",
        "    all_label = torch.stack(all_label, dim=0)\n",
        "    all_pred = torch.stack(all_pred, dim=0)\n",
        "    all_label = all_label.squeeze().cpu().data.squeeze().numpy()\n",
        "    all_pred = all_pred.cpu().data.squeeze().numpy()\n",
        "    return all_label, all_pred\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(model, dataloader, device, save_path='confmat.png', normalize=True):\n",
        "    # Get prediction\n",
        "    all_label, all_pred = get_label_and_pred(model, dataloader, device)\n",
        "    confmat = confusion_matrix(all_label, all_pred)\n",
        "\n",
        "    # Normalize the matrix\n",
        "    if normalize:\n",
        "        confmat = confmat.astype('float') / confmat.sum(axis=1)[:, np.newaxis]\n",
        "    # Draw matrix\n",
        "    plt.figure(figsize=(20,20))\n",
        "    # confmat = np.random.rand(100,100)\n",
        "    plt.imshow(confmat, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.colorbar()\n",
        "    # Add ticks\n",
        "    ticks = np.arange(100)\n",
        "    plt.xticks(ticks, fontsize=8)\n",
        "    plt.yticks(ticks, fontsize=8)\n",
        "    plt.grid(True)\n",
        "    # Add title & labels\n",
        "    plt.title('Confusion matrix', fontsize=20)\n",
        "    plt.xlabel('Predicted label', fontsize=20)\n",
        "    plt.ylabel('True label', fontsize=20)\n",
        "    # Save figure\n",
        "    plt.savefig(save_path)\n",
        "\n",
        "    # Ranking\n",
        "    sorted_index = np.diag(confmat).argsort()\n",
        "    for i in range(10):\n",
        "        # print(type(sorted_index[i]))\n",
        "        print(test_set.label_to_word(int(sorted_index[i])), confmat[sorted_index[i]][sorted_index[i]])\n",
        "    # Save to csv\n",
        "    np.savetxt('matrix.csv', confmat, delimiter=',')\n",
        "\n",
        "\n",
        "def visualize_attn(I, c):\n",
        "    # Image\n",
        "    img = I.permute((1,2,0)).cpu().numpy()\n",
        "    # Heatmap\n",
        "    N, C, H, W = c.size()\n",
        "    a = F.softmax(c.view(N,C,-1), dim=2).view(N,C,H,W)\n",
        "    up_factor = 128/H\n",
        "    # print(up_factor, I.size(), c.size())\n",
        "    if up_factor > 1:\n",
        "        a = F.interpolate(a, scale_factor=up_factor, mode='bilinear', align_corners=False)\n",
        "    attn = utils.make_grid(a, nrow=4, normalize=True, scale_each=True)\n",
        "    attn = attn.permute((1,2,0)).mul(255).byte().cpu().numpy()\n",
        "    attn = cv2.applyColorMap(attn, cv2.COLORMAP_JET)\n",
        "    attn = cv2.cvtColor(attn, cv2.COLOR_BGR2RGB)\n",
        "    # Add the heatmap to the image\n",
        "    vis = 0.6 * img + 0.4 * attn\n",
        "    return torch.from_numpy(vis).permute(2,0,1)\n",
        "\n",
        "\n",
        "def plot_attention_map(model, dataloader, device):\n",
        "    # Summary writer\n",
        "    writer = SummaryWriter(\"runs/attention_{:%Y-%m-%d_%H-%M-%S}\".format(datetime.now()))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(dataloader):\n",
        "            # get images\n",
        "            inputs = data['data'].to(device)\n",
        "            if batch_idx == 0:\n",
        "                images = inputs[0:16,:,:,:,:]\n",
        "                I = utils.make_grid(images[:,:,0,:,:], nrow=4, normalize=True, scale_each=True)\n",
        "                writer.add_image('origin', I)\n",
        "                _, c1, c2, c3, c4 = model(images)\n",
        "                # print(I.shape, c1.shape, c2.shape, c3.shape, c4.shape)\n",
        "                attn1 = visualize_attn(I, c1[:,:,0,:,:])\n",
        "                writer.add_image('attn1', attn1)\n",
        "                attn2 = visualize_attn(I, c2[:,:,0,:,:])\n",
        "                writer.add_image('attn2', attn2)\n",
        "                attn3 = visualize_attn(I, c3[:,:,0,:,:])\n",
        "                writer.add_image('attn3', attn3)\n",
        "                attn4 = visualize_attn(I, c4[:,:,0,:,:])\n",
        "                writer.add_image('attn4', attn4)\n",
        "                break\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Calculate Word Error Rate\n",
        "Word Error Rate = (Substitutions + Insertions + Deletions) / Number of Words Spoken\n",
        "Reference:\n",
        "https://holianh.github.io/portfolio/Cach-tinh-WER/\n",
        "https://github.com/imalic3/python-word-error-rate\n",
        "\"\"\"\n",
        "def wer(r, h):\n",
        "    # initialisation\n",
        "    d = np.zeros((len(r)+1)*(len(h)+1), dtype=np.uint8)\n",
        "    d = d.reshape((len(r)+1, len(h)+1))\n",
        "    for i in range(len(r)+1):\n",
        "        for j in range(len(h)+1):\n",
        "            if i == 0:\n",
        "                d[0][j] = j\n",
        "            elif j == 0:\n",
        "                d[i][0] = i\n",
        "\n",
        "    # computation\n",
        "    for i in range(1, len(r)+1):\n",
        "        for j in range(1, len(h)+1):\n",
        "            if r[i-1] == h[j-1]:\n",
        "                d[i][j] = d[i-1][j-1]\n",
        "            else:\n",
        "                substitution = d[i-1][j-1] + 1\n",
        "                insertion = d[i][j-1] + 1\n",
        "                deletion = d[i-1][j] + 1\n",
        "                d[i][j] = min(substitution, insertion, deletion)\n",
        "\n",
        "    return float(d[len(r)][len(h)]) / len(r) * 100\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Calculate WER\n",
        "    r = [1,2,3,4]\n",
        "    h = [1,1,3,5,6]\n",
        "    print(wer(r, h))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSu2jA0INbkE"
      },
      "source": [
        "# **validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IUjm0jvkCHMG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
        "\n",
        "def val_epoch(model, criterion, dataloader, device, epoch, logger, writer):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(dataloader):\n",
        "            inputs = data['data'].to(device)\n",
        "            labels = data['label'].to(device)\n",
        "\n",
        "            if not isinstance(inputs, torch.Tensor) or not isinstance(labels, torch.Tensor):\n",
        "                print(f'Error: Data or labels are not tensors at batch index {batch_idx}.')\n",
        "                continue\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            if isinstance(outputs, tuple) or isinstance(outputs, list):\n",
        "                outputs = outputs[0]\n",
        "\n",
        "            if outputs.size(0) != labels.size(0):\n",
        "                print(f'Error: Mismatch between output size {outputs.size(0)} and labels size {labels.size(0)} at batch index {batch_idx}.')\n",
        "                continue\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_labels.append(labels)\n",
        "            all_preds.append(predicted)\n",
        "            all_probs.append(outputs)\n",
        "\n",
        "    validation_loss = sum(losses) / len(losses)\n",
        "    all_labels = torch.cat(all_labels).cpu()\n",
        "    all_preds = torch.cat(all_preds).cpu()\n",
        "    all_probs = torch.cat(all_probs).cpu()\n",
        "\n",
        "    # 计算准确率\n",
        "    validation_acc = accuracy_score(all_labels.numpy(), all_preds.numpy())\n",
        "\n",
        "    # 计算召回率、精确率和 F1 分数\n",
        "    recall = recall_score(all_labels.numpy(), all_preds.numpy())\n",
        "    precision = precision_score(all_labels.numpy(), all_preds.numpy())\n",
        "    f1 = f1_score(all_labels.numpy(), all_preds.numpy())\n",
        "\n",
        "    # 计算 AUC-ROC 分数，假设您的输出为二元分类的概率\n",
        "    # 注意：您需要根据您的模型输出调整这部分\n",
        "    probs = torch.nn.functional.softmax(all_probs, dim=1)[:, 1]\n",
        "    auc_roc = roc_auc_score(all_labels.numpy(), probs.numpy())\n",
        "\n",
        "    print('Loss/val', validation_loss, epoch)\n",
        "    print('Accuracy/val', validation_acc, epoch)\n",
        "    print(f'Validation - Epoch: {epoch}, Loss: {validation_loss:.4f}, Accuracy: {validation_acc:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, F1: {f1:.4f}, AUC-ROC: {auc_roc:.4f}')\n",
        "\n",
        "    return validation_loss, validation_acc, recall, precision, f1, auc_roc, all_labels, all_probs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs8zXYZtNk3f"
      },
      "source": [
        "# conv3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ppsYw5NDNv24"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.hub import load_state_dict_from_url\n",
        "import torchvision\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "\n",
        "import os,inspect,sys\n",
        "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
        "sys.path.insert(0,currentdir)\n",
        "#from Attention import ProjectorBlock3D, LinearAttentionBlock3D\n",
        "\n",
        "\"\"\"\n",
        "Implementation of 3D CNN.\n",
        "\"\"\"\n",
        "class CNN3D(nn.Module):\n",
        "    def __init__(self, sample_size=128, sample_duration=16, drop_p=0.0, hidden1=512, hidden2=256, num_classes=100):\n",
        "        super(CNN3D, self).__init__()\n",
        "        self.sample_size = sample_size\n",
        "        self.sample_duration = sample_duration\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # network params\n",
        "        self.ch1, self.ch2, self.ch3 = 32, 48, 48\n",
        "        self.k1, self.k2, self.k3 = (3,7,7), (3,7,7), (3,5,5)\n",
        "        self.s1, self.s2, self.s3 = (2,2,2), (2,2,2), (2,2,2)\n",
        "        self.p1, self.p2, self.p3 = (0,0,0), (0,0,0), (0,0,0)\n",
        "        self.d1, self.d2, self.d3 = (1,1,1), (1,1,1), (1,1,1)\n",
        "        self.hidden1, self.hidden2 = hidden1, hidden2\n",
        "        self.drop_p = drop_p\n",
        "        self.pool_k, self.pool_s, self.pool_p, self.pool_d = (1,2,2), (1,2,2), (0,0,0), (1,1,1)\n",
        "        # Conv1\n",
        "        self.conv1_output_shape = self.compute_output_shape(self.sample_duration, self.sample_size,\n",
        "            self.sample_size, self.k1, self.s1, self.p1, self.d1)\n",
        "        # self.conv1_output_shape = self.compute_output_shape(self.conv1_output_shape[0], self.conv1_output_shape[1],\n",
        "        #     self.conv1_output_shape[2], self.pool_k, self.pool_s, self.pool_p, self.pool_d)\n",
        "        # Conv2\n",
        "        self.conv2_output_shape = self.compute_output_shape(self.conv1_output_shape[0], self.conv1_output_shape[1],\n",
        "            self.conv1_output_shape[2], self.k2, self.s2, self.p2, self.d2)\n",
        "        # self.conv2_output_shape = self.compute_output_shape(self.conv2_output_shape[0], self.conv2_output_shape[1],\n",
        "        #     self.conv2_output_shape[2], self.pool_k, self.pool_s, self.pool_p, self.pool_d)\n",
        "        # Conv3\n",
        "        self.conv3_output_shape = self.compute_output_shape(self.conv2_output_shape[0], self.conv2_output_shape[1],\n",
        "            self.conv2_output_shape[2], self.k3, self.s3, self.p3, self.d3)\n",
        "        # print(self.conv1_output_shape, self.conv2_output_shape, self.conv3_output_shape)\n",
        "\n",
        "        # network architecture\n",
        "        # in_channels=1 for grayscale, 3 for rgb\n",
        "        self.conv1 = nn.Conv3d(in_channels=3, out_channels=self.ch1, kernel_size=self.k1,\n",
        "            stride=self.s1, padding=self.p1, dilation=self.d1)\n",
        "        self.bn1 = nn.BatchNorm3d(self.ch1)\n",
        "        self.conv2 = nn.Conv3d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k2,\n",
        "            stride=self.s2, padding=self.p2, dilation=self.d2)\n",
        "        self.bn2 = nn.BatchNorm3d(self.ch2)\n",
        "        self.conv3 = nn.Conv3d(in_channels=self.ch2, out_channels=self.ch3, kernel_size=self.k3,\n",
        "            stride=self.s3, padding=self.p3, dilation=self.d3)\n",
        "        self.bn3 = nn.BatchNorm3d(self.ch3)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.drop = nn.Dropout3d(p=self.drop_p)\n",
        "        self.pool = nn.MaxPool3d(kernel_size=self.pool_k)\n",
        "        self.fc1 = nn.Linear(self.ch3 * self.conv3_output_shape[0] * self.conv3_output_shape[1] * self.conv3_output_shape[2], self.hidden1)\n",
        "        self.fc2 = nn.Linear(self.hidden1, self.hidden2)\n",
        "        self.fc3 = nn.Linear(self.hidden2, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv1\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        # x = self.pool(x)\n",
        "        # x = self.drop(x)\n",
        "        # Conv2\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        # x = self.pool(x)\n",
        "        # x = self.drop(x)\n",
        "        # Conv3\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        # x = self.drop(x)\n",
        "        # MLP\n",
        "        # print(x.shape)\n",
        "        # x.size(0) ------ batch_size\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def compute_output_shape(self, D_in, H_in, W_in, k, s, p, d):\n",
        "        # Conv\n",
        "        D_out = np.floor((D_in + 2*p[0] - d[0]*(k[0] - 1) - 1)/s[0] + 1).astype(int)\n",
        "        H_out = np.floor((H_in + 2*p[1] - d[1]*(k[1] - 1) - 1)/s[1] + 1).astype(int)\n",
        "        W_out = np.floor((W_in + 2*p[2] - d[2]*(k[2] - 1) - 1)/s[2] + 1).astype(int)\n",
        "\n",
        "        return D_out, H_out, W_out\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Implementation of 3D Resnet\n",
        "Reference: Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?\n",
        "\"\"\"\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    # planes refer to the number of feature maps\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.downsample = downsample\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv3d(\n",
        "            planes, planes, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        # conv1\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        # conv2\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        # downsample\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        # print(out.shape, residual.shape)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    # planes refer to the number of feature maps\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.downsample = downsample\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            inplanes, planes, kernel_size=1, bias=False) # kernal_size=1 don't need padding\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.conv2 = nn.Conv3d(\n",
        "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        # conv1\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        # conv2\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        # conv3\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        # downsample\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        # print(out.shape, residual.shape)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def downsample_basic_block(x, planes, stride):\n",
        "    # decrease data resolution if stride not equals to 1\n",
        "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
        "    # shape: (batch_size, channel, t, h, w)\n",
        "    # try to match the channel size\n",
        "    zero_pads = torch.Tensor(\n",
        "        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n",
        "        out.size(4)).zero_()\n",
        "    if isinstance(out.data, torch.cuda.FloatTensor):\n",
        "        zero_pads = zero_pads.cuda()\n",
        "\n",
        "    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, shortcut_type, sample_size, sample_duration, attention=False, num_classes=500):\n",
        "        super(ResNet, self).__init__()\n",
        "        # initialize inplanes to 64, it'll be changed later\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            3, 64, kernel_size=7, stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n",
        "        # layers refers to the number of blocks in each layer\n",
        "        self.layer1 = self._make_layer(\n",
        "            block, 64, layers[0], shortcut_type, stride=1)\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, 128, layers[1], shortcut_type, stride=2)\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, 256, layers[2], shortcut_type, stride=2)\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, 512, layers[3], shortcut_type, stride=2)\n",
        "        # calclatue kernal size for average pooling\n",
        "        last_duration = int(math.ceil(sample_duration / 16))\n",
        "        last_size = int(math.ceil(sample_size / 32))\n",
        "        self.avgpool = nn.AvgPool3d(\n",
        "            (last_duration, last_size, last_size), stride=1)\n",
        "        # attention blocks\n",
        "        self.attention = attention\n",
        "        if self.attention:\n",
        "            self.attn1 = LinearAttentionBlock3D(in_channels=512*block.expansion, normalize_attn=True)\n",
        "            self.attn2 = LinearAttentionBlock3D(in_channels=512*block.expansion, normalize_attn=True)\n",
        "            self.attn3 = LinearAttentionBlock3D(in_channels=512*block.expansion, normalize_attn=True)\n",
        "            self.attn4 = LinearAttentionBlock3D(in_channels=512*block.expansion, normalize_attn=True)\n",
        "            self.projector1 = ProjectorBlock3D(in_channels=64*block.expansion, out_channels=512*block.expansion)\n",
        "            self.projector2 = ProjectorBlock3D(in_channels=128*block.expansion, out_channels=512*block.expansion)\n",
        "            self.projector3 = ProjectorBlock3D(in_channels=256*block.expansion, out_channels=512*block.expansion)\n",
        "            self.fc = nn.Linear(512 * block.expansion * 4, num_classes)\n",
        "        else:\n",
        "            self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "        # init the weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, shortcut_type, stride):\n",
        "        downsample = None\n",
        "        # when the in-channel and the out-channel dismatch, downsample!!!\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            # stride once for downsample and block.\n",
        "            if shortcut_type == 'A':\n",
        "                downsample = partial(\n",
        "                    downsample_basic_block,\n",
        "                    planes=planes * block.expansion,\n",
        "                    stride=stride)\n",
        "            else:\n",
        "                downsample = nn.Sequential(\n",
        "                    nn.Conv3d(\n",
        "                        self.inplanes,\n",
        "                        planes * block.expansion,\n",
        "                        kernel_size=1,\n",
        "                        stride=stride,\n",
        "                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n",
        "\n",
        "        layers = []\n",
        "        # only the first block needs downsample.\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        # change inplanes for the next layer\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        l1 = self.layer1(x)\n",
        "        l2 = self.layer2(l1)\n",
        "        l3 = self.layer3(l2)\n",
        "        l4 = self.layer4(l3)\n",
        "\n",
        "        g = self.avgpool(l4)\n",
        "        # attention\n",
        "        if self.attention:\n",
        "            # print(l1.shape, l2.shape, l3.shape, l4.shape, g.shape)\n",
        "            c1, g1 = self.attn1(self.projector1(l1), g)\n",
        "            c2, g2 = self.attn2(self.projector2(l2), g)\n",
        "            c3, g3 = self.attn3(self.projector3(l3), g)\n",
        "            c4, g4 = self.attn4(l4, g)\n",
        "            g = torch.cat((g1,g2,g3,g4), dim=1)\n",
        "            x = self.fc(g)\n",
        "        else:\n",
        "            c1, c2, c3, c4 = None, None, None, None\n",
        "            # x.size(0) ------ batch_size\n",
        "            g = g.view(g.size(0), -1)\n",
        "            x = self.fc(g)\n",
        "\n",
        "        return [x, c1, c2, c3, c4]\n",
        "\n",
        "    def load_my_state_dict(self, state_dict):\n",
        "        my_state_dict = self.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name == 'fc.weight' or name == 'fc.bias':\n",
        "                continue\n",
        "            my_state_dict[name].copy_(param.data)\n",
        "\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# ...[省略其他import和类定义]...\n",
        "\n",
        "def resnet18(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\"\"\"\n",
        "    model = models.resnet18(pretrained=pretrained, progress=progress, **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet34(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\"\"\"\n",
        "    model = models.resnet34(pretrained=pretrained, progress=progress, **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet101(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\"\"\"\n",
        "    model = models.resnet101(pretrained=pretrained, progress=progress, **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet50(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"构建一个ResNet-50模型。\"\"\"\n",
        "    model = models.resnet50(pretrained=pretrained,progress=progress, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\"\"\"\n",
        "    model = models.resnet152(pretrained=pretrained, progress=progress, **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet200(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-200 model.\"\"\"\n",
        "    model = models.resnet200(pretrained=pretrained, progress=progress, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# ...[省略其他函数和类定义]...\n",
        "\n",
        "# 使用时可以直接调用\n",
        "#cnn3d = resnet50(pretrained=True, progress=True, num_classes=num_classes)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "3D CNN Models from torchvision.models\n",
        "Reference: https://pytorch.org/docs/stable/torchvision/models.html#video-classification\n",
        "\"\"\"\n",
        "class r3d_18(nn.Module):\n",
        "    def __init__(self, pretrained=True, num_classes=500):\n",
        "        super(r3d_18, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "        self.num_classes = num_classes\n",
        "        model = torchvision.models.video.r3d_18(pretrained=self.pretrained)\n",
        "        # delete the last fc layer\n",
        "        modules = list(model.children())[:-1]\n",
        "        # print(modules)\n",
        "        self.r3d_18 = nn.Sequential(*modules)\n",
        "        self.fc1 = nn.Linear(model.fc.in_features, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.r3d_18(x)\n",
        "        # print(out.shape)\n",
        "        # Flatten the layer to fc\n",
        "        out = out.flatten(1)\n",
        "        out = self.fc1(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class mc3_18(nn.Module):\n",
        "    def __init__(self, pretrained=True, num_classes=500):\n",
        "        super(mc3_18, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "        self.num_classes = num_classes\n",
        "        model = torchvision.models.video.mc3_18(pretrained=self.pretrained)\n",
        "        # delete the last fc layer\n",
        "        modules = list(model.children())[:-1]\n",
        "        # print(modules)\n",
        "        self.mc3_18 = nn.Sequential(*modules)\n",
        "        self.fc1 = nn.Linear(model.fc.in_features, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.mc3_18(x)\n",
        "        # print(out.shape)\n",
        "        # Flatten the layer to fc\n",
        "        out = out.flatten(1)\n",
        "        out = self.fc1(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class r2plus1d_18(nn.Module):\n",
        "    def __init__(self, pretrained=True, num_classes=500):\n",
        "        super(r2plus1d_18, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "        self.num_classes = num_classes\n",
        "        model = torchvision.models.video.r2plus1d_18(pretrained=self.pretrained)\n",
        "        # delete the last fc layer\n",
        "        modules = list(model.children())[:-1]\n",
        "        # print(modules)\n",
        "        self.r2plus1d_18 = nn.Sequential(*modules)\n",
        "        self.fc1 = nn.Linear(model.fc.in_features, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.r2plus1d_18(x)\n",
        "        # print(out.shape)\n",
        "        # Flatten the layer to fc\n",
        "        out = out.flatten(1)\n",
        "        out = self.fc1(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# 调整数据维度以匹配模型的输入要求\n",
        "#data = dataset[0]['data'].permute(1, 0, 2, 3, 4)  # 将维度重排为 [num_channels, 1, num_frames, height, width]\n",
        "\n",
        "# 选择你希望使用的帧数\n",
        "#desired_num_frames = 30  # 选择你希望使用的帧数\n",
        "#data = data[:, :, :desired_num_frames, :, :]  # 切片选择帧数\n",
        "#data = data.unsqueeze(0)  # 添加批次维度，变成 [1, num_channels, num_frames, height, width]\n",
        "\n",
        "# 将处理后的数据传递给模型进行推理\n",
        "#output = cnn3d(data)\n",
        "\n",
        "# 在这里可以处理模型的输出，如计算损失或进行后续的操作\n",
        "\n",
        "\n",
        "# Test\n",
        "if __name__ == '__main__':\n",
        "    import sys\n",
        "    sys.path.append(\"..\")\n",
        "    import torchvision.transforms as transforms\n",
        "\n",
        "    sample_size = 128\n",
        "    sample_duration = 16\n",
        "    num_classes = 2\n",
        "    transform = transforms.Compose([transforms.Resize([sample_size, sample_size]), transforms.ToTensor()])\n",
        "    dataset = ASD_Isolated(data_path=\"/content/drive/MyDrive/output_frames\",\n",
        "\n",
        "        num_classes=num_classes, transform=transform)\n",
        "    #cnn3d = CNN3D(sample_size=sample_size, sample_duration=sample_duration, num_classes=num_classes)\n",
        "    #cnn3d = resnet50(pretrained=True, progress=True, sample_size=sample_size, sample_duration=sample_duration, attention=True, num_classes=num_classes)\n",
        "    cnn3d = r3d_18(pretrained=True, num_classes=num_classes)\n",
        "    # cnn3d = mc3_18(pretrained=True, num_classes=num_classes)\n",
        "    # cnn3d = r2plus1d_18(pretrained=True, num_classes=num_classes)\n",
        "    # print(dataset[0]['images'].shape)\n",
        "\n",
        "\n",
        "\n",
        "    #print(cnn3d(dataset[0]['data'].unsqueeze(0)))\n",
        "\n",
        "    # Test for loading pretrained models\n",
        "    # state_dict = torch.load('resnet-18-kinetics.pth')\n",
        "    # for name, param in state_dict.items():\n",
        "    #     print(name)\n",
        "    # # print(state_dict['arch'])\n",
        "    # # print(state_dict['optimizer'])\n",
        "    # # print(state_dict['epoch'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UmhxQsr3roB"
      },
      "source": [
        "# **ASD_CNN3D**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7-sqw01FhU6I",
        "outputId": "1296a2bb-e4c5-4cff-fcd4-885be405f3bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "######################Training Starts#######################\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss {'train': 0.6457189880311489} 1\n",
            "Accuracy {'train': 0.7388535031847133} 1\n",
            "第 1 轮平均训练损失: 0.645719 | 准确率: 73.89%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss/val 0.7488630060106516 0\n",
            "Accuracy/val 0.8589743589743589 0\n",
            "Validation - Epoch: 0, Loss: 0.7489, Accuracy: 0.8590, Recall: 0.2143, Precision: 1.0000, F1: 0.3529, AUC-ROC: 0.8672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss {'train': 0.06159824933856726} 2\n",
            "Accuracy {'train': 0.9872611464968153} 2\n",
            "第 2 轮平均训练损失: 0.061598 | 准确率: 98.73%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss/val 0.3366762474179268 1\n",
            "Accuracy/val 0.9230769230769231 1\n",
            "Validation - Epoch: 1, Loss: 0.3367, Accuracy: 0.9231, Recall: 0.7857, Precision: 0.7857, F1: 0.7857, AUC-ROC: 0.9475\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss {'train': 0.008570991491433233} 3\n",
            "Accuracy {'train': 0.9978768577494692} 3\n",
            "第 3 轮平均训练损失: 0.008571 | 准确率: 99.79%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss/val 1.9930184236145578 2\n",
            "Accuracy/val 0.8333333333333334 2\n",
            "Validation - Epoch: 2, Loss: 1.9930, Accuracy: 0.8333, Recall: 0.0714, Precision: 1.0000, F1: 0.1333, AUC-ROC: 0.8672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss {'train': 0.0005665759887051536} 4\n",
            "Accuracy {'train': 1.0} 4\n",
            "第 4 轮平均训练损失: 0.000567 | 准确率: 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss/val 4.071958021721002 3\n",
            "Accuracy/val 0.8205128205128205 3\n",
            "Validation - Epoch: 3, Loss: 4.0720, Accuracy: 0.8205, Recall: 0.0000, Precision: 0.0000, F1: 0.0000, AUC-ROC: 0.7098\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-b5105547e400>\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# Training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Validation phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-24345ae73740>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, criterion, optimizer, dataloader, device, epoch, logger, log_interval, writer)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# 反向传播 & 优化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Assume the following modules are correctly imported:\n",
        "# from dataset import ASD_Isol\n",
        "# from train import train_epoch\n",
        "# from validation import val_epoch\n",
        "# from Conv3D import CNN3D\n",
        "\n",
        "# Setting paths\n",
        "train_data_path = \"/content/drive/MyDrive/output_frames/train\"\n",
        "val_data_path = \"/content/drive/MyDrive/output_frames/test\"\n",
        "model_path = \"/content/drive/MyDrive/cnn3d_models\"\n",
        "log_path = \"cnn3d_log_{:%Y-%m-%d_%H-%M-%S}.log\".format(datetime.now())\n",
        "sum_path = \"cnn3d_runs_{:%Y-%m-%d_%H-%M-%S}\".format(datetime.now())\n",
        "\n",
        "# Logging setup\n",
        "logging.basicConfig(level=logging.INFO, format='%(message)s', handlers=[logging.FileHandler(log_path), logging.StreamHandler()])\n",
        "logger = logging.getLogger('CNN3D')\n",
        "logger.info('Logging setup complete...')\n",
        "writer = SummaryWriter(sum_path)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "num_classes = 2\n",
        "epochs = 5\n",
        "batch_size = 48\n",
        "learning_rate = 1e-3\n",
        "log_interval = 20\n",
        "sample_size = 128\n",
        "sample_duration = 30  # Video sample of 30 frames\n",
        "\n",
        "# Load data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize([sample_size, sample_size]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Instantiating the dataset class for train and test sets\n",
        "train_dataset = ASD_Isolated(data_path=train_data_path, transform=transform)\n",
        "val_dataset = ASD_Isolated(data_path=val_data_path, transform=transform)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Create CNN3D model\n",
        "model = CNN3D(sample_size=sample_size, sample_duration=sample_duration, num_classes=num_classes).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Lists to store metrics\n",
        "train_losses, train_accuracies = [], []\n",
        "test_losses, test_accuracies = [], []\n",
        "test_recalls, test_precisions, test_f1s, test_auc_scores = [], [], [], []\n",
        "\n",
        "# Variables to store labels and probabilities of the last epoch\n",
        "last_labels, last_probs = None, None\n",
        "\n",
        "# Training starts\n",
        "print(\"Training Starts\".center(60, '#'))\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    train_loss, train_accuracy = train_epoch(model, criterion, optimizer, train_loader, device, epoch, logger, log_interval, writer)\n",
        "\n",
        "    # Validation phase\n",
        "    test_loss, test_accuracy, test_recall, test_precision, test_f1, test_auc_roc, labels, probs = val_epoch(model, criterion, val_loader, device, epoch, logger, writer)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    test_recalls.append(test_recall)\n",
        "    test_precisions.append(test_precision)\n",
        "    test_f1s.append(test_f1)\n",
        "    test_auc_scores.append(test_auc_roc)\n",
        "\n",
        "    # Update last labels and probabilities\n",
        "    last_labels, last_probs = labels, probs\n",
        "\n",
        "print(\"Training Complete\".center(60, '#'))\n",
        "\n",
        "# Plotting training and testing losses and accuracies\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.title('Training and Testing Loss per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(test_accuracies, label='Test Accuracy')\n",
        "plt.title('Training and Testing Accuracy per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plotting recall, precision, and F1-score\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(test_recalls, label='Test Recall')\n",
        "plt.title('Test Recall per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(test_precisions, label='Test Precision')\n",
        "plt.title('Test Precision per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(test_f1s, label='Test F1 Score')\n",
        "plt.title('Test F1 Score per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# AUC-ROC Curve for the last epoch\n",
        "probs = F.softmax(last_probs, dim=1)[:, 1].numpy()  # Assuming binary classification\n",
        "fpr, tpr, _ = roc_curve(last_labels.numpy(), probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC Curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10FKEslpMpx9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from fvcore.nn import FlopCountAnalysis, parameter_count\n",
        "# Create a dummy input tensor for FLOPs calculation\n",
        "# Adjust the size according to the input dimensions your model expects\n",
        "dummy_input = torch.randn(1, 3, sample_duration, sample_size, sample_size).to(device)\n",
        "\n",
        "# Calculate FLOPs\n",
        "flops = FlopCountAnalysis(model, dummy_input)\n",
        "print(\"Total FLOPs: {flops.total()}\")\n",
        "\n",
        "# Calculate Parameters\n",
        "params = parameter_count(model)\n",
        "print(f\"Total Parameters: {params['total']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}