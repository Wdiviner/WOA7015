{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3hFJHLJGOlm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import transforms, models\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing steps for the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the pre-trained ResNet model\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "# Remove the last layer of the ResNet model to obtain the feature extractor\n",
        "resnet_feat = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "# Create an empty list to store the features and labels\n",
        "resnet.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOim-AbVGVjx",
        "outputId": "30df2d82-2e7a-41ad-de06-64889751ebfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 83.0MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def has_less_than_30_images(folder_path):\n",
        "    # Supported image file extensions\n",
        "    image_extensions = {'.png', '.jpg', '.jpeg'}\n",
        "\n",
        "    # Count the number of image files\n",
        "    image_count = 0\n",
        "    for file in os.listdir(folder_path):\n",
        "        if any(file.endswith(ext) for ext in image_extensions):\n",
        "            image_count += 1\n",
        "\n",
        "            # If 30 or more images are found, return False\n",
        "            if image_count >= 30:\n",
        "                return False\n",
        "    # print(\"image_count : \", image_count)\n",
        "    # If fewer than 30 images, return True\n",
        "    return True\n",
        "\n",
        "def frame_2_data(data_dir):\n",
        "    samples = []\n",
        "    # Loop over the folders in the dataset\n",
        "    for image_folder in os.listdir(data_dir):\n",
        "        if image_folder == '.DS_Store':\n",
        "            continue\n",
        "\n",
        "        print(\"image_folder : \", image_folder)\n",
        "\n",
        "        if \"hand_flapping\" in image_folder:\n",
        "            label = 0\n",
        "        elif \"arm_flapping\" in image_folder:\n",
        "            label = 1\n",
        "\n",
        "        image_folder_path = os.path.join(data_dir, image_folder)\n",
        "        # print(\"image_folder_path : \", image_folder_path)\n",
        "\n",
        "        frame_count = 0\n",
        "        frames = []\n",
        "        for image_file in os.listdir(image_folder_path):\n",
        "            if has_less_than_30_images(image_folder_path):\n",
        "                continue\n",
        "\n",
        "            if image_file == \".DS_Store\":\n",
        "                continue\n",
        "\n",
        "            if image_file.endswith(('.png', '.jpg', '.jpeg')):  # check for image files\n",
        "                frame_count += 1\n",
        "                image_path = os.path.join(image_folder_path, image_file)\n",
        "                image = cv2.imread(image_path)\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                image = transform(image)\n",
        "\n",
        "                frames.append(image)\n",
        "\n",
        "        # when finish current file, then next one\n",
        "        # Stack the frames into a tensor of shape (num_frames, 3, 224, 224)\n",
        "        frames_tensor = torch.stack(frames, dim=0)\n",
        "        # Extract the features using the pre-trained ResNet50 model\n",
        "        with torch.no_grad():\n",
        "            features_tensor = resnet_feat(frames_tensor)\n",
        "            # print(features_tensor.shape)\n",
        "        # Flatten the features tensor\n",
        "        features_tensor = torch.flatten(features_tensor, start_dim=1)\n",
        "        # Convert the features tensor to a numpy array\n",
        "        features = features_tensor.cpu().numpy()\n",
        "        # Append the features and label to the samples list\n",
        "        samples.append((features, label))\n",
        "\n",
        "    np.random.shuffle(samples)\n",
        "    return samples"
      ],
      "metadata": {
        "id": "dXms6CWGGWS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_dir = '/content/drive/MyDrive/train'\n",
        "test_data_dir = '/content/drive/MyDrive/test'\n",
        "\n",
        "train_samples = frame_2_data(train_data_dir)\n",
        "test_samples = frame_2_data(test_data_dir)\n",
        "\n",
        "train_features, train_labels = zip(*train_samples)\n",
        "test_features, test_labels = zip(*test_samples)\n",
        "\n",
        "\n",
        "# Convert the labels to numerical labels using a LabelEncoder\n",
        "le = LabelEncoder()\n",
        "train_numerical_labels = le.fit_transform(train_labels)\n",
        "test_numerical_labels = le.fit_transform(test_labels)\n",
        "# Convert the features and labels arrays to numpy arrays\n",
        "# Convert the features and labels arrays to numpy arrays\n",
        "train_features = np.array(train_features)\n",
        "train_labels = train_numerical_labels\n",
        "test_features = np.array(test_features)\n",
        "test_labels = test_numerical_labels\n",
        "\n",
        "# Print the shapes of the features and labels arrays\n",
        "print(\"Train Features shape:\", train_features.shape)\n",
        "print(\"Train Labels shape:\", train_labels.shape)\n",
        "print(\"Test Features shape:\", test_features.shape)\n",
        "print(\"Test Labels shape:\", test_labels.shape)\n",
        "\n",
        "# Save the features and labels to numpy arrays\n",
        "np.save('train_features.npy', train_features)\n",
        "np.save('train_labels.npy', train_labels)\n",
        "np.save('test_features.npy', test_features)\n",
        "np.save('test_labels.npy', test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGKnh0QBGiob",
        "outputId": "17c59109-b28d-43ff-cf5a-ad734f431015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_folder :  12_hand_flapping_1_1\n",
            "image_folder :  25_arm_flapping_1_1\n",
            "image_folder :  27_arm_flapping_1_1\n",
            "image_folder :  28_hand_flapping_1_1\n",
            "image_folder :  19_arm_flapping_1_1\n",
            "image_folder :  1_arm_flapping_1_1\n",
            "image_folder :  14_hand_flapping_1_1\n",
            "Train Features shape: (5, 30, 512)\n",
            "Train Labels shape: (5,)\n",
            "Test Features shape: (2, 30, 512)\n",
            "Test Labels shape: (2,)\n"
          ]
        }
      ]
    }
  ]
}