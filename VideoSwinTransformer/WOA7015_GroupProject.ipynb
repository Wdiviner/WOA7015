{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33Hg_jp3i_4N"
      },
      "source": [
        "# Data & Environment Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osLYgM0mjHCi"
      },
      "source": [
        "## Load Pre-processed data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl3lmUcqoZCU",
        "outputId": "9b25a515-1abd-496d-c737-adb1553d52b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'group_project_data'...\n",
            "remote: Enumerating objects: 36672, done.\u001b[K\n",
            "remote: Counting objects: 100% (3561/3561), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3558/3558), done.\u001b[K\n",
            "remote: Total 36672 (delta 4), reused 3147 (delta 3), pack-reused 33111\u001b[K\n",
            "Receiving objects: 100% (36672/36672), 1.86 GiB | 35.28 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Updating files: 100% (32104/32104), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/VinceChin/group_project_data.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-Kzr8bdpKKL"
      },
      "source": [
        "## Import Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQz-FIQNlJRR",
        "outputId": "7054297e-1788-4063-cd84-fc28a0e84a09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting timm\n",
            "  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.9.12\n"
          ]
        }
      ],
      "source": [
        "#install dependencies: given that my colab has CUDA 11.8\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xqYSItnpJiV"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_QG-4REkyxq"
      },
      "source": [
        "## Import mmaction2 & other packages needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHxSL__rghK8",
        "outputId": "ca05dc47-18e2-4084-f70f-3f45507a2be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Copyright (C) 2021 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# check nvcc version\n",
        "!nvcc -V\n",
        "# check GCC version\n",
        "!gcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pWoolBnPmNV0",
        "outputId": "f01fb6a2-06b0-4b0a-aa87-05985cf6b5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openmim\n",
            "  Downloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m883.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from openmim) (8.1.7)\n",
            "Collecting colorama (from openmim)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting model-index (from openmim)\n",
            "  Downloading model_index-0.1.11-py3-none-any.whl (34 kB)\n",
            "Collecting opendatalab (from openmim)\n",
            "  Downloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from openmim) (1.5.3)\n",
            "Requirement already satisfied: pip>=19.3 in /usr/local/lib/python3.10/dist-packages (from openmim) (23.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from openmim) (2.31.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim) (13.7.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim) (0.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (6.0.1)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (3.5.1)\n",
            "Collecting ordered-set (from model-index->openmim)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting pycryptodome (from opendatalab->openmim)\n",
            "  Downloading pycryptodome-3.19.1-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim) (4.66.1)\n",
            "Collecting openxlab (from opendatalab->openmim)\n",
            "  Downloading openxlab-0.0.33-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.5/299.5 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (1.23.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->openmim) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->openmim) (1.16.0)\n",
            "Collecting oss2~=2.17.0 (from openxlab->opendatalab->openmim)\n",
            "  Downloading oss2-2.17.0.tar.gz (259 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests (from openmim)\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich (from openmim)\n",
            "  Downloading rich-13.4.2-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setuptools~=60.2.0 (from openxlab->opendatalab->openmim)\n",
            "  Downloading setuptools-60.2.0-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.1/953.1 kB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from opendatalab->openmim)\n",
            "  Downloading tqdm-4.65.2-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1 (from requests->openmim)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting crcmod>=1.7 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading aliyun_python_sdk_kms-2.16.2-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aliyun-python-sdk-core>=2.13.12 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading aliyun-python-sdk-core-2.14.0.tar.gz (443 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.0/443.0 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cryptography>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (2.21)\n",
            "Building wheels for collected packages: oss2, aliyun-python-sdk-core, crcmod\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oss2: filename=oss2-2.17.0-py3-none-any.whl size=112371 sha256=2c58118ae4ff8814140c62210af5ebfa391158e42d5ccd3fe6bb255aed0ec868\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/04/7b/7e61b8157fdf211c5131375240d0d86ca82e2a88ead9672c88\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.14.0-py3-none-any.whl size=535289 sha256=f47429478e2469571797c8d81fcafeea2fb6080a1ea444771cc27afe3d188ccb\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/3c/68/b7eab618d9f1d5e7d386296f1e07e2cf36aaa1eb5161885038\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31404 sha256=93500010ba7a9044ba42db277fc606cf65be87a4d83e9da6d1655ea7f46efe07\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "Successfully built oss2 aliyun-python-sdk-core crcmod\n",
            "Installing collected packages: crcmod, urllib3, tqdm, setuptools, pycryptodome, ordered-set, jmespath, colorama, rich, requests, model-index, aliyun-python-sdk-core, aliyun-python-sdk-kms, oss2, openxlab, opendatalab, openmim\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.7.0\n",
            "    Uninstalling rich-13.7.0:\n",
            "      Successfully uninstalled rich-13.7.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "cvxpy 1.3.2 requires setuptools>65.5.1, but you have setuptools 60.2.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.28.2 which is incompatible.\n",
            "yfinance 0.2.33 requires requests>=2.31, but you have requests 2.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aliyun-python-sdk-core-2.14.0 aliyun-python-sdk-kms-2.16.2 colorama-0.4.6 crcmod-1.7 jmespath-0.10.0 model-index-0.1.11 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.33 ordered-set-4.1.0 oss2-2.17.0 pycryptodome-3.19.1 requests-2.28.2 rich-13.4.2 setuptools-60.2.0 tqdm-4.65.2 urllib3-1.26.18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu121/torch2.1.0/index.html\n",
            "Collecting mmengine\n",
            "  Downloading mmengine-0.10.2-py3-none-any.whl (450 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.4/450.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict (from mmengine)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmengine) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmengine) (1.23.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmengine) (6.0.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmengine) (13.4.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from mmengine) (2.4.0)\n",
            "Collecting yapf (from mmengine)\n",
            "  Downloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.10/dist-packages (from mmengine) (4.8.0.76)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine) (2.16.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine) (7.0.1)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine) (4.1.0)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->mmengine) (3.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mmengine) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mmengine) (1.16.0)\n",
            "Installing collected packages: addict, yapf, mmengine\n",
            "Successfully installed addict-2.4.0 mmengine-0.10.2 yapf-0.40.2\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu121/torch2.1.0/index.html\n",
            "Collecting mmcv>=2.0.0\n",
            "  Downloading https://download.openmmlab.com/mmcv/dist/cu121/torch2.1.0/mmcv-2.1.0-cp310-cp310-manylinux1_x86_64.whl (94.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.1/94.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0) (2.4.0)\n",
            "Requirement already satisfied: mmengine>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0) (0.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0) (23.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0) (9.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0) (6.0.1)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0) (0.40.2)\n",
            "Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0) (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv>=2.0.0) (3.7.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv>=2.0.0) (13.4.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv>=2.0.0) (2.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv>=2.0.0) (7.0.1)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv>=2.0.0) (4.1.0)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv>=2.0.0) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->mmcv>=2.0.0) (3.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.0) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.0) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine>=0.3.0->mmcv>=2.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine>=0.3.0->mmcv>=2.0.0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mmengine>=0.3.0->mmcv>=2.0.0) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mmengine>=0.3.0->mmcv>=2.0.0) (1.16.0)\n",
            "Installing collected packages: mmcv\n",
            "Successfully installed mmcv-2.1.0\n",
            "Cloning into 'mmaction2'...\n",
            "remote: Enumerating objects: 22864, done.\u001b[K\n",
            "remote: Total 22864 (delta 0), reused 0 (delta 0), pack-reused 22864\u001b[K\n",
            "Receiving objects: 100% (22864/22864), 69.62 MiB | 35.75 MiB/s, done.\n",
            "Resolving deltas: 100% (16106/16106), done.\n",
            "/content/mmaction2\n",
            "Obtaining file:///content/mmaction2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting decord>=0.4.1 (from mmaction2==1.2.0)\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops (from mmaction2==1.2.0)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmaction2==1.2.0) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmaction2==1.2.0) (1.23.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mmaction2==1.2.0) (4.8.0.76)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mmaction2==1.2.0) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mmaction2==1.2.0) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from mmaction2==1.2.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->mmaction2==1.2.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->mmaction2==1.2.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->mmaction2==1.2.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->mmaction2==1.2.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->mmaction2==1.2.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->mmaction2==1.2.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->mmaction2==1.2.0) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmaction2==1.2.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmaction2==1.2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmaction2==1.2.0) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmaction2==1.2.0) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmaction2==1.2.0) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmaction2==1.2.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmaction2==1.2.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mmaction2==1.2.0) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->mmaction2==1.2.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->mmaction2==1.2.0) (1.3.0)\n",
            "Installing collected packages: einops, decord, mmaction2\n",
            "  Running setup.py develop for mmaction2\n",
            "Successfully installed decord-0.6.0 einops-0.7.0 mmaction2-1.2.0\n",
            "Collecting av>=9.0 (from -r requirements/optional.txt (line 1))\n",
            "  Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from -r requirements/optional.txt (line 2)) (0.18.3)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from -r requirements/optional.txt (line 3)) (0.4.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from -r requirements/optional.txt (line 4)) (0.10.1)\n",
            "Collecting lmdb (from -r requirements/optional.txt (line 5))\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from -r requirements/optional.txt (line 6)) (1.0.3)\n",
            "Collecting openai-clip (from -r requirements/optional.txt (line 7))\n",
            "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from -r requirements/optional.txt (line 8)) (23.2)\n",
            "Collecting pims (from -r requirements/optional.txt (line 9))\n",
            "  Downloading PIMS-0.6.1.tar.gz (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyTurboJPEG (from -r requirements/optional.txt (line 10))\n",
            "  Downloading PyTurboJPEG-1.7.3.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from -r requirements/optional.txt (line 11)) (0.12.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements/optional.txt (line 12)) (2.15.1)\n",
            "Collecting wandb (from -r requirements/optional.txt (line 13))\n",
            "  Downloading wandb-0.16.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug->-r requirements/optional.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from imgaug->-r requirements/optional.txt (line 3)) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug->-r requirements/optional.txt (line 3)) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from imgaug->-r requirements/optional.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug->-r requirements/optional.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug->-r requirements/optional.txt (line 3)) (0.19.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug->-r requirements/optional.txt (line 3)) (4.8.0.76)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug->-r requirements/optional.txt (line 3)) (2.31.6)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug->-r requirements/optional.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements/optional.txt (line 4)) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements/optional.txt (line 4)) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements/optional.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements/optional.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements/optional.txt (line 4)) (0.58.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements/optional.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements/optional.txt (line 4)) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements/optional.txt (line 4)) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements/optional.txt (line 4)) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements/optional.txt (line 4)) (1.0.7)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements/optional.txt (line 6)) (4.65.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements/optional.txt (line 6)) (2.28.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements/optional.txt (line 6)) (0.1.10)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements/optional.txt (line 6)) (0.4.9)\n",
            "Collecting ftfy (from openai-clip->-r requirements/optional.txt (line 7))\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from openai-clip->-r requirements/optional.txt (line 7)) (2023.6.3)\n",
            "Collecting slicerator>=0.9.8 (from pims->-r requirements/optional.txt (line 9))\n",
            "  Downloading slicerator-1.1.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->-r requirements/optional.txt (line 11)) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements/optional.txt (line 12)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements/optional.txt (line 12)) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements/optional.txt (line 12)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements/optional.txt (line 12)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements/optional.txt (line 12)) (3.5.1)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements/optional.txt (line 12)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements/optional.txt (line 12)) (60.2.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements/optional.txt (line 12)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements/optional.txt (line 12)) (3.0.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements/optional.txt (line 13)) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements/optional.txt (line 13))\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements/optional.txt (line 13)) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->-r requirements/optional.txt (line 13))\n",
            "  Downloading sentry_sdk-1.39.2-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements/optional.txt (line 13))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements/optional.txt (line 13)) (6.0.1)\n",
            "Collecting setproctitle (from wandb->-r requirements/optional.txt (line 13))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements/optional.txt (line 13)) (1.4.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->-r requirements/optional.txt (line 11)) (2.21)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements/optional.txt (line 13))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements/optional.txt (line 12)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements/optional.txt (line 12)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements/optional.txt (line 12)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements/optional.txt (line 12)) (1.3.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->-r requirements/optional.txt (line 4)) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->-r requirements/optional.txt (line 4)) (4.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->-r requirements/optional.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->-r requirements/optional.txt (line 6)) (3.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->-r requirements/optional.txt (line 6)) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->-r requirements/optional.txt (line 6)) (2023.11.17)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->-r requirements/optional.txt (line 3)) (3.2.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->-r requirements/optional.txt (line 3)) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->-r requirements/optional.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->-r requirements/optional.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements/optional.txt (line 12)) (2.1.3)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->openai-clip->-r requirements/optional.txt (line 7)) (0.2.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->-r requirements/optional.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->-r requirements/optional.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->-r requirements/optional.txt (line 3)) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->-r requirements/optional.txt (line 3)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->-r requirements/optional.txt (line 3)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->-r requirements/optional.txt (line 3)) (2.8.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements/optional.txt (line 13))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements/optional.txt (line 12)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements/optional.txt (line 12)) (3.2.2)\n",
            "Building wheels for collected packages: openai-clip, pims, PyTurboJPEG\n",
            "  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368645 sha256=5398140ce80629fa2d31363b998846f6b7975920b614c507e0ade6a8630528d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/77/8e/8d2f862df6bf7fb4e2007062d2cbaeae49862ec7b56d041229\n",
            "  Building wheel for pims (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pims: filename=PIMS-0.6.1-py3-none-any.whl size=82630 sha256=87a2d495c126a534e51bc499f5d56a11e5bfb6184d21b7e03ff6f926c1ee6425\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/bf/3e/bfa77232d942f8244145f9c713b6b38f6ef04b6fb5c021c114\n",
            "  Building wheel for PyTurboJPEG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyTurboJPEG: filename=PyTurboJPEG-1.7.3-py3-none-any.whl size=12463 sha256=6b6e01d4c5cb57c034c1ae3a466b1f76e17cdaebae05c1fd3dfd9e6403f0a2b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/bb/3d/d1032d5ec98ae80c4f92f41a07773d4f9641c64a10fd8c31b4\n",
            "Successfully built openai-clip pims PyTurboJPEG\n",
            "Installing collected packages: slicerator, lmdb, smmap, setproctitle, sentry-sdk, PyTurboJPEG, ftfy, docker-pycreds, av, pims, openai-clip, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 PyTurboJPEG-1.7.3 av-11.0.0 docker-pycreds-0.4.0 ftfy-6.1.3 gitdb-4.0.11 lmdb-1.4.1 openai-clip-1.0.1 pims-0.6.1 sentry-sdk-1.39.2 setproctitle-1.3.3 slicerator-1.1.0 smmap-5.0.1 wandb-0.16.2\n"
          ]
        }
      ],
      "source": [
        "# install MMEngine, MMCV and MMDetection using MIM\n",
        "\n",
        "%pip install -U openmim\n",
        "!mim install mmengine\n",
        "!mim install \"mmcv>=2.0.0\"\n",
        "\n",
        "# Install mmaction2\n",
        "!rm -rf mmaction2\n",
        "!git clone https://github.com/open-mmlab/mmaction2.git -b main\n",
        "%cd mmaction2\n",
        "\n",
        "!pip install -e .\n",
        "\n",
        "# Install some optional requirements\n",
        "!pip install -r requirements/optional.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJjQXYnErSd8"
      },
      "source": [
        "## Check the result of Importing mmaction2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No_zZAFpWC-a",
        "outputId": "48fdbb9f-4583-4062-bfa2-713eabaa91d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121 True\n",
            "1.2.0\n",
            "12.1\n",
            "GCC 9.3\n",
            "OrderedDict([('sys.platform', 'linux'), ('Python', '3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]'), ('CUDA available', True), ('numpy_random_seed', 2147483648), ('GPU 0', 'Tesla T4'), ('CUDA_HOME', '/usr/local/cuda'), ('NVCC', 'Cuda compilation tools, release 12.2, V12.2.140'), ('GCC', 'x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0'), ('PyTorch', '2.1.0+cu121'), ('PyTorch compiling details', 'PyTorch built with:\\n  - GCC 9.3\\n  - C++ Version: 201703\\n  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\\n  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\\n  - LAPACK is enabled (usually provided by MKL)\\n  - NNPACK is enabled\\n  - CPU capability usage: AVX2\\n  - CUDA Runtime 12.1\\n  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\\n  - CuDNN 8.9.2\\n  - Magma 2.6.1\\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \\n'), ('TorchVision', '0.16.0+cu121'), ('OpenCV', '4.8.0'), ('MMEngine', '0.10.2')])\n"
          ]
        }
      ],
      "source": [
        "# Check Pytorch installation\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "\n",
        "# Check MMAction2 installation\n",
        "import mmaction\n",
        "print(mmaction.__version__)\n",
        "\n",
        "# Check MMCV installation\n",
        "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
        "print(get_compiling_cuda_version())\n",
        "print(get_compiler_version())\n",
        "\n",
        "# Check MMEngine installation\n",
        "from mmengine.utils.dl_utils import collect_env\n",
        "print(collect_env())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGoTxIcCnmRz"
      },
      "source": [
        "## Download pre trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdG1k_U8nUlP",
        "outputId": "0888fd5c-025a-4b3d-f423-191702bca168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-10 10:58:59--  https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "Resolving download.openmmlab.com (download.openmmlab.com)... 163.181.66.108, 163.181.66.111, 163.181.66.109, ...\n",
            "Connecting to download.openmmlab.com (download.openmmlab.com)|163.181.66.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 97579339 (93M) [application/octet-stream]\n",
            "Saving to: ‘checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth’\n",
            "\n",
            "checkpoints/tsn_r50 100%[===================>]  93.06M  8.74MB/s    in 11s     \n",
            "\n",
            "2024-01-10 10:59:11 (8.55 MB/s) - ‘checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth’ saved [97579339/97579339]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir checkpoints\n",
        "!wget -c https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth \\\n",
        "      -O checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxEnfsRTkjs1"
      },
      "source": [
        "# Video Swin Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76rpnl0FTebg"
      },
      "source": [
        "## Self Custom Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8uyhUZYTjPW"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence, List\n",
        "\n",
        "from mmengine.evaluator import BaseMetric\n",
        "from mmengine.registry import METRICS\n",
        "from sklearn.metrics import f1_score, recall_score, roc_auc_score, roc_curve\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "@METRICS.register_module()  # registry\n",
        "class MySelfCustomMetric(BaseMetric):\n",
        "    default_prefix = 'SelfCustomMetric'  # set prefix showed in logs\n",
        "\n",
        "    def process(self, data_batch: Sequence[dict], data_samples: Sequence[dict]):\n",
        "        \"\"\"Process one batch of data and predictions. The processed\n",
        "        Results should be stored in `self.results`, which will be used\n",
        "        to compute the metrics when all batches have been processed.\n",
        "\n",
        "        Args:\n",
        "            data_batch (Sequence[Tuple[Any, dict]]): A batch of data\n",
        "                from the dataloader.\n",
        "            data_samples (Sequence[dict]): A batch of outputs from\n",
        "                the model.\n",
        "        \"\"\"\n",
        "        data_samples = data_samples[0]\n",
        "\n",
        "        result = {\n",
        "            'pred': data_samples['pred_label'][0],\n",
        "            'gt': data_samples['gt_label'][0],\n",
        "            'pd_score': data_samples['pred_score'][1]\n",
        "        }\n",
        "\n",
        "        # save batch result in reuslt list\n",
        "        self.results.append(result)\n",
        "\n",
        "    def compute_metrics(self, results: List):\n",
        "      print(results)\n",
        "\n",
        "      preds = np.concatenate([np.expand_dims(res['pred'], axis=0) for res in results])\n",
        "      gts = np.concatenate([np.expand_dims(res['gt'], axis=0) for res in results])\n",
        "      pred_scores = np.concatenate([np.expand_dims(res['pd_score'], axis=0) for res in results])\n",
        "\n",
        "      # accuracy\n",
        "      acc = (preds == gts).sum() / preds.size\n",
        "\n",
        "      # f1 score & recall\n",
        "      f1 = f1_score(gts, preds, average='binary')\n",
        "      recall = recall_score(gts, preds, average='binary')\n",
        "\n",
        "      # auc & roc\n",
        "      auc = roc_auc_score(gts, pred_scores)\n",
        "      fpr, tpr, thresholds = roc_curve(gts, pred_scores)\n",
        "\n",
        "      # return result\n",
        "      return {\n",
        "        'accuracy': acc,\n",
        "        'f1_score': f1,\n",
        "        'recall': recall,\n",
        "        'auc': auc,\n",
        "        'roc_curve': (fpr, tpr, thresholds)\n",
        "      }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G77GXIOi7CEP"
      },
      "source": [
        "## Using Cropped Frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M2DyGcF-Oav"
      },
      "outputs": [],
      "source": [
        "from mmaction.apis import inference_recognizer, init_recognizer\n",
        "from mmengine import Config\n",
        "from mmengine.runner import set_random_seed\n",
        "\n",
        "cfg = Config.fromfile('./configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow.py')\n",
        "\n",
        "dataset_type = 'RawframeDataset'\n",
        "\n",
        "# Modify dataset type and path\n",
        "cfg.data_root = '../group_project_data/output_yolo3'\n",
        "cfg.data_root_val = '../group_project_data/output_yolo3'\n",
        "cfg.ann_file_train = './Annotations/train.txt'\n",
        "cfg.ann_file_val = './Annotations/val.txt'\n",
        "cfg.ann_file_test = './Annotations/test.txt'\n",
        "\n",
        "# Modify num classes of the model in cls_head\n",
        "cfg.model.cls_head.num_classes = 2\n",
        "cfg.clip_len = 30\n",
        "#modify the clip_len and num_clip\n",
        "\n",
        "# We can use the pre-trained TSN model\n",
        "cfg.load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
        "\n",
        "# Set up working dir to save files and logs.\n",
        "cfg.work_dir = './tutorial_exps'\n",
        "\n",
        "# modify the data_preprocessor\n",
        "cfg.model.backbone.in_channels = 3\n",
        "cfg.model.data_preprocessor.mean = [0.485, 0.456, 0.406]\n",
        "cfg.model.data_preprocessor.std = [0.229, 0.224, 0.225]\n",
        "\n",
        "#rewrite the pipline\n",
        "train_pipeline = [\n",
        "    dict(\n",
        "        type='SampleFrames', clip_len=cfg.clip_len, frame_interval=1, num_clips=1),\n",
        "    dict(type='RawFrameDecode', **cfg.file_client_args),\n",
        "    dict(type='Resize', scale=(-1, 64)),\n",
        "    dict(type='RandomResizedCrop'),\n",
        "    dict(type='Resize', scale=(64, 64), keep_ratio=False),\n",
        "    dict(type='Flip', flip_ratio=0.5),\n",
        "    dict(type='FormatShape', input_format='NCHW'),\n",
        "    dict(type='PackActionInputs')\n",
        "]\n",
        "val_pipeline = [\n",
        "    dict(\n",
        "        type='SampleFrames',\n",
        "        clip_len=cfg.clip_len,\n",
        "        frame_interval=1,\n",
        "        test_mode=True),\n",
        "    dict(type='RawFrameDecode', **cfg.file_client_args),\n",
        "    dict(type='Resize', scale=(-1, 64)),\n",
        "    dict(type='CenterCrop', crop_size=64),\n",
        "    dict(type='FormatShape', input_format='NCHW'),\n",
        "    dict(type='PackActionInputs')\n",
        "]\n",
        "test_pipeline = [\n",
        "    dict(\n",
        "        type='SampleFrames',\n",
        "        clip_len=cfg.clip_len,\n",
        "        frame_interval=1,\n",
        "        test_mode=True),\n",
        "    dict(type='RawFrameDecode'),\n",
        "    dict(type='Resize', scale=(-1, 64)),\n",
        "    dict(type='TenCrop', crop_size=64),\n",
        "    dict(type='FormatShape', input_format='NCHW'),\n",
        "    dict(type='PackActionInputs')\n",
        "]\n",
        "cfg.test_evaluator = [dict(type='MySelfCustomMetric')]\n",
        "cfg.train_pipeline = train_pipeline\n",
        "cfg.val_pipeline = val_pipeline\n",
        "cfg.test_pipeline = test_pipeline\n",
        "\n",
        "frame_modality = 'RGB'\n",
        "filename_template = 'img_{:05d}.jpg'\n",
        "\n",
        "cfg.test_dataloader.dataset.ann_file = cfg.ann_file_test\n",
        "cfg.test_dataloader.dataset.data_prefix.img = cfg.data_root\n",
        "cfg.test_dataloader.dataset.modality = frame_modality\n",
        "cfg.test_dataloader.dataset.filename_tmpl = filename_template\n",
        "cfg.test_dataloader.dataset.pipeline=test_pipeline\n",
        "\n",
        "cfg.train_dataloader.dataset.ann_file = cfg.ann_file_train\n",
        "cfg.train_dataloader.dataset.data_prefix.img = cfg.data_root_val\n",
        "cfg.train_dataloader.dataset.modality = frame_modality\n",
        "cfg.train_dataloader.dataset.filename_tmpl = filename_template\n",
        "cfg.train_dataloader.dataset.pipeline=train_pipeline\n",
        "\n",
        "cfg.val_dataloader.dataset.ann_file = cfg.ann_file_val\n",
        "cfg.val_dataloader.dataset.data_prefix.img  = cfg.data_root_val\n",
        "cfg.val_dataloader.dataset.modality = frame_modality\n",
        "cfg.val_dataloader.dataset.filename_tmpl = filename_template\n",
        "cfg.val_dataloader.dataset.pipeline=val_pipeline\n",
        "\n",
        "# The original learning rate (LR) is set for 8-GPU training.\n",
        "# We divide it by 8 since we only use one GPU.\n",
        "cfg.train_dataloader.batch_size = cfg.train_dataloader.batch_size // 2\n",
        "cfg.val_dataloader.batch_size = cfg.val_dataloader.batch_size // 2\n",
        "cfg.optim_wrapper.optimizer.lr = cfg.optim_wrapper.optimizer.lr / 8 / 4\n",
        "cfg.train_cfg.max_epochs = 10\n",
        "\n",
        "cfg.train_dataloader.num_workers = 2\n",
        "cfg.val_dataloader.num_workers = 2\n",
        "cfg.test_dataloader.num_workers = 2\n",
        "\n",
        "# We can initialize the logger for training and have a look\n",
        "# at the final config used for training\n",
        "# print(f'Config:\\n{cfg.pretty_text}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1Upf_an7YET"
      },
      "source": [
        "## Using Raw Frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5d3aYT17a3O"
      },
      "outputs": [],
      "source": [
        "from mmaction.apis import inference_recognizer, init_recognizer\n",
        "from mmengine import Config\n",
        "from mmengine.runner import set_random_seed\n",
        "\n",
        "cfg_raw = Config.fromfile('./configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow.py')\n",
        "\n",
        "dataset_type = 'RawframeDataset'\n",
        "\n",
        "# Modify dataset type and path\n",
        "cfg_raw.data_root = '../group_project_data/output_frames'\n",
        "cfg_raw.data_root_val = '../group_project_data/output_frames'\n",
        "cfg_raw.ann_file_train = './Annotations/train.txt'\n",
        "cfg_raw.ann_file_val = './Annotations/val.txt'\n",
        "cfg_raw.ann_file_test = './Annotations/test.txt'\n",
        "\n",
        "# Modify num classes of the model in cls_head\n",
        "cfg_raw.model.cls_head.num_classes = 2\n",
        "cfg_raw.clip_len = 30\n",
        "#modify the clip_len and num_clip\n",
        "\n",
        "# We can use the pre-trained TSN model\n",
        "cfg_raw.load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
        "\n",
        "# Set up working dir to save files and logs.\n",
        "cfg_raw.work_dir = './tutorial_exps'\n",
        "\n",
        "# modify the data_preprocessor\n",
        "cfg_raw.model.backbone.in_channels = 3\n",
        "cfg_raw.model.data_preprocessor.mean = [0.485, 0.456, 0.406]\n",
        "cfg_raw.model.data_preprocessor.std = [0.229, 0.224, 0.225]\n",
        "\n",
        "#rewrite the pipline\n",
        "train_pipeline = [\n",
        "    dict(\n",
        "        type='SampleFrames', clip_len=cfg_raw.clip_len, frame_interval=1, num_clips=1),\n",
        "    dict(type='RawFrameDecode', **cfg_raw.file_client_args),\n",
        "    dict(type='Resize', scale=(-1, 64)),\n",
        "    dict(type='RandomResizedCrop'),\n",
        "    dict(type='Resize', scale=(64, 64), keep_ratio=False),\n",
        "    dict(type='Flip', flip_ratio=0.5),\n",
        "    dict(type='FormatShape', input_format='NCHW'),\n",
        "    dict(type='PackActionInputs')\n",
        "]\n",
        "val_pipeline = [\n",
        "    dict(\n",
        "        type='SampleFrames',\n",
        "        clip_len=cfg_raw.clip_len,\n",
        "        frame_interval=1,\n",
        "        test_mode=True),\n",
        "    dict(type='RawFrameDecode', **cfg_raw.file_client_args),\n",
        "    dict(type='Resize', scale=(-1, 64)),\n",
        "    dict(type='CenterCrop', crop_size=64),\n",
        "    dict(type='FormatShape', input_format='NCHW'),\n",
        "    dict(type='PackActionInputs')\n",
        "]\n",
        "test_pipeline = [\n",
        "    dict(\n",
        "        type='SampleFrames',\n",
        "        clip_len=cfg_raw.clip_len,\n",
        "        frame_interval=1,\n",
        "        test_mode=True),\n",
        "    dict(type='RawFrameDecode'),\n",
        "    dict(type='Resize', scale=(-1, 64)),\n",
        "    dict(type='TenCrop', crop_size=64),\n",
        "    dict(type='FormatShape', input_format='NCHW'),\n",
        "    dict(type='PackActionInputs')\n",
        "]\n",
        "cfg_raw.train_pipeline = train_pipeline\n",
        "cfg_raw.val_pipeline = val_pipeline\n",
        "cfg_raw.test_pipeline = test_pipeline\n",
        "\n",
        "cfg_raw.test_evaluator = [dict(type='MySelfCustomMetric')]\n",
        "\n",
        "frame_modality = 'RGB'\n",
        "filename_template = 'img_{:05d}.jpg'\n",
        "\n",
        "cfg_raw.test_dataloader.dataset.ann_file = cfg_raw.ann_file_test\n",
        "cfg_raw.test_dataloader.dataset.data_prefix.img = cfg_raw.data_root\n",
        "cfg_raw.test_dataloader.dataset.modality = frame_modality\n",
        "cfg_raw.test_dataloader.dataset.filename_tmpl = filename_template\n",
        "cfg_raw.test_dataloader.dataset.pipeline=test_pipeline\n",
        "\n",
        "cfg_raw.train_dataloader.dataset.ann_file = cfg_raw.ann_file_train\n",
        "cfg_raw.train_dataloader.dataset.data_prefix.img = cfg_raw.data_root_val\n",
        "cfg_raw.train_dataloader.dataset.modality = frame_modality\n",
        "cfg_raw.train_dataloader.dataset.filename_tmpl = filename_template\n",
        "cfg_raw.train_dataloader.dataset.pipeline=train_pipeline\n",
        "\n",
        "cfg_raw.val_dataloader.dataset.ann_file = cfg_raw.ann_file_val\n",
        "cfg_raw.val_dataloader.dataset.data_prefix.img  = cfg_raw.data_root_val\n",
        "cfg_raw.val_dataloader.dataset.modality = frame_modality\n",
        "cfg_raw.val_dataloader.dataset.filename_tmpl = filename_template\n",
        "cfg_raw.val_dataloader.dataset.pipeline=val_pipeline\n",
        "\n",
        "# The original learning rate (LR) is set for 8-GPU training.\n",
        "# We divide it by 8 since we only use one GPU.\n",
        "cfg_raw.train_dataloader.batch_size = cfg_raw.train_dataloader.batch_size // 2\n",
        "cfg_raw.val_dataloader.batch_size = cfg_raw.val_dataloader.batch_size // 2\n",
        "cfg_raw.optim_wrapper.optimizer.lr = cfg_raw.optim_wrapper.optimizer.lr / 8 / 4\n",
        "cfg_raw.train_cfg.max_epochs = 10\n",
        "\n",
        "cfg_raw.train_dataloader.num_workers = 2\n",
        "cfg_raw.val_dataloader.num_workers = 2\n",
        "cfg_raw.test_dataloader.num_workers = 2\n",
        "\n",
        "# We can initialize the logger for training and have a look\n",
        "# at the final config used for training\n",
        "# print(f'Config:\\n{cfg_raw.pretty_text}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLUcIndIsI_6"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SADOQiEys7E_"
      },
      "source": [
        "## Spilt the DataSet into Train, Validation and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzFs8EV9s4nZ"
      },
      "outputs": [],
      "source": [
        "from pickle import NONE\n",
        "import shutil\n",
        "\n",
        "def split_data_with_oversampling(videos_dict, do_oversampling=False):\n",
        "  print(videos_dict)\n",
        "  type_0_keys = [key for key, value in videos_dict.items() if value['video_type'] == 0]\n",
        "  type_1_keys = [key for key, value in videos_dict.items() if value['video_type'] == 1]\n",
        "\n",
        "  random.shuffle(type_0_keys)\n",
        "  type_1_keys.extend(type_0_keys[2:])\n",
        "  random.shuffle(type_1_keys)\n",
        "\n",
        "  split_idx_1 = int(0.8 * len(type_1_keys))\n",
        "  split_idx_2 = int(0.9 * len(type_1_keys))\n",
        "\n",
        "  # at least each kind of dataset should have at least 1 type 0 videos\n",
        "  train_keys = type_1_keys[:split_idx_1]\n",
        "  train_keys.append(type_0_keys[0])\n",
        "  valid_keys = type_1_keys[split_idx_1:split_idx_2]\n",
        "  train_keys.append(type_0_keys[1])\n",
        "  test_keys = type_1_keys[split_idx_2:]\n",
        "  test_keys.append(type_0_keys[2])\n",
        "\n",
        "  train_folders = [folder for key in train_keys for folder in videos_dict[key]['frame_folders']]\n",
        "  valid_folders = [folder for key in valid_keys for folder in videos_dict[key]['frame_folders']]\n",
        "  test_folders = [folder for key in test_keys for folder in videos_dict[key]['frame_folders']]\n",
        "\n",
        "  if do_oversampling:\n",
        "    type_0_frame_count = sum(len(videos_dict[key]['frame_folders']) for key in type_0_keys)\n",
        "    type_1_frame_count = sum(len(videos_dict[key]['frame_folders']) for key in type_1_keys[:split_idx_1])\n",
        "\n",
        "    if type_0_frame_count < type_1_frame_count:\n",
        "      oversampling_count = type_1_frame_count - type_0_frame_count\n",
        "      oversampled_folders = random.choices([folder for key in type_0_keys for folder in videos_dict[key]['frame_folders']], k=oversampling_count)\n",
        "      train_folders.extend(oversampled_folders)\n",
        "\n",
        "  random.shuffle(train_folders)\n",
        "  return train_folders, valid_folders, test_folders\n",
        "\n",
        "# 使用函数进行数据划分，选择是否进行 oversampling\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "def spilt_dataset_and_create_annotations(yaml_path, annotation_path, do_oversampling):\n",
        "  video_dict = {}\n",
        "  with open(yaml_path, 'r') as file:\n",
        "    video_dict = yaml.safe_load(file)\n",
        "  train_folders, valid_folders, test_folders = split_data_with_oversampling(video_dict, do_oversampling=True)\n",
        "\n",
        "  # 创建 Annotations 文件夹（如果不存在）\n",
        "  if os.path.exists(annotation_path):\n",
        "    shutil.rmtree(annotation_path)\n",
        "\n",
        "  annotations_dir = annotation_path\n",
        "  os.makedirs(annotations_dir, exist_ok=True)\n",
        "\n",
        "  # create annotations\n",
        "  def write_annotations(filename, video_dict, frame_folders):\n",
        "    with open(os.path.join(annotations_dir, filename), 'w') as file:\n",
        "        for folder in frame_folders:\n",
        "          parts = folder.split('_')\n",
        "          video = '_'.join(parts[:-2])\n",
        "          video_type = video_dict[video]['video_type']\n",
        "          file.write(f\"{folder} 30 {video_type}\\n\")\n",
        "\n",
        "  write_annotations('train.txt', video_dict, train_folders)\n",
        "  write_annotations('val.txt', video_dict, valid_folders)\n",
        "  write_annotations('test.txt', video_dict, test_folders)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5On9gz21kGyD"
      },
      "source": [
        "## Create & Set seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq7vNI0WkK50"
      },
      "outputs": [],
      "source": [
        "# seeds\n",
        "import random\n",
        "random_seeds = [10, 42, 50, 66, 70]\n",
        "\n",
        "result_dict = {'OverSampling+Cropped' : None,\n",
        "               'Cropped': None,\n",
        "               'Oversampling': None,\n",
        "               'Baseline': None}\n",
        "\n",
        "\n",
        "#set seeds\n",
        "def set_seeds(seeds):\n",
        "  torch.manual_seed(seeds)\n",
        "\n",
        "  torch.cuda.manual_seed(seeds)\n",
        "  torch.cuda.manual_seed_all(seeds)\n",
        "\n",
        "  random.seed(seeds)\n",
        "  np.random.seed(seeds)\n",
        "\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi5E0A73qnd_"
      },
      "source": [
        "## With Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dDBWkdDRk6oz",
        "outputId": "31e91145-ae18-43de-a70e-1a7486a574db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 15:50:54 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 1165313289\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 1165313289\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 15:50:54 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_yolo3'\n",
            "data_root_val = '../group_project_data/output_yolo3'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 15:51:26 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 15:51:26 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 15:51:26 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 15:51:27 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 15:51:27 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 15:51:27 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 15:51:58 - mmengine - INFO - Epoch(train)  [1][20/41]  lr: 1.5625e-04  eta: 0:10:13  time: 1.5721  data_time: 0.9535  memory: 3712  grad_norm: 6.4938  loss: 0.7067  top1_acc: 0.6250  top5_acc: 1.0000  loss_cls: 0.7067\n",
            "01/10 15:52:27 - mmengine - INFO - Epoch(train)  [1][40/41]  lr: 1.5625e-04  eta: 0:09:19  time: 1.4508  data_time: 0.8349  memory: 3712  grad_norm: 6.0533  loss: 0.6314  top1_acc: 0.7500  top5_acc: 1.0000  loss_cls: 0.6314\n",
            "01/10 15:52:28 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_155054\n",
            "01/10 15:52:28 - mmengine - INFO - Epoch(train)  [1][41/41]  lr: 1.5625e-04  eta: 0:09:08  time: 1.4022  data_time: 0.7910  memory: 3332  grad_norm: 6.0614  loss: 0.6272  top1_acc: 0.7857  top5_acc: 1.0000  loss_cls: 0.6272\n",
            "01/10 15:52:59 - mmengine - INFO - Epoch(train)  [2][20/41]  lr: 1.5625e-04  eta: 0:08:50  time: 1.5835  data_time: 0.9552  memory: 3712  grad_norm: 5.9911  loss: 0.5536  top1_acc: 0.7500  top5_acc: 1.0000  loss_cls: 0.5536\n",
            "01/10 15:53:29 - mmengine - INFO - Epoch(train)  [2][40/41]  lr: 1.5625e-04  eta: 0:08:17  time: 1.4942  data_time: 0.8671  memory: 3712  grad_norm: 5.6134  loss: 0.5085  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.5085\n",
            "01/10 15:53:30 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_155054\n",
            "01/10 15:53:30 - mmengine - INFO - Epoch(train)  [2][41/41]  lr: 1.5625e-04  eta: 0:08:12  time: 1.4185  data_time: 0.7958  memory: 3332  grad_norm: 5.6279  loss: 0.5061  top1_acc: 0.8571  top5_acc: 1.0000  loss_cls: 0.5061\n",
            "01/10 15:54:00 - mmengine - INFO - Epoch(train)  [3][20/41]  lr: 1.5625e-04  eta: 0:07:43  time: 1.5219  data_time: 0.9071  memory: 3712  grad_norm: 6.1416  loss: 0.4737  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4737\n",
            "01/10 15:54:30 - mmengine - INFO - Epoch(train)  [3][40/41]  lr: 1.5625e-04  eta: 0:07:12  time: 1.4786  data_time: 0.8665  memory: 3712  grad_norm: 5.8010  loss: 0.4176  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4176\n",
            "01/10 15:54:30 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_155054\n",
            "01/10 15:54:30 - mmengine - INFO - Epoch(train)  [3][41/41]  lr: 1.5625e-04  eta: 0:07:08  time: 1.4091  data_time: 0.8002  memory: 3332  grad_norm: 5.7735  loss: 0.4133  top1_acc: 0.8571  top5_acc: 1.0000  loss_cls: 0.4133\n",
            "01/10 15:55:01 - mmengine - INFO - Epoch(train)  [4][20/41]  lr: 1.5625e-04  eta: 0:06:40  time: 1.5540  data_time: 0.9338  memory: 3712  grad_norm: 5.7852  loss: 0.3862  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.3862\n",
            "01/10 15:55:30 - mmengine - INFO - Epoch(train)  [4][40/41]  lr: 1.5625e-04  eta: 0:06:08  time: 1.4224  data_time: 0.8006  memory: 3712  grad_norm: 5.0212  loss: 0.3199  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3199\n",
            "01/10 15:55:31 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_155054\n",
            "01/10 15:55:31 - mmengine - INFO - Epoch(train)  [4][41/41]  lr: 1.5625e-04  eta: 0:06:06  time: 1.4148  data_time: 0.7979  memory: 3332  grad_norm: 4.9903  loss: 0.3171  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3171\n",
            "01/10 15:56:00 - mmengine - INFO - Epoch(train)  [5][20/41]  lr: 1.5625e-04  eta: 0:05:35  time: 1.4669  data_time: 0.8392  memory: 3712  grad_norm: 5.5869  loss: 0.3115  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3115\n",
            "01/10 15:56:30 - mmengine - INFO - Epoch(train)  [5][40/41]  lr: 1.5625e-04  eta: 0:05:06  time: 1.5194  data_time: 0.8932  memory: 3712  grad_norm: 5.2678  loss: 0.2863  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2863\n",
            "01/10 15:56:32 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_155054\n",
            "01/10 15:56:32 - mmengine - INFO - Epoch(train)  [5][41/41]  lr: 1.5625e-04  eta: 0:05:05  time: 1.4981  data_time: 0.8757  memory: 3332  grad_norm: 5.3006  loss: 0.2835  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2835\n",
            "01/10 15:56:32 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 15:56:34 - mmengine - INFO - Epoch(val) [5][2/2]    acc/top1: 1.0000  acc/top5: 1.0000  acc/mean1: 1.0000  data_time: 0.6358  time: 0.7534\n",
            "01/10 15:56:35 - mmengine - INFO - The best checkpoint with 1.0000 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 15:57:06 - mmengine - INFO - Epoch(train)  [6][20/41]  lr: 1.5625e-04  eta: 0:04:35  time: 1.4863  data_time: 0.8661  memory: 3712  grad_norm: 5.3466  loss: 0.2685  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2685\n",
            "01/10 15:57:35 - mmengine - INFO - Epoch(train)  [6][40/41]  lr: 1.5625e-04  eta: 0:04:05  time: 1.4421  data_time: 0.8158  memory: 3712  grad_norm: 5.2512  loss: 0.2372  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2372\n",
            "01/10 15:57:37 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_155054\n",
            "01/10 15:57:37 - mmengine - INFO - Epoch(train)  [6][41/41]  lr: 1.5625e-04  eta: 0:04:03  time: 1.3871  data_time: 0.7697  memory: 3332  grad_norm: 5.1708  loss: 0.2313  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2313\n",
            "01/10 15:58:11 - mmengine - INFO - Epoch(train)  [7][20/41]  lr: 1.5625e-04  eta: 0:03:36  time: 1.7053  data_time: 1.0701  memory: 3712  grad_norm: 4.6208  loss: 0.2131  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2131\n",
            "01/10 15:58:39 - mmengine - INFO - Epoch(train)  [7][40/41]  lr: 1.5625e-04  eta: 0:03:05  time: 1.4245  data_time: 0.7995  memory: 3712  grad_norm: 4.5889  loss: 0.1929  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1929\n",
            "01/10 15:58:41 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_155054\n",
            "01/10 15:58:41 - mmengine - INFO - Epoch(train)  [7][41/41]  lr: 1.5625e-04  eta: 0:03:03  time: 1.3819  data_time: 0.7607  memory: 3332  grad_norm: 4.6869  loss: 0.1938  top1_acc: 0.9286  top5_acc: 1.0000  loss_cls: 0.1938\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-25afe340f22d>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_cfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;31m# start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m   \u001b[0mresult_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmengine/runner/runner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1777\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1778\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_run'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmengine/runner/loops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_epochs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decide_current_val_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmengine/runner/loops.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before_train_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os.path as osp\n",
        "import mmengine\n",
        "from mmengine.runner import Runner\n",
        "from mmengine.logging import print_log\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "result_list = []\n",
        "\n",
        "# split the datasets\n",
        "for i in range(len(random_seeds)):\n",
        "  # set seeds\n",
        "  set_seeds(random_seeds[i])\n",
        "\n",
        "  yaml_path = '../group_project_data/sample.yaml'\n",
        "  annotation_path = './Annotations'\n",
        "  spilt_dataset_and_create_annotations(yaml_path, annotation_path, do_oversampling=True)\n",
        "  experiment_annotation_path = f'./experiment-annotations/Cropped_Oversampled/experiment_{i}'\n",
        "  # save current annotations\n",
        "  if osp.exists(experiment_annotation_path):\n",
        "    shutil.rmtree(experiment_annotation_path)\n",
        "  shutil.copytree(annotation_path, experiment_annotation_path)\n",
        "\n",
        "  mmengine.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
        "  # build the runner\n",
        "  runner = Runner.from_cfg(cfg)\n",
        "  # start training\n",
        "  runner.train()\n",
        "  result_list.append(runner.test())\n",
        "\n",
        "result_dict['Oversampling+Cropped'] = result_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzinuSrU00Qw"
      },
      "source": [
        "## Without oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yPBRGIK1VoN",
        "outputId": "2ba860c9-3c86-4301-8e55-3256fe94ba25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 11:50:06 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 1165313289\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 1165313289\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 11:50:06 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_yolo3'\n",
            "data_root_val = '../group_project_data/output_yolo3'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 11:50:07 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 11:50:07 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 11:50:38 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 11:50:38 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 11:50:38 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 11:50:38 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 11:51:12 - mmengine - INFO - Epoch(train)  [1][20/41]  lr: 1.5625e-04  eta: 0:11:10  time: 1.7197  data_time: 1.0478  memory: 3714  grad_norm: 6.4938  loss: 0.7067  top1_acc: 0.6250  top5_acc: 1.0000  loss_cls: 0.7067\n",
            "01/10 11:51:40 - mmengine - INFO - Epoch(train)  [1][40/41]  lr: 1.5625e-04  eta: 0:09:36  time: 1.3954  data_time: 0.7272  memory: 3714  grad_norm: 6.0533  loss: 0.6314  top1_acc: 0.7500  top5_acc: 1.0000  loss_cls: 0.6314\n",
            "01/10 11:51:41 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_115006\n",
            "01/10 11:51:41 - mmengine - INFO - Epoch(train)  [1][41/41]  lr: 1.5625e-04  eta: 0:09:25  time: 1.2812  data_time: 0.6218  memory: 3335  grad_norm: 6.0614  loss: 0.6272  top1_acc: 0.7857  top5_acc: 1.0000  loss_cls: 0.6272\n",
            "01/10 11:52:09 - mmengine - INFO - Epoch(train)  [2][20/41]  lr: 1.5625e-04  eta: 0:08:45  time: 1.4479  data_time: 0.7934  memory: 3714  grad_norm: 5.9911  loss: 0.5536  top1_acc: 0.7500  top5_acc: 1.0000  loss_cls: 0.5536\n",
            "01/10 11:52:40 - mmengine - INFO - Epoch(train)  [2][40/41]  lr: 1.5625e-04  eta: 0:08:15  time: 1.5117  data_time: 0.8550  memory: 3714  grad_norm: 5.6134  loss: 0.5085  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.5085\n",
            "01/10 11:52:40 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_115006\n",
            "01/10 11:52:40 - mmengine - INFO - Epoch(train)  [2][41/41]  lr: 1.5625e-04  eta: 0:08:10  time: 1.4094  data_time: 0.7618  memory: 3335  grad_norm: 5.6279  loss: 0.5061  top1_acc: 0.8571  top5_acc: 1.0000  loss_cls: 0.5061\n",
            "01/10 11:53:12 - mmengine - INFO - Epoch(train)  [3][20/41]  lr: 1.5625e-04  eta: 0:07:44  time: 1.5619  data_time: 0.9000  memory: 3714  grad_norm: 6.1416  loss: 0.4737  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4737\n",
            "01/10 11:53:40 - mmengine - INFO - Epoch(train)  [3][40/41]  lr: 1.5625e-04  eta: 0:07:09  time: 1.4118  data_time: 0.7582  memory: 3714  grad_norm: 5.8010  loss: 0.4176  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4176\n",
            "01/10 11:53:40 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_115006\n",
            "01/10 11:53:40 - mmengine - INFO - Epoch(train)  [3][41/41]  lr: 1.5625e-04  eta: 0:07:06  time: 1.2962  data_time: 0.6492  memory: 3335  grad_norm: 5.7735  loss: 0.4133  top1_acc: 0.8571  top5_acc: 1.0000  loss_cls: 0.4133\n",
            "01/10 11:54:11 - mmengine - INFO - Epoch(train)  [4][20/41]  lr: 1.5625e-04  eta: 0:06:38  time: 1.5394  data_time: 0.8748  memory: 3714  grad_norm: 5.7852  loss: 0.3862  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.3862\n",
            "01/10 11:54:38 - mmengine - INFO - Epoch(train)  [4][40/41]  lr: 1.5625e-04  eta: 0:06:04  time: 1.3515  data_time: 0.6903  memory: 3714  grad_norm: 5.0212  loss: 0.3199  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3199\n",
            "01/10 11:54:40 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_115006\n",
            "01/10 11:54:40 - mmengine - INFO - Epoch(train)  [4][41/41]  lr: 1.5625e-04  eta: 0:06:02  time: 1.3290  data_time: 0.6720  memory: 3335  grad_norm: 4.9903  loss: 0.3171  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3171\n",
            "01/10 11:55:08 - mmengine - INFO - Epoch(train)  [5][20/41]  lr: 1.5625e-04  eta: 0:05:31  time: 1.4054  data_time: 0.7505  memory: 3714  grad_norm: 5.5869  loss: 0.3115  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3115\n",
            "01/10 11:55:39 - mmengine - INFO - Epoch(train)  [5][40/41]  lr: 1.5625e-04  eta: 0:05:04  time: 1.5498  data_time: 0.8787  memory: 3714  grad_norm: 5.2678  loss: 0.2863  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2863\n",
            "01/10 11:55:41 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_115006\n",
            "01/10 11:55:41 - mmengine - INFO - Epoch(train)  [5][41/41]  lr: 1.5625e-04  eta: 0:05:02  time: 1.5058  data_time: 0.8426  memory: 3335  grad_norm: 5.3006  loss: 0.2835  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2835\n",
            "01/10 11:55:41 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 11:55:43 - mmengine - INFO - Epoch(val) [5][2/2]    acc/top1: 1.0000  acc/top5: 1.0000  acc/mean1: 1.0000  data_time: 0.5476  time: 0.6636\n",
            "01/10 11:55:44 - mmengine - INFO - The best checkpoint with 1.0000 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 11:56:14 - mmengine - INFO - Epoch(train)  [6][20/41]  lr: 1.5625e-04  eta: 0:04:33  time: 1.4674  data_time: 0.8000  memory: 3714  grad_norm: 5.3466  loss: 0.2685  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2685\n",
            "01/10 11:56:44 - mmengine - INFO - Epoch(train)  [6][40/41]  lr: 1.5625e-04  eta: 0:04:03  time: 1.4823  data_time: 0.8212  memory: 3714  grad_norm: 5.2512  loss: 0.2372  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2372\n",
            "01/10 11:56:45 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_115006\n",
            "01/10 11:56:45 - mmengine - INFO - Epoch(train)  [6][41/41]  lr: 1.5625e-04  eta: 0:04:01  time: 1.4152  data_time: 0.7582  memory: 3335  grad_norm: 5.1708  loss: 0.2313  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2313\n",
            "01/10 11:57:16 - mmengine - INFO - Epoch(train)  [7][20/41]  lr: 1.5625e-04  eta: 0:03:33  time: 1.5690  data_time: 0.9025  memory: 3714  grad_norm: 4.6208  loss: 0.2131  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2131\n",
            "01/10 11:57:45 - mmengine - INFO - Epoch(train)  [7][40/41]  lr: 1.5625e-04  eta: 0:03:03  time: 1.4295  data_time: 0.7684  memory: 3714  grad_norm: 4.5889  loss: 0.1929  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1929\n",
            "01/10 11:57:46 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_115006\n",
            "01/10 11:57:46 - mmengine - INFO - Epoch(train)  [7][41/41]  lr: 1.5625e-04  eta: 0:03:01  time: 1.3721  data_time: 0.7150  memory: 3335  grad_norm: 4.6869  loss: 0.1938  top1_acc: 0.9286  top5_acc: 1.0000  loss_cls: 0.1938\n",
            "01/10 11:58:16 - mmengine - INFO - Epoch(train)  [8][20/41]  lr: 1.5625e-04  eta: 0:02:32  time: 1.5380  data_time: 0.8788  memory: 3714  grad_norm: 4.6732  loss: 0.1817  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.1817\n",
            "01/10 11:58:45 - mmengine - INFO - Epoch(train)  [8][40/41]  lr: 1.5625e-04  eta: 0:02:02  time: 1.4500  data_time: 0.7924  memory: 3714  grad_norm: 4.3294  loss: 0.1704  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1704\n",
            "01/10 11:58:46 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_115006\n",
            "01/10 11:58:46 - mmengine - INFO - Epoch(train)  [8][41/41]  lr: 1.5625e-04  eta: 0:02:01  time: 1.4171  data_time: 0.7639  memory: 3335  grad_norm: 4.2622  loss: 0.1670  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1670\n",
            "01/10 11:59:18 - mmengine - INFO - Epoch(train)  [9][20/41]  lr: 1.5625e-04  eta: 0:01:31  time: 1.5666  data_time: 0.9049  memory: 3714  grad_norm: 4.8515  loss: 0.1807  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.1807\n",
            "01/10 11:59:46 - mmengine - INFO - Epoch(train)  [9][40/41]  lr: 1.5625e-04  eta: 0:01:02  time: 1.4130  data_time: 0.7514  memory: 3714  grad_norm: 4.7790  loss: 0.1617  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.1617\n",
            "01/10 11:59:46 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_115006\n",
            "01/10 11:59:47 - mmengine - INFO - Epoch(train)  [9][41/41]  lr: 1.5625e-04  eta: 0:01:00  time: 1.4092  data_time: 0.7518  memory: 3335  grad_norm: 4.6582  loss: 0.1568  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1568\n",
            "01/10 12:00:17 - mmengine - INFO - Epoch(train) [10][20/41]  lr: 1.5625e-04  eta: 0:00:31  time: 1.5417  data_time: 0.8707  memory: 3714  grad_norm: 4.5494  loss: 0.1552  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1552\n",
            "01/10 12:00:48 - mmengine - INFO - Epoch(train) [10][40/41]  lr: 1.5625e-04  eta: 0:00:01  time: 1.5337  data_time: 0.8769  memory: 3714  grad_norm: 4.3568  loss: 0.1428  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1428\n",
            "01/10 12:00:49 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_115006\n",
            "01/10 12:00:49 - mmengine - INFO - Epoch(train) [10][41/41]  lr: 1.5625e-04  eta: 0:00:00  time: 1.5196  data_time: 0.8673  memory: 3335  grad_norm: 4.1368  loss: 0.1350  top1_acc: 0.9286  top5_acc: 1.0000  loss_cls: 0.1350\n",
            "01/10 12:00:49 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 12:00:51 - mmengine - INFO - Epoch(val) [10][2/2]    acc/top1: 1.0000  acc/top5: 1.0000  acc/mean1: 1.0000  data_time: 0.3266  time: 0.4002\n",
            "01/10 12:00:53 - mmengine - INFO - Epoch(test) [ 20/109]    eta: 0:00:08  time: 0.0923  data_time: 0.0395  memory: 825  \n",
            "01/10 12:00:55 - mmengine - INFO - Epoch(test) [ 40/109]    eta: 0:00:06  time: 0.0914  data_time: 0.0243  memory: 659  \n",
            "01/10 12:00:57 - mmengine - INFO - Epoch(test) [ 60/109]    eta: 0:00:04  time: 0.0933  data_time: 0.0300  memory: 659  \n",
            "01/10 12:00:59 - mmengine - INFO - Epoch(test) [ 80/109]    eta: 0:00:02  time: 0.0978  data_time: 0.0429  memory: 659  \n",
            "01/10 12:01:00 - mmengine - INFO - Epoch(test) [100/109]    eta: 0:00:00  time: 0.0961  data_time: 0.0450  memory: 659  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9479)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9636)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9798)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9851)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9777)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9754)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9802)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9745)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9784)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9804)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9920)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9843)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9648)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9718)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9632)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9645)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9605)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9622)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9854)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9431)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9776)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9584)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9264)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9362)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9758)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9643)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9844)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9778)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9856)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9733)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9628)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9634)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9700)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9614)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9778)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9802)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9676)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6055)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4457)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4926)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0034)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0028)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0027)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0020)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0114)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0057)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0051)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0019)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0039)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0017)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0016)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0039)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0031)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0125)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0088)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0012)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0047)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0023)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0009)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0028)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0022)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0025)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0013)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0068)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0083)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0153)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0052)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0057)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0076)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0023)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0033)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0024)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0035)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0081)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0023)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0026)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0015)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0023)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0050)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0044)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0099)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0019)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0026)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0054)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0084)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0023)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0060)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0024)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0113)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0072)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0064)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0131)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0058)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0114)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0149)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0028)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0040)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0032)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0039)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0029)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0131)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0060)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0107)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0047)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0030)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0021)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0008)}]\n",
            "01/10 12:01:01 - mmengine - INFO - Epoch(test) [109/109]    SelfCustomMetric/accuracy: 0.9817  SelfCustomMetric/f1_score: 0.9744  SelfCustomMetric/recall: 0.9500  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.   , 0.025, 1.   , 1.   ]), array([1.9920324e+00, 9.9203241e-01, 4.4566023e-01, 8.2085689e-04],\n",
            "      dtype=float32))  data_time: 0.0336  time: 0.0932\n",
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 12:01:01 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 1608637542\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 1608637542\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 12:01:02 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_yolo3'\n",
            "data_root_val = '../group_project_data/output_yolo3'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 12:01:02 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 12:01:02 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 12:01:33 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 12:01:33 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 12:01:33 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 12:01:33 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 12:02:05 - mmengine - INFO - Epoch(train)  [1][20/38]  lr: 1.5625e-04  eta: 0:09:39  time: 1.6101  data_time: 0.9426  memory: 3710  grad_norm: 6.5596  loss: 0.7006  top1_acc: 0.4375  top5_acc: 1.0000  loss_cls: 0.7006\n",
            "01/10 12:02:31 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_120101\n",
            "01/10 12:02:31 - mmengine - INFO - Epoch(train)  [1][38/38]  lr: 1.5625e-04  eta: 0:08:37  time: 1.4538  data_time: 0.7929  memory: 3710  grad_norm: 6.4098  loss: 0.6318  top1_acc: 0.6154  top5_acc: 1.0000  loss_cls: 0.6318\n",
            "01/10 12:03:01 - mmengine - INFO - Epoch(train)  [2][20/38]  lr: 1.5625e-04  eta: 0:08:05  time: 1.4958  data_time: 0.8387  memory: 3710  grad_norm: 5.6052  loss: 0.5490  top1_acc: 0.8125  top5_acc: 1.0000  loss_cls: 0.5490\n",
            "01/10 12:03:28 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_120101\n",
            "01/10 12:03:28 - mmengine - INFO - Epoch(train)  [2][38/38]  lr: 1.5625e-04  eta: 0:07:39  time: 1.4630  data_time: 0.8098  memory: 3710  grad_norm: 5.5350  loss: 0.4738  top1_acc: 0.6923  top5_acc: 1.0000  loss_cls: 0.4738\n",
            "01/10 12:03:58 - mmengine - INFO - Epoch(train)  [3][20/38]  lr: 1.5625e-04  eta: 0:07:08  time: 1.4970  data_time: 0.8388  memory: 3710  grad_norm: 5.8572  loss: 0.4400  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.4400\n",
            "01/10 12:04:26 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_120101\n",
            "01/10 12:04:26 - mmengine - INFO - Epoch(train)  [3][38/38]  lr: 1.5625e-04  eta: 0:06:42  time: 1.4720  data_time: 0.8213  memory: 3710  grad_norm: 4.9845  loss: 0.3709  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3709\n",
            "01/10 12:04:57 - mmengine - INFO - Epoch(train)  [4][20/38]  lr: 1.5625e-04  eta: 0:06:14  time: 1.5741  data_time: 0.9086  memory: 3710  grad_norm: 4.9014  loss: 0.3257  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.3257\n",
            "01/10 12:05:25 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_120101\n",
            "01/10 12:05:25 - mmengine - INFO - Epoch(train)  [4][38/38]  lr: 1.5625e-04  eta: 0:05:48  time: 1.5247  data_time: 0.8621  memory: 3710  grad_norm: 5.5851  loss: 0.3248  top1_acc: 0.8462  top5_acc: 1.0000  loss_cls: 0.3248\n",
            "01/10 12:05:57 - mmengine - INFO - Epoch(train)  [5][20/38]  lr: 1.5625e-04  eta: 0:05:18  time: 1.5639  data_time: 0.8973  memory: 3710  grad_norm: 5.1103  loss: 0.2854  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.2854\n",
            "01/10 12:06:21 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_120101\n",
            "01/10 12:06:21 - mmengine - INFO - Epoch(train)  [5][38/38]  lr: 1.5625e-04  eta: 0:04:47  time: 1.4229  data_time: 0.7694  memory: 3710  grad_norm: 5.1822  loss: 0.2738  top1_acc: 0.9231  top5_acc: 1.0000  loss_cls: 0.2738\n",
            "01/10 12:06:21 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 12:06:24 - mmengine - INFO - Epoch(val) [5][4/4]    acc/top1: 0.6731  acc/top5: 1.0000  acc/mean1: 0.3365  data_time: 0.3458  time: 0.4637\n",
            "01/10 12:06:25 - mmengine - INFO - The best checkpoint with 0.6731 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 12:07:00 - mmengine - INFO - Epoch(train)  [6][20/38]  lr: 1.5625e-04  eta: 0:04:19  time: 1.6304  data_time: 0.9662  memory: 3710  grad_norm: 4.8919  loss: 0.2452  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2452\n",
            "01/10 12:07:25 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_120101\n",
            "01/10 12:07:25 - mmengine - INFO - Epoch(train)  [6][38/38]  lr: 1.5625e-04  eta: 0:03:50  time: 1.3807  data_time: 0.7188  memory: 3710  grad_norm: 5.0239  loss: 0.2258  top1_acc: 0.8462  top5_acc: 1.0000  loss_cls: 0.2258\n",
            "01/10 12:07:57 - mmengine - INFO - Epoch(train)  [7][20/38]  lr: 1.5625e-04  eta: 0:03:21  time: 1.6168  data_time: 0.9550  memory: 3710  grad_norm: 4.9143  loss: 0.2152  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2152\n",
            "01/10 12:08:21 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_120101\n",
            "01/10 12:08:21 - mmengine - INFO - Epoch(train)  [7][38/38]  lr: 1.5625e-04  eta: 0:02:52  time: 1.2940  data_time: 0.6441  memory: 3710  grad_norm: 4.3544  loss: 0.1793  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1793\n",
            "01/10 12:08:53 - mmengine - INFO - Epoch(train)  [8][20/38]  lr: 1.5625e-04  eta: 0:02:22  time: 1.6090  data_time: 0.9340  memory: 3710  grad_norm: 4.7306  loss: 0.1854  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1854\n",
            "01/10 12:09:19 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_120101\n",
            "01/10 12:09:19 - mmengine - INFO - Epoch(train)  [8][38/38]  lr: 1.5625e-04  eta: 0:01:54  time: 1.4440  data_time: 0.7858  memory: 3710  grad_norm: 4.1740  loss: 0.1588  top1_acc: 0.9231  top5_acc: 1.0000  loss_cls: 0.1588\n",
            "01/10 12:09:51 - mmengine - INFO - Epoch(train)  [9][20/38]  lr: 1.5625e-04  eta: 0:01:25  time: 1.6177  data_time: 0.9629  memory: 3710  grad_norm: 3.6962  loss: 0.1324  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1324\n",
            "01/10 12:10:17 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_120101\n",
            "01/10 12:10:17 - mmengine - INFO - Epoch(train)  [9][38/38]  lr: 1.5625e-04  eta: 0:00:57  time: 1.4074  data_time: 0.7593  memory: 3710  grad_norm: 4.1867  loss: 0.1404  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1404\n",
            "01/10 12:10:49 - mmengine - INFO - Epoch(train) [10][20/38]  lr: 1.5625e-04  eta: 0:00:27  time: 1.6068  data_time: 0.9466  memory: 3710  grad_norm: 3.8256  loss: 0.1261  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1261\n",
            "01/10 12:11:14 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_120101\n",
            "01/10 12:11:14 - mmengine - INFO - Epoch(train) [10][38/38]  lr: 1.5625e-04  eta: 0:00:00  time: 1.4052  data_time: 0.7515  memory: 3710  grad_norm: 4.5960  loss: 0.1558  top1_acc: 0.8462  top5_acc: 1.0000  loss_cls: 0.1558\n",
            "01/10 12:11:14 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 12:11:17 - mmengine - INFO - Epoch(val) [10][4/4]    acc/top1: 0.7308  acc/top5: 1.0000  acc/mean1: 0.3654  data_time: 0.2609  time: 0.3614\n",
            "01/10 12:11:17 - mmengine - INFO - The previous best checkpoint /content/mmaction2/tutorial_exps/best_acc_top1_epoch_5.pth is removed\n",
            "01/10 12:11:17 - mmengine - INFO - The best checkpoint with 0.7308 acc/top1 at 10 epoch is saved to best_acc_top1_epoch_10.pth.\n",
            "01/10 12:11:21 - mmengine - INFO - Epoch(test) [ 20/146]    eta: 0:00:11  time: 0.0878  data_time: 0.0121  memory: 820  \n",
            "01/10 12:11:26 - mmengine - INFO - Epoch(test) [ 40/146]    eta: 0:00:17  time: 0.2454  data_time: 0.1829  memory: 654  \n",
            "01/10 12:11:28 - mmengine - INFO - Epoch(test) [ 60/146]    eta: 0:00:12  time: 0.1000  data_time: 0.0310  memory: 654  \n",
            "01/10 12:11:29 - mmengine - INFO - Epoch(test) [ 80/146]    eta: 0:00:08  time: 0.0806  data_time: 0.0052  memory: 654  \n",
            "01/10 12:11:31 - mmengine - INFO - Epoch(test) [100/146]    eta: 0:00:05  time: 0.0799  data_time: 0.0058  memory: 654  \n",
            "01/10 12:11:33 - mmengine - INFO - Epoch(test) [120/146]    eta: 0:00:02  time: 0.0809  data_time: 0.0101  memory: 654  \n",
            "01/10 12:11:34 - mmengine - INFO - Epoch(test) [140/146]    eta: 0:00:00  time: 0.0809  data_time: 0.0095  memory: 654  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9302)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9219)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9239)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8898)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9107)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8985)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9177)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8784)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8855)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9512)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9352)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8683)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8948)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8709)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9354)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8087)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9578)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9787)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9420)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9040)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8482)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9070)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9284)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9679)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9346)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9382)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9239)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9466)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8665)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9050)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8569)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7769)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.2710)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.2992)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.2312)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0933)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0961)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.2973)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.4898)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.2906)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.2551)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9152)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7982)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7750)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8664)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8093)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8976)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8425)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0086)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0081)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0130)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0187)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0125)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0112)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0091)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0034)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0046)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0046)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0058)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0038)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0057)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0114)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0038)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0067)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0069)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0056)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0046)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0027)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0117)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0062)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0154)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0262)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0145)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0108)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0101)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0079)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0073)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0054)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0094)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0103)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0130)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0106)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0078)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0090)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0238)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0124)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0074)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0165)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0100)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0086)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0069)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0093)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0085)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0088)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0107)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0100)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0120)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0080)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0060)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0083)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0060)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0107)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0111)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0119)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0190)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0159)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0116)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0101)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0122)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0110)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0049)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0101)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0192)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0098)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0141)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0052)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0140)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0062)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0162)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0113)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0144)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0129)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0201)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0208)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0251)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0080)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0085)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0102)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0091)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0126)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0100)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0085)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0109)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0172)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0149)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0176)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0210)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0104)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0205)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0133)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0144)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0535)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0226)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0321)}]\n",
            "01/10 12:11:35 - mmengine - INFO - Epoch(test) [146/146]    SelfCustomMetric/accuracy: 1.0000  SelfCustomMetric/f1_score: 1.0000  SelfCustomMetric/recall: 1.0000  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.        , 0.02564103, 1.        , 1.        ]), array([1.9787033 , 0.9787033 , 0.7749559 , 0.00270668], dtype=float32))  data_time: 0.0353  time: 0.1068\n",
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 12:11:35 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 2124297904\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 2124297904\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 12:11:35 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_yolo3'\n",
            "data_root_val = '../group_project_data/output_yolo3'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 12:11:36 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 12:11:36 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 12:12:07 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 12:12:07 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 12:12:07 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 12:12:07 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 12:12:34 - mmengine - INFO - Epoch(train)  [1][20/41]  lr: 1.5625e-04  eta: 0:08:40  time: 1.3342  data_time: 0.6801  memory: 3713  grad_norm: 6.0673  loss: 0.6540  top1_acc: 0.5000  top5_acc: 1.0000  loss_cls: 0.6540\n",
            "01/10 12:13:02 - mmengine - INFO - Epoch(train)  [1][40/41]  lr: 1.5625e-04  eta: 0:08:33  time: 1.4431  data_time: 0.7769  memory: 3713  grad_norm: 6.4963  loss: 0.6107  top1_acc: 0.7500  top5_acc: 1.0000  loss_cls: 0.6107\n",
            "01/10 12:13:03 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_121135\n",
            "01/10 12:13:03 - mmengine - INFO - Epoch(train)  [1][41/41]  lr: 1.5625e-04  eta: 0:08:24  time: 1.3896  data_time: 0.7328  memory: 2723  grad_norm: 6.4997  loss: 0.6083  top1_acc: 0.9091  top5_acc: 1.0000  loss_cls: 0.6083\n",
            "01/10 12:13:32 - mmengine - INFO - Epoch(train)  [2][20/41]  lr: 1.5625e-04  eta: 0:08:06  time: 1.4493  data_time: 0.7929  memory: 3713  grad_norm: 5.8621  loss: 0.5503  top1_acc: 0.6250  top5_acc: 1.0000  loss_cls: 0.5503\n",
            "01/10 12:13:57 - mmengine - INFO - Epoch(train)  [2][40/41]  lr: 1.5625e-04  eta: 0:07:29  time: 1.2783  data_time: 0.6258  memory: 3713  grad_norm: 5.6174  loss: 0.4840  top1_acc: 0.6250  top5_acc: 1.0000  loss_cls: 0.4840\n",
            "01/10 12:13:58 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_121135\n",
            "01/10 12:13:58 - mmengine - INFO - Epoch(train)  [2][41/41]  lr: 1.5625e-04  eta: 0:07:24  time: 1.2738  data_time: 0.6306  memory: 2723  grad_norm: 5.6599  loss: 0.4858  top1_acc: 0.9091  top5_acc: 1.0000  loss_cls: 0.4858\n",
            "01/10 12:14:28 - mmengine - INFO - Epoch(train)  [3][20/41]  lr: 1.5625e-04  eta: 0:07:05  time: 1.4814  data_time: 0.8227  memory: 3713  grad_norm: 5.7470  loss: 0.4467  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.4467\n",
            "01/10 12:14:53 - mmengine - INFO - Epoch(train)  [3][40/41]  lr: 1.5625e-04  eta: 0:06:32  time: 1.2807  data_time: 0.6216  memory: 3713  grad_norm: 5.3456  loss: 0.3932  top1_acc: 0.7500  top5_acc: 1.0000  loss_cls: 0.3932\n",
            "01/10 12:14:54 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_121135\n",
            "01/10 12:14:54 - mmengine - INFO - Epoch(train)  [3][41/41]  lr: 1.5625e-04  eta: 0:06:29  time: 1.1794  data_time: 0.5337  memory: 2723  grad_norm: 5.5483  loss: 0.3992  top1_acc: 0.8182  top5_acc: 1.0000  loss_cls: 0.3992\n",
            "01/10 12:15:22 - mmengine - INFO - Epoch(train)  [4][20/41]  lr: 1.5625e-04  eta: 0:06:04  time: 1.4282  data_time: 0.7650  memory: 3713  grad_norm: 5.0627  loss: 0.3586  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3586\n",
            "01/10 12:15:50 - mmengine - INFO - Epoch(train)  [4][40/41]  lr: 1.5625e-04  eta: 0:05:37  time: 1.3666  data_time: 0.7092  memory: 3713  grad_norm: 4.9460  loss: 0.3233  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.3233\n",
            "01/10 12:15:50 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_121135\n",
            "01/10 12:15:50 - mmengine - INFO - Epoch(train)  [4][41/41]  lr: 1.5625e-04  eta: 0:05:34  time: 1.3080  data_time: 0.6605  memory: 2723  grad_norm: 4.9812  loss: 0.3227  top1_acc: 0.9091  top5_acc: 1.0000  loss_cls: 0.3227\n",
            "01/10 12:16:20 - mmengine - INFO - Epoch(train)  [5][20/41]  lr: 1.5625e-04  eta: 0:05:10  time: 1.4801  data_time: 0.8223  memory: 3713  grad_norm: 5.1918  loss: 0.3006  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3006\n",
            "01/10 12:16:45 - mmengine - INFO - Epoch(train)  [5][40/41]  lr: 1.5625e-04  eta: 0:04:40  time: 1.2652  data_time: 0.6070  memory: 3713  grad_norm: 4.8244  loss: 0.2667  top1_acc: 0.8125  top5_acc: 1.0000  loss_cls: 0.2667\n",
            "01/10 12:16:45 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_121135\n",
            "01/10 12:16:46 - mmengine - INFO - Epoch(train)  [5][41/41]  lr: 1.5625e-04  eta: 0:04:38  time: 1.2553  data_time: 0.6070  memory: 2723  grad_norm: 4.9882  loss: 0.2728  top1_acc: 0.7273  top5_acc: 1.0000  loss_cls: 0.2728\n",
            "01/10 12:16:46 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 12:16:47 - mmengine - INFO - Epoch(val) [5][3/3]    acc/top1: 0.6944  acc/top5: 1.0000  acc/mean1: 0.3472  data_time: 0.2727  time: 0.3690\n",
            "01/10 12:16:48 - mmengine - INFO - The best checkpoint with 0.6944 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 12:17:18 - mmengine - INFO - Epoch(train)  [6][20/41]  lr: 1.5625e-04  eta: 0:04:11  time: 1.3523  data_time: 0.6888  memory: 3713  grad_norm: 4.9491  loss: 0.2475  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2475\n",
            "01/10 12:17:45 - mmengine - INFO - Epoch(train)  [6][40/41]  lr: 1.5625e-04  eta: 0:03:43  time: 1.3344  data_time: 0.6775  memory: 3713  grad_norm: 5.2614  loss: 0.2358  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.2358\n",
            "01/10 12:17:46 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_121135\n",
            "01/10 12:17:46 - mmengine - INFO - Epoch(train)  [6][41/41]  lr: 1.5625e-04  eta: 0:03:42  time: 1.3633  data_time: 0.7168  memory: 2723  grad_norm: 5.2962  loss: 0.2347  top1_acc: 0.7273  top5_acc: 1.0000  loss_cls: 0.2347\n",
            "01/10 12:18:16 - mmengine - INFO - Epoch(train)  [7][20/41]  lr: 1.5625e-04  eta: 0:03:16  time: 1.4749  data_time: 0.8146  memory: 3713  grad_norm: 4.2785  loss: 0.1900  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1900\n",
            "01/10 12:18:42 - mmengine - INFO - Epoch(train)  [7][40/41]  lr: 1.5625e-04  eta: 0:02:48  time: 1.2911  data_time: 0.6335  memory: 3713  grad_norm: 4.6179  loss: 0.1938  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1938\n",
            "01/10 12:18:43 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_121135\n",
            "01/10 12:18:43 - mmengine - INFO - Epoch(train)  [7][41/41]  lr: 1.5625e-04  eta: 0:02:47  time: 1.3052  data_time: 0.6579  memory: 2723  grad_norm: 4.6645  loss: 0.1940  top1_acc: 0.9091  top5_acc: 1.0000  loss_cls: 0.1940\n",
            "01/10 12:19:10 - mmengine - INFO - Epoch(train)  [8][20/41]  lr: 1.5625e-04  eta: 0:02:20  time: 1.3787  data_time: 0.7264  memory: 3713  grad_norm: 4.8004  loss: 0.1904  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1904\n",
            "01/10 12:19:37 - mmengine - INFO - Epoch(train)  [8][40/41]  lr: 1.5625e-04  eta: 0:01:52  time: 1.3437  data_time: 0.6889  memory: 3713  grad_norm: 4.5607  loss: 0.1679  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1679\n",
            "01/10 12:19:38 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_121135\n",
            "01/10 12:19:38 - mmengine - INFO - Epoch(train)  [8][41/41]  lr: 1.5625e-04  eta: 0:01:51  time: 1.3344  data_time: 0.6893  memory: 2723  grad_norm: 4.7563  loss: 0.1723  top1_acc: 0.9091  top5_acc: 1.0000  loss_cls: 0.1723\n",
            "01/10 12:20:06 - mmengine - INFO - Epoch(train)  [9][20/41]  lr: 1.5625e-04  eta: 0:01:24  time: 1.4314  data_time: 0.7763  memory: 3713  grad_norm: 3.7594  loss: 0.1337  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1337\n",
            "01/10 12:20:33 - mmengine - INFO - Epoch(train)  [9][40/41]  lr: 1.5625e-04  eta: 0:00:57  time: 1.3540  data_time: 0.6926  memory: 3713  grad_norm: 4.1993  loss: 0.1399  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1399\n",
            "01/10 12:20:34 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_121135\n",
            "01/10 12:20:34 - mmengine - INFO - Epoch(train)  [9][41/41]  lr: 1.5625e-04  eta: 0:00:55  time: 1.3439  data_time: 0.6923  memory: 2723  grad_norm: 4.2528  loss: 0.1414  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1414\n",
            "01/10 12:21:04 - mmengine - INFO - Epoch(train) [10][20/41]  lr: 1.5625e-04  eta: 0:00:28  time: 1.4887  data_time: 0.8242  memory: 3713  grad_norm: 3.1669  loss: 0.1011  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1011\n",
            "01/10 12:21:28 - mmengine - INFO - Epoch(train) [10][40/41]  lr: 1.5625e-04  eta: 0:00:01  time: 1.2148  data_time: 0.5623  memory: 3713  grad_norm: 4.8324  loss: 0.1559  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1559\n",
            "01/10 12:21:28 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_121135\n",
            "01/10 12:21:28 - mmengine - INFO - Epoch(train) [10][41/41]  lr: 1.5625e-04  eta: 0:00:00  time: 1.2045  data_time: 0.5616  memory: 2723  grad_norm: 4.5908  loss: 0.1467  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1467\n",
            "01/10 12:21:28 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 12:21:31 - mmengine - INFO - Epoch(val) [10][3/3]    acc/top1: 0.7222  acc/top5: 1.0000  acc/mean1: 0.3611  data_time: 0.3146  time: 0.4096\n",
            "01/10 12:21:31 - mmengine - INFO - The previous best checkpoint /content/mmaction2/tutorial_exps/best_acc_top1_epoch_5.pth is removed\n",
            "01/10 12:21:32 - mmengine - INFO - The best checkpoint with 0.7222 acc/top1 at 10 epoch is saved to best_acc_top1_epoch_10.pth.\n",
            "01/10 12:21:38 - mmengine - INFO - Epoch(test) [ 20/140]    eta: 0:00:22  time: 0.1852  data_time: 0.1338  memory: 823  \n",
            "01/10 12:21:42 - mmengine - INFO - Epoch(test) [ 40/140]    eta: 0:00:18  time: 0.1882  data_time: 0.1424  memory: 657  \n",
            "01/10 12:21:45 - mmengine - INFO - Epoch(test) [ 60/140]    eta: 0:00:15  time: 0.1973  data_time: 0.1494  memory: 657  \n",
            "01/10 12:21:49 - mmengine - INFO - Epoch(test) [ 80/140]    eta: 0:00:11  time: 0.1765  data_time: 0.1169  memory: 657  \n",
            "01/10 12:21:51 - mmengine - INFO - Epoch(test) [100/140]    eta: 0:00:06  time: 0.0811  data_time: 0.0069  memory: 657  \n",
            "01/10 12:21:52 - mmengine - INFO - Epoch(test) [120/140]    eta: 0:00:03  time: 0.0803  data_time: 0.0055  memory: 657  \n",
            "01/10 12:21:54 - mmengine - INFO - Epoch(test) [140/140]    eta: 0:00:00  time: 0.0798  data_time: 0.0044  memory: 657  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9980)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9947)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9612)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9977)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9982)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9567)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9849)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9959)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9861)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9708)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9819)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9963)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9892)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9897)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9902)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9962)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9920)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9782)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9817)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9909)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9816)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9989)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9963)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9931)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9443)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9864)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9733)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9946)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9926)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9973)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9690)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9966)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9793)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9769)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9952)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9909)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9852)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9960)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9876)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9757)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9908)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9805)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9839)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9926)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9873)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9718)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9541)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9908)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9983)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9856)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9940)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6613)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6425)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4910)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7524)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6858)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8601)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6648)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9199)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9685)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9700)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6803)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9116)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8604)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9445)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9380)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9474)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9562)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9701)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9795)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9780)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0015)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0031)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0027)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0023)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0135)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0054)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0037)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0019)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0017)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0007)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0006)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0042)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0305)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0094)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0033)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0066)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0031)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0005)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0025)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0049)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0016)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0007)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0048)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0093)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0121)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0058)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0026)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0078)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0028)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0041)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0018)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0049)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0118)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0035)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0034)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0027)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0032)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0039)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0025)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0177)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0029)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0024)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0087)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0057)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0110)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0029)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0029)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0014)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0155)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0078)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0071)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0213)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0115)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0225)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0232)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0024)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0022)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0019)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0027)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0028)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0122)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0078)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0236)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0053)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0034)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0015)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0009)}]\n",
            "01/10 12:21:54 - mmengine - INFO - Epoch(test) [140/140]    SelfCustomMetric/accuracy: 0.9929  SelfCustomMetric/f1_score: 0.9929  SelfCustomMetric/recall: 0.9859  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.        , 0.01408451, 1.        , 1.        ]), array([1.9988961e+00, 9.9889612e-01, 4.9098918e-01, 5.0193752e-04],\n",
            "      dtype=float32))  data_time: 0.0799  time: 0.1412\n",
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 12:21:54 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 662660116\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 662660116\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 12:21:54 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_yolo3'\n",
            "data_root_val = '../group_project_data/output_yolo3'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 12:21:55 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 12:21:55 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 12:22:26 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 12:22:26 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 12:22:26 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 12:22:26 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 12:23:02 - mmengine - INFO - Epoch(train)  [1][20/36]  lr: 1.5625e-04  eta: 0:10:21  time: 1.8272  data_time: 1.1576  memory: 3709  grad_norm: 6.3067  loss: 0.6726  top1_acc: 0.6875  top5_acc: 1.0000  loss_cls: 0.6726\n",
            "01/10 12:23:23 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_122154\n",
            "01/10 12:23:23 - mmengine - INFO - Epoch(train)  [1][36/36]  lr: 1.5625e-04  eta: 0:08:37  time: 1.4147  data_time: 0.7438  memory: 3709  grad_norm: 6.3105  loss: 0.6444  top1_acc: 0.7500  top5_acc: 1.0000  loss_cls: 0.6444\n",
            "01/10 12:23:54 - mmengine - INFO - Epoch(train)  [2][20/36]  lr: 1.5625e-04  eta: 0:08:00  time: 1.5530  data_time: 0.8930  memory: 3709  grad_norm: 5.6177  loss: 0.5594  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.5594\n",
            "01/10 12:24:20 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_122154\n",
            "01/10 12:24:20 - mmengine - INFO - Epoch(train)  [2][36/36]  lr: 1.5625e-04  eta: 0:07:35  time: 1.5551  data_time: 0.9017  memory: 3709  grad_norm: 5.4373  loss: 0.5203  top1_acc: 0.6250  top5_acc: 1.0000  loss_cls: 0.5203\n",
            "01/10 12:24:51 - mmengine - INFO - Epoch(train)  [3][20/36]  lr: 1.5625e-04  eta: 0:07:02  time: 1.5462  data_time: 0.8810  memory: 3709  grad_norm: 5.6476  loss: 0.4897  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4897\n",
            "01/10 12:25:17 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_122154\n",
            "01/10 12:25:17 - mmengine - INFO - Epoch(train)  [3][36/36]  lr: 1.5625e-04  eta: 0:06:39  time: 1.6067  data_time: 0.9400  memory: 3709  grad_norm: 5.5729  loss: 0.4426  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4426\n",
            "01/10 12:25:49 - mmengine - INFO - Epoch(train)  [4][20/36]  lr: 1.5625e-04  eta: 0:06:08  time: 1.6074  data_time: 0.9428  memory: 3709  grad_norm: 5.3112  loss: 0.4079  top1_acc: 0.8125  top5_acc: 1.0000  loss_cls: 0.4079\n",
            "01/10 12:26:15 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_122154\n",
            "01/10 12:26:15 - mmengine - INFO - Epoch(train)  [4][36/36]  lr: 1.5625e-04  eta: 0:05:43  time: 1.4880  data_time: 0.8225  memory: 3709  grad_norm: 5.0322  loss: 0.3660  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3660\n",
            "01/10 12:26:47 - mmengine - INFO - Epoch(train)  [5][20/36]  lr: 1.5625e-04  eta: 0:05:11  time: 1.5940  data_time: 0.9346  memory: 3709  grad_norm: 4.9293  loss: 0.3300  top1_acc: 0.6250  top5_acc: 1.0000  loss_cls: 0.3300\n",
            "01/10 12:27:12 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_122154\n",
            "01/10 12:27:12 - mmengine - INFO - Epoch(train)  [5][36/36]  lr: 1.5625e-04  eta: 0:04:45  time: 1.4856  data_time: 0.8250  memory: 3709  grad_norm: 5.2521  loss: 0.3281  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.3281\n",
            "01/10 12:27:12 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 12:27:13 - mmengine - INFO - Epoch(val) [5][2/2]    acc/top1: 1.0000  acc/top5: 1.0000  acc/mean1: 1.0000  data_time: 0.2459  time: 0.3190\n",
            "01/10 12:27:14 - mmengine - INFO - The best checkpoint with 1.0000 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 12:27:51 - mmengine - INFO - Epoch(train)  [6][20/36]  lr: 1.5625e-04  eta: 0:04:15  time: 1.6698  data_time: 1.0020  memory: 3709  grad_norm: 4.9575  loss: 0.2925  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.2925\n",
            "01/10 12:28:17 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_122154\n",
            "01/10 12:28:17 - mmengine - INFO - Epoch(train)  [6][36/36]  lr: 1.5625e-04  eta: 0:03:49  time: 1.6043  data_time: 0.9413  memory: 3709  grad_norm: 5.0433  loss: 0.2723  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2723\n",
            "01/10 12:28:49 - mmengine - INFO - Epoch(train)  [7][20/36]  lr: 1.5625e-04  eta: 0:03:18  time: 1.6365  data_time: 0.9684  memory: 3709  grad_norm: 4.8593  loss: 0.2663  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2663\n",
            "01/10 12:29:13 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_122154\n",
            "01/10 12:29:13 - mmengine - INFO - Epoch(train)  [7][36/36]  lr: 1.5625e-04  eta: 0:02:52  time: 1.5858  data_time: 0.9259  memory: 3709  grad_norm: 4.7387  loss: 0.2476  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2476\n",
            "01/10 12:29:46 - mmengine - INFO - Epoch(train)  [8][20/36]  lr: 1.5625e-04  eta: 0:02:20  time: 1.6117  data_time: 0.9383  memory: 3709  grad_norm: 4.9322  loss: 0.2343  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2343\n",
            "01/10 12:30:11 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_122154\n",
            "01/10 12:30:11 - mmengine - INFO - Epoch(train)  [8][36/36]  lr: 1.5625e-04  eta: 0:01:54  time: 1.5461  data_time: 0.8875  memory: 3709  grad_norm: 5.2881  loss: 0.2321  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2321\n",
            "01/10 12:30:42 - mmengine - INFO - Epoch(train)  [9][20/36]  lr: 1.5625e-04  eta: 0:01:22  time: 1.5839  data_time: 0.9205  memory: 3709  grad_norm: 4.1107  loss: 0.1818  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1818\n",
            "01/10 12:31:06 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_122154\n",
            "01/10 12:31:06 - mmengine - INFO - Epoch(train)  [9][36/36]  lr: 1.5625e-04  eta: 0:00:57  time: 1.5692  data_time: 0.9085  memory: 3709  grad_norm: 4.0360  loss: 0.1747  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1747\n",
            "01/10 12:31:40 - mmengine - INFO - Epoch(train) [10][20/36]  lr: 1.5625e-04  eta: 0:00:25  time: 1.6596  data_time: 0.9959  memory: 3709  grad_norm: 4.1785  loss: 0.1612  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1612\n",
            "01/10 12:32:04 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_122154\n",
            "01/10 12:32:04 - mmengine - INFO - Epoch(train) [10][36/36]  lr: 1.5625e-04  eta: 0:00:00  time: 1.5127  data_time: 0.8488  memory: 3709  grad_norm: 4.3653  loss: 0.1616  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.1616\n",
            "01/10 12:32:04 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 12:32:06 - mmengine - INFO - Epoch(val) [10][2/2]    acc/top1: 1.0000  acc/top5: 1.0000  acc/mean1: 1.0000  data_time: 0.1425  time: 0.1976\n",
            "01/10 12:32:09 - mmengine - INFO - Epoch(test) [ 20/151]    eta: 0:00:21  time: 0.1621  data_time: 0.1022  memory: 820  \n",
            "01/10 12:32:12 - mmengine - INFO - Epoch(test) [ 40/151]    eta: 0:00:15  time: 0.1106  data_time: 0.0571  memory: 654  \n",
            "01/10 12:32:14 - mmengine - INFO - Epoch(test) [ 60/151]    eta: 0:00:11  time: 0.1030  data_time: 0.0496  memory: 654  \n",
            "01/10 12:32:15 - mmengine - INFO - Epoch(test) [ 80/151]    eta: 0:00:08  time: 0.0863  data_time: 0.0197  memory: 654  \n",
            "01/10 12:32:17 - mmengine - INFO - Epoch(test) [100/151]    eta: 0:00:05  time: 0.0808  data_time: 0.0038  memory: 654  \n",
            "01/10 12:32:19 - mmengine - INFO - Epoch(test) [120/151]    eta: 0:00:03  time: 0.0808  data_time: 0.0052  memory: 654  \n",
            "01/10 12:32:20 - mmengine - INFO - Epoch(test) [140/151]    eta: 0:00:01  time: 0.0809  data_time: 0.0049  memory: 654  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9935)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9726)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9638)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7301)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9241)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8697)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0049)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0136)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0083)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0164)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0158)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0166)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0074)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0102)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0048)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0121)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0102)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0247)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0155)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0063)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0108)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0061)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0079)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0070)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0055)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0047)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0032)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0125)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0157)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0232)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0118)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0129)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0156)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0130)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0065)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0063)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0086)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0080)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0047)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0059)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0063)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0123)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0135)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0126)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0069)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0146)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0085)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0161)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0075)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0221)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0078)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0249)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0168)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0184)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0233)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0186)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0323)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0292)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0060)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0109)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0093)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0077)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0033)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0242)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0144)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0233)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0139)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0119)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0063)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6965)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5292)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5851)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7754)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4932)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6185)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5746)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0049)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0136)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0083)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0164)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0158)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0166)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0074)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0102)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0048)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0121)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0102)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0247)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0155)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0063)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0108)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0061)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0079)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0070)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0055)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0047)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0032)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0125)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0157)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0232)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0118)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0129)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0156)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0130)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0065)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0063)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0086)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0080)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0047)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0059)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0063)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0123)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0135)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0126)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0069)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0146)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0085)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0161)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0075)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0221)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0078)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0249)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0168)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0184)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0233)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0186)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0323)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0292)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0060)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0109)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0093)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0077)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0033)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0242)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0144)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0233)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0139)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0119)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0063)}]\n",
            "01/10 12:32:21 - mmengine - INFO - Epoch(test) [151/151]    SelfCustomMetric/accuracy: 0.9934  SelfCustomMetric/f1_score: 0.9600  SelfCustomMetric/recall: 0.9231  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.        , 0.07692308, 1.        , 1.        ]), array([1.9934627 , 0.9934627 , 0.49318725, 0.00323809], dtype=float32))  data_time: 0.0326  time: 0.0992\n",
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 12:32:21 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 1836014926\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 1836014926\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 12:32:22 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_yolo3'\n",
            "data_root_val = '../group_project_data/output_yolo3'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 12:32:22 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 12:32:22 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 12:32:53 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 12:32:53 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 12:32:53 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 12:32:53 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 12:33:30 - mmengine - INFO - Epoch(train)  [1][20/28]  lr: 1.5625e-04  eta: 0:07:54  time: 1.8268  data_time: 1.1635  memory: 3712  grad_norm: 6.4729  loss: 0.6773  top1_acc: 0.4375  top5_acc: 1.0000  loss_cls: 0.6773\n",
            "01/10 12:33:43 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_123221\n",
            "01/10 12:33:43 - mmengine - INFO - Epoch(train)  [1][28/28]  lr: 1.5625e-04  eta: 0:07:24  time: 1.6330  data_time: 0.9997  memory: 3712  grad_norm: 6.5728  loss: 0.6570  top1_acc: 0.0000  top5_acc: 1.0000  loss_cls: 0.6570\n",
            "01/10 12:34:20 - mmengine - INFO - Epoch(train)  [2][20/28]  lr: 1.5625e-04  eta: 0:06:58  time: 1.8628  data_time: 1.1910  memory: 3712  grad_norm: 5.4682  loss: 0.5667  top1_acc: 0.8125  top5_acc: 1.0000  loss_cls: 0.5667\n",
            "01/10 12:34:31 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_123221\n",
            "01/10 12:34:31 - mmengine - INFO - Epoch(train)  [2][28/28]  lr: 1.5625e-04  eta: 0:06:28  time: 1.5825  data_time: 0.9512  memory: 3712  grad_norm: 5.5568  loss: 0.5365  top1_acc: 0.5000  top5_acc: 1.0000  loss_cls: 0.5365\n",
            "01/10 12:35:10 - mmengine - INFO - Epoch(train)  [3][20/28]  lr: 1.5625e-04  eta: 0:06:07  time: 1.9932  data_time: 1.3172  memory: 3712  grad_norm: 5.5430  loss: 0.4954  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4954\n",
            "01/10 12:35:21 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_123221\n",
            "01/10 12:35:21 - mmengine - INFO - Epoch(train)  [3][28/28]  lr: 1.5625e-04  eta: 0:05:43  time: 1.6764  data_time: 1.0314  memory: 3712  grad_norm: 6.0847  loss: 0.4848  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.4848\n",
            "01/10 12:35:57 - mmengine - INFO - Epoch(train)  [4][20/28]  lr: 1.5625e-04  eta: 0:05:10  time: 1.8003  data_time: 1.1305  memory: 3712  grad_norm: 5.0977  loss: 0.4293  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.4293\n",
            "01/10 12:36:09 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_123221\n",
            "01/10 12:36:09 - mmengine - INFO - Epoch(train)  [4][28/28]  lr: 1.5625e-04  eta: 0:04:52  time: 1.6256  data_time: 0.9908  memory: 3712  grad_norm: 5.7128  loss: 0.4350  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.4350\n",
            "01/10 12:36:49 - mmengine - INFO - Epoch(train)  [5][20/28]  lr: 1.5625e-04  eta: 0:04:24  time: 2.0384  data_time: 1.3557  memory: 3712  grad_norm: 4.7337  loss: 0.3587  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3587\n",
            "01/10 12:36:58 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_123221\n",
            "01/10 12:36:58 - mmengine - INFO - Epoch(train)  [5][28/28]  lr: 1.5625e-04  eta: 0:04:05  time: 1.6726  data_time: 1.0278  memory: 3712  grad_norm: 4.8871  loss: 0.3376  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3376\n",
            "01/10 12:36:58 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 12:37:05 - mmengine - INFO - Epoch(val) [5][7/7]    acc/top1: 0.9167  acc/top5: 1.0000  acc/mean1: 0.8846  data_time: 0.6986  time: 0.8346\n",
            "01/10 12:37:06 - mmengine - INFO - The best checkpoint with 0.9167 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 12:37:44 - mmengine - INFO - Epoch(train)  [6][20/28]  lr: 1.5625e-04  eta: 0:03:31  time: 1.8667  data_time: 1.1838  memory: 3712  grad_norm: 4.8235  loss: 0.3196  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3196\n",
            "01/10 12:37:56 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_123221\n",
            "01/10 12:37:56 - mmengine - INFO - Epoch(train)  [6][28/28]  lr: 1.5625e-04  eta: 0:03:15  time: 1.6673  data_time: 1.0229  memory: 3712  grad_norm: 5.0076  loss: 0.3097  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3097\n",
            "01/10 12:38:32 - mmengine - INFO - Epoch(train)  [7][20/28]  lr: 1.5625e-04  eta: 0:02:41  time: 1.7920  data_time: 1.1202  memory: 3712  grad_norm: 4.5650  loss: 0.2776  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2776\n",
            "01/10 12:38:44 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_123221\n",
            "01/10 12:38:44 - mmengine - INFO - Epoch(train)  [7][28/28]  lr: 1.5625e-04  eta: 0:02:26  time: 1.6606  data_time: 1.0246  memory: 3712  grad_norm: 4.9959  loss: 0.2859  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2859\n",
            "01/10 12:39:23 - mmengine - INFO - Epoch(train)  [8][20/28]  lr: 1.5625e-04  eta: 0:01:52  time: 1.9406  data_time: 1.2665  memory: 3712  grad_norm: 4.8410  loss: 0.2578  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2578\n",
            "01/10 12:39:33 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_123221\n",
            "01/10 12:39:33 - mmengine - INFO - Epoch(train)  [8][28/28]  lr: 1.5625e-04  eta: 0:01:37  time: 1.7588  data_time: 1.1162  memory: 3712  grad_norm: 4.5137  loss: 0.2357  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2357\n",
            "01/10 12:40:09 - mmengine - INFO - Epoch(train)  [9][20/28]  lr: 1.5625e-04  eta: 0:01:03  time: 1.7923  data_time: 1.1230  memory: 3712  grad_norm: 4.0947  loss: 0.2093  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2093\n",
            "01/10 12:40:22 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_123221\n",
            "01/10 12:40:22 - mmengine - INFO - Epoch(train)  [9][28/28]  lr: 1.5625e-04  eta: 0:00:48  time: 1.6127  data_time: 0.9776  memory: 3712  grad_norm: 3.9854  loss: 0.1880  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1880\n",
            "01/10 12:41:00 - mmengine - INFO - Epoch(train) [10][20/28]  lr: 1.5625e-04  eta: 0:00:14  time: 1.8914  data_time: 1.2181  memory: 3712  grad_norm: 4.5194  loss: 0.2008  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2008\n",
            "01/10 12:41:14 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_123221\n",
            "01/10 12:41:14 - mmengine - INFO - Epoch(train) [10][28/28]  lr: 1.5625e-04  eta: 0:00:00  time: 1.7285  data_time: 1.0888  memory: 3712  grad_norm: 4.9988  loss: 0.2189  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2189\n",
            "01/10 12:41:14 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 12:41:22 - mmengine - INFO - Epoch(val) [10][7/7]    acc/top1: 0.7685  acc/top5: 1.0000  acc/mean1: 0.6795  data_time: 0.7660  time: 0.9004\n",
            "01/10 12:41:24 - mmengine - INFO - Epoch(test) [ 20/132]    eta: 0:00:09  time: 0.0837  data_time: 0.0101  memory: 822  \n",
            "01/10 12:41:26 - mmengine - INFO - Epoch(test) [ 40/132]    eta: 0:00:07  time: 0.0802  data_time: 0.0034  memory: 656  \n",
            "01/10 12:41:28 - mmengine - INFO - Epoch(test) [ 60/132]    eta: 0:00:05  time: 0.0830  data_time: 0.0079  memory: 656  \n",
            "01/10 12:41:29 - mmengine - INFO - Epoch(test) [ 80/132]    eta: 0:00:04  time: 0.0841  data_time: 0.0165  memory: 656  \n",
            "01/10 12:41:31 - mmengine - INFO - Epoch(test) [100/132]    eta: 0:00:02  time: 0.0811  data_time: 0.0090  memory: 656  \n",
            "01/10 12:41:32 - mmengine - INFO - Epoch(test) [120/132]    eta: 0:00:00  time: 0.0810  data_time: 0.0055  memory: 656  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6737)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6616)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6757)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.3569)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4015)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.3734)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4675)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6129)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.3897)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6738)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4434)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5762)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5966)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8136)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7429)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8561)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8572)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9307)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8698)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8444)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9002)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8071)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8206)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6900)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7704)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8306)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8031)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8833)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8367)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8100)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8162)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8349)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7965)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8292)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8387)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8586)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7851)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7649)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7480)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8401)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6611)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8577)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9229)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8419)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7344)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7146)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8032)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9105)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8728)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7622)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8533)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8226)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9188)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7025)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7079)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7028)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6433)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9892)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9253)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9191)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6294)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8216)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6628)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0347)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0279)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0414)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0222)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0549)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0462)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0443)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0256)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0333)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0329)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0182)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0372)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0313)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0592)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0692)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0182)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0293)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0243)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0283)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0244)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0266)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0349)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0217)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0414)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0323)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0834)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0467)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0370)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0446)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0447)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0196)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0227)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0193)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0314)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0338)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0232)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0257)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0216)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0255)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0356)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0411)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0458)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0281)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0240)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0397)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0268)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0458)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0305)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0430)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0230)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0636)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0497)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0468)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0622)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0455)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0579)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0708)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0278)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0431)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0357)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0285)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0188)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0684)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0492)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0556)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0443)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0295)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0387)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0210)}]\n",
            "01/10 12:41:34 - mmengine - INFO - Epoch(test) [132/132]    SelfCustomMetric/accuracy: 0.9545  SelfCustomMetric/f1_score: 0.9500  SelfCustomMetric/recall: 0.9048  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.        , 0.01587302, 1.        , 1.        ]), array([1.9891509 , 0.9891509 , 0.35687304, 0.01818558], dtype=float32))  data_time: 0.0101  time: 0.0825\n"
          ]
        }
      ],
      "source": [
        "import os.path as osp\n",
        "import mmengine\n",
        "from mmengine.runner import Runner\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "result_list = []\n",
        "# split the datasets\n",
        "for i in range(len(random_seeds)):\n",
        "  set_seeds(random_seeds[i])\n",
        "  yaml_path = '../group_project_data/sample.yaml'\n",
        "  annotation_path = './Annotations'\n",
        "  spilt_dataset_and_create_annotations(yaml_path, annotation_path, do_oversampling=False)\n",
        "\n",
        "  # Create work_dir\n",
        "  mmengine.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
        "\n",
        "  # build the runner\n",
        "  runner = Runner.from_cfg(cfg)\n",
        "\n",
        "  # start training\n",
        "  runner.train()\n",
        "\n",
        "  result_list.append(runner.test())\n",
        "\n",
        "result_dict['Cropped'] = result_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDZaIis_7ir_"
      },
      "source": [
        "## Using Initial Frames + Over Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_xNiaV57upr",
        "outputId": "dbe5ef04-982e-4a24-8fd2-77418feb6430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 12:42:21 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 1165313289\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 1165313289\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 12:42:21 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_frames'\n",
            "data_root_val = '../group_project_data/output_frames'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 12:42:21 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 12:42:21 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 12:42:52 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 12:42:53 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 12:42:53 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 12:42:53 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 12:43:38 - mmengine - INFO - Epoch(train)  [1][20/41]  lr: 1.5625e-04  eta: 0:14:47  time: 2.2768  data_time: 1.5983  memory: 3711  grad_norm: 7.4482  loss: 0.7139  top1_acc: 0.4375  top5_acc: 1.0000  loss_cls: 0.7139\n",
            "01/10 12:44:20 - mmengine - INFO - Epoch(train)  [1][40/41]  lr: 1.5625e-04  eta: 0:13:25  time: 2.0777  data_time: 1.3969  memory: 3711  grad_norm: 6.6584  loss: 0.6252  top1_acc: 0.6875  top5_acc: 1.0000  loss_cls: 0.6252\n",
            "01/10 12:44:20 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_124221\n",
            "01/10 12:44:20 - mmengine - INFO - Epoch(train)  [1][41/41]  lr: 1.5625e-04  eta: 0:13:09  time: 1.9612  data_time: 1.2888  memory: 3332  grad_norm: 6.6699  loss: 0.6204  top1_acc: 0.6429  top5_acc: 1.0000  loss_cls: 0.6204\n",
            "01/10 12:45:02 - mmengine - INFO - Epoch(train)  [2][20/41]  lr: 1.5625e-04  eta: 0:12:21  time: 2.1010  data_time: 1.4383  memory: 3711  grad_norm: 6.6392  loss: 0.5550  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.5550\n",
            "01/10 12:45:43 - mmengine - INFO - Epoch(train)  [2][40/41]  lr: 1.5625e-04  eta: 0:11:30  time: 2.0218  data_time: 1.3634  memory: 3711  grad_norm: 6.3679  loss: 0.5160  top1_acc: 0.8125  top5_acc: 1.0000  loss_cls: 0.5160\n",
            "01/10 12:45:44 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_124221\n",
            "01/10 12:45:44 - mmengine - INFO - Epoch(train)  [2][41/41]  lr: 1.5625e-04  eta: 0:11:25  time: 1.9244  data_time: 1.2740  memory: 3332  grad_norm: 6.4204  loss: 0.5165  top1_acc: 0.5714  top5_acc: 1.0000  loss_cls: 0.5165\n",
            "01/10 12:46:29 - mmengine - INFO - Epoch(train)  [3][20/41]  lr: 1.5625e-04  eta: 0:10:52  time: 2.2359  data_time: 1.5775  memory: 3711  grad_norm: 6.4698  loss: 0.4664  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4664\n",
            "01/10 12:47:09 - mmengine - INFO - Epoch(train)  [3][40/41]  lr: 1.5625e-04  eta: 0:10:04  time: 2.0137  data_time: 1.3600  memory: 3711  grad_norm: 6.2810  loss: 0.4233  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.4233\n",
            "01/10 12:47:09 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_124221\n",
            "01/10 12:47:09 - mmengine - INFO - Epoch(train)  [3][41/41]  lr: 1.5625e-04  eta: 0:09:59  time: 1.8140  data_time: 1.1682  memory: 3332  grad_norm: 6.3876  loss: 0.4238  top1_acc: 0.7857  top5_acc: 1.0000  loss_cls: 0.4238\n",
            "01/10 12:47:53 - mmengine - INFO - Epoch(train)  [4][20/41]  lr: 1.5625e-04  eta: 0:09:20  time: 2.1824  data_time: 1.5370  memory: 3711  grad_norm: 6.0890  loss: 0.3704  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.3704\n",
            "01/10 12:48:32 - mmengine - INFO - Epoch(train)  [4][40/41]  lr: 1.5625e-04  eta: 0:08:34  time: 1.9659  data_time: 1.3197  memory: 3711  grad_norm: 5.5838  loss: 0.3124  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3124\n",
            "01/10 12:48:34 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_124221\n",
            "01/10 12:48:34 - mmengine - INFO - Epoch(train)  [4][41/41]  lr: 1.5625e-04  eta: 0:08:31  time: 1.9383  data_time: 1.2962  memory: 3332  grad_norm: 5.4555  loss: 0.3027  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3027\n",
            "01/10 12:49:15 - mmengine - INFO - Epoch(train)  [5][20/41]  lr: 1.5625e-04  eta: 0:07:49  time: 2.0711  data_time: 1.4246  memory: 3711  grad_norm: 5.8071  loss: 0.2886  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.2886\n",
            "01/10 12:49:55 - mmengine - INFO - Epoch(train)  [5][40/41]  lr: 1.5625e-04  eta: 0:07:06  time: 1.9989  data_time: 1.3467  memory: 3711  grad_norm: 5.7752  loss: 0.2698  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2698\n",
            "01/10 12:49:58 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_124221\n",
            "01/10 12:49:58 - mmengine - INFO - Epoch(train)  [5][41/41]  lr: 1.5625e-04  eta: 0:07:05  time: 1.9591  data_time: 1.3121  memory: 3332  grad_norm: 5.8075  loss: 0.2689  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2689\n",
            "01/10 12:49:58 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 12:50:02 - mmengine - INFO - Epoch(val) [5][2/2]    acc/top1: 1.0000  acc/top5: 1.0000  acc/mean1: 1.0000  data_time: 1.2204  time: 1.3265\n",
            "01/10 12:50:02 - mmengine - INFO - The best checkpoint with 1.0000 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 12:50:48 - mmengine - INFO - Epoch(train)  [6][20/41]  lr: 1.5625e-04  eta: 0:06:26  time: 2.2342  data_time: 1.5803  memory: 3711  grad_norm: 5.6294  loss: 0.2542  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2542\n",
            "01/10 12:51:29 - mmengine - INFO - Epoch(train)  [6][40/41]  lr: 1.5625e-04  eta: 0:05:43  time: 2.0320  data_time: 1.3840  memory: 3711  grad_norm: 5.0313  loss: 0.2208  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2208\n",
            "01/10 12:51:29 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_124221\n",
            "01/10 12:51:29 - mmengine - INFO - Epoch(train)  [6][41/41]  lr: 1.5625e-04  eta: 0:05:40  time: 2.0206  data_time: 1.3767  memory: 3332  grad_norm: 4.9452  loss: 0.2144  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2144\n",
            "01/10 12:52:13 - mmengine - INFO - Epoch(train)  [7][20/41]  lr: 1.5625e-04  eta: 0:05:00  time: 2.1691  data_time: 1.5168  memory: 3711  grad_norm: 4.9921  loss: 0.1973  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1973\n",
            "01/10 12:52:53 - mmengine - INFO - Epoch(train)  [7][40/41]  lr: 1.5625e-04  eta: 0:04:17  time: 2.0219  data_time: 1.3775  memory: 3711  grad_norm: 5.1880  loss: 0.2063  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2063\n",
            "01/10 12:52:54 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_124221\n",
            "01/10 12:52:54 - mmengine - INFO - Epoch(train)  [7][41/41]  lr: 1.5625e-04  eta: 0:04:15  time: 1.8832  data_time: 1.2477  memory: 3332  grad_norm: 5.3557  loss: 0.2076  top1_acc: 0.8571  top5_acc: 1.0000  loss_cls: 0.2076\n",
            "01/10 12:53:37 - mmengine - INFO - Epoch(train)  [8][20/41]  lr: 1.5625e-04  eta: 0:03:34  time: 2.1487  data_time: 1.5013  memory: 3711  grad_norm: 5.0139  loss: 0.1717  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.1717\n",
            "01/10 12:54:17 - mmengine - INFO - Epoch(train)  [8][40/41]  lr: 1.5625e-04  eta: 0:02:52  time: 1.9887  data_time: 1.3407  memory: 3711  grad_norm: 4.5640  loss: 0.1597  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1597\n",
            "01/10 12:54:20 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_124221\n",
            "01/10 12:54:20 - mmengine - INFO - Epoch(train)  [8][41/41]  lr: 1.5625e-04  eta: 0:02:50  time: 1.9966  data_time: 1.3521  memory: 3332  grad_norm: 4.6187  loss: 0.1592  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1592\n",
            "01/10 12:55:04 - mmengine - INFO - Epoch(train)  [9][20/41]  lr: 1.5625e-04  eta: 0:02:09  time: 2.1904  data_time: 1.5414  memory: 3711  grad_norm: 5.5394  loss: 0.1816  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1816\n",
            "01/10 12:55:45 - mmengine - INFO - Epoch(train)  [9][40/41]  lr: 1.5625e-04  eta: 0:01:27  time: 2.0537  data_time: 1.4123  memory: 3711  grad_norm: 5.4972  loss: 0.1627  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.1627\n",
            "01/10 12:55:46 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_124221\n",
            "01/10 12:55:46 - mmengine - INFO - Epoch(train)  [9][41/41]  lr: 1.5625e-04  eta: 0:01:25  time: 1.9335  data_time: 1.3005  memory: 3332  grad_norm: 5.6202  loss: 0.1668  top1_acc: 0.7857  top5_acc: 1.0000  loss_cls: 0.1668\n",
            "01/10 12:56:28 - mmengine - INFO - Epoch(train) [10][20/41]  lr: 1.5625e-04  eta: 0:00:43  time: 2.0907  data_time: 1.4403  memory: 3711  grad_norm: 4.5558  loss: 0.1351  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1351\n",
            "01/10 12:57:10 - mmengine - INFO - Epoch(train) [10][40/41]  lr: 1.5625e-04  eta: 0:00:02  time: 2.1266  data_time: 1.4783  memory: 3711  grad_norm: 4.5178  loss: 0.1297  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1297\n",
            "01/10 12:57:11 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_124221\n",
            "01/10 12:57:11 - mmengine - INFO - Epoch(train) [10][41/41]  lr: 1.5625e-04  eta: 0:00:00  time: 2.0718  data_time: 1.4278  memory: 3332  grad_norm: 4.3006  loss: 0.1231  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1231\n",
            "01/10 12:57:11 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 12:57:14 - mmengine - INFO - Epoch(val) [10][2/2]    acc/top1: 1.0000  acc/top5: 1.0000  acc/mean1: 1.0000  data_time: 0.6687  time: 0.7514\n",
            "01/10 12:57:17 - mmengine - INFO - Epoch(test) [ 20/109]    eta: 0:00:10  time: 0.1221  data_time: 0.0756  memory: 820  \n",
            "01/10 12:57:20 - mmengine - INFO - Epoch(test) [ 40/109]    eta: 0:00:09  time: 0.1604  data_time: 0.1189  memory: 654  \n",
            "01/10 12:57:23 - mmengine - INFO - Epoch(test) [ 60/109]    eta: 0:00:06  time: 0.1350  data_time: 0.0850  memory: 654  \n",
            "01/10 12:57:25 - mmengine - INFO - Epoch(test) [ 80/109]    eta: 0:00:03  time: 0.1035  data_time: 0.0562  memory: 654  \n",
            "01/10 12:57:27 - mmengine - INFO - Epoch(test) [100/109]    eta: 0:00:01  time: 0.0896  data_time: 0.0404  memory: 654  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9911)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9839)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9861)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9780)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9715)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9632)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9776)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9708)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9712)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9651)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9827)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9752)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9848)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9866)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9773)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9905)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9956)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9900)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9966)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9915)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9979)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9859)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9761)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9497)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9964)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9946)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9964)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9942)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9921)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9850)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9836)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9796)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9905)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9849)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9770)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9624)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9829)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7589)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6641)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7004)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0059)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0023)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0044)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0058)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0096)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0036)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0032)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0032)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0020)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0045)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0028)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0023)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0018)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0062)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0049)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0028)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0026)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0039)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0051)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0020)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0032)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0031)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0048)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0026)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0035)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0061)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0029)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0031)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0038)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0080)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0062)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0016)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0048)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0045)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0032)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0033)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0040)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0059)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0025)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0020)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0070)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0038)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0018)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0076)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0021)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0014)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0036)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0024)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0034)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0048)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0036)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0065)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0175)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0148)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0217)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0191)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0040)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0018)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0036)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0047)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0053)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0058)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0042)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0242)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0049)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0067)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0034)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0031)}]\n",
            "01/10 12:57:27 - mmengine - INFO - Epoch(test) [109/109]    SelfCustomMetric/accuracy: 1.0000  SelfCustomMetric/f1_score: 1.0000  SelfCustomMetric/recall: 1.0000  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.   , 0.025, 1.   , 1.   ]), array([1.9978564e+00, 9.9785632e-01, 6.6413343e-01, 1.3815074e-03],\n",
            "      dtype=float32))  data_time: 0.0717  time: 0.1186\n",
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 12:57:27 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 1608637542\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 1608637542\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 12:57:28 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_frames'\n",
            "data_root_val = '../group_project_data/output_frames'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 12:57:28 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 12:57:28 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 12:57:59 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 12:58:00 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 12:58:00 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 12:58:00 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 12:58:42 - mmengine - INFO - Epoch(train)  [1][20/38]  lr: 1.5625e-04  eta: 0:12:52  time: 2.1457  data_time: 1.5102  memory: 3711  grad_norm: 7.3525  loss: 0.7117  top1_acc: 0.3750  top5_acc: 1.0000  loss_cls: 0.7117\n",
            "01/10 12:59:21 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_125727\n",
            "01/10 12:59:21 - mmengine - INFO - Epoch(train)  [1][38/38]  lr: 1.5625e-04  eta: 0:12:15  time: 2.1163  data_time: 1.4792  memory: 3711  grad_norm: 7.1304  loss: 0.6265  top1_acc: 0.4615  top5_acc: 1.0000  loss_cls: 0.6265\n",
            "01/10 13:00:07 - mmengine - INFO - Epoch(train)  [2][20/38]  lr: 1.5625e-04  eta: 0:11:45  time: 2.2655  data_time: 1.6160  memory: 3711  grad_norm: 6.5012  loss: 0.5611  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.5611\n",
            "01/10 13:00:43 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_125727\n",
            "01/10 13:00:43 - mmengine - INFO - Epoch(train)  [2][38/38]  lr: 1.5625e-04  eta: 0:10:54  time: 2.0489  data_time: 1.4163  memory: 3711  grad_norm: 6.2807  loss: 0.4903  top1_acc: 0.7692  top5_acc: 1.0000  loss_cls: 0.4903\n",
            "01/10 13:01:30 - mmengine - INFO - Epoch(train)  [3][20/38]  lr: 1.5625e-04  eta: 0:10:22  time: 2.3482  data_time: 1.7059  memory: 3711  grad_norm: 6.4568  loss: 0.4523  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4523\n",
            "01/10 13:02:07 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_125727\n",
            "01/10 13:02:07 - mmengine - INFO - Epoch(train)  [3][38/38]  lr: 1.5625e-04  eta: 0:09:38  time: 2.0632  data_time: 1.4296  memory: 3711  grad_norm: 5.6266  loss: 0.3735  top1_acc: 0.9231  top5_acc: 1.0000  loss_cls: 0.3735\n",
            "01/10 13:02:51 - mmengine - INFO - Epoch(train)  [4][20/38]  lr: 1.5625e-04  eta: 0:08:55  time: 2.1816  data_time: 1.5354  memory: 3711  grad_norm: 5.4323  loss: 0.3279  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3279\n",
            "01/10 13:03:29 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_125727\n",
            "01/10 13:03:29 - mmengine - INFO - Epoch(train)  [4][38/38]  lr: 1.5625e-04  eta: 0:08:13  time: 2.0435  data_time: 1.4102  memory: 3711  grad_norm: 5.9673  loss: 0.3148  top1_acc: 0.9231  top5_acc: 1.0000  loss_cls: 0.3148\n",
            "01/10 13:04:14 - mmengine - INFO - Epoch(train)  [5][20/38]  lr: 1.5625e-04  eta: 0:07:32  time: 2.2593  data_time: 1.6039  memory: 3711  grad_norm: 5.6327  loss: 0.2807  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2807\n",
            "01/10 13:04:51 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_125727\n",
            "01/10 13:04:51 - mmengine - INFO - Epoch(train)  [5][38/38]  lr: 1.5625e-04  eta: 0:06:51  time: 2.1492  data_time: 1.5108  memory: 3711  grad_norm: 5.5809  loss: 0.2443  top1_acc: 0.9231  top5_acc: 1.0000  loss_cls: 0.2443\n",
            "01/10 13:04:51 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 13:04:55 - mmengine - INFO - Epoch(val) [5][4/4]    acc/top1: 0.9038  acc/top5: 1.0000  acc/mean1: 0.4519  data_time: 0.6629  time: 0.7795\n",
            "01/10 13:04:57 - mmengine - INFO - The best checkpoint with 0.9038 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 13:05:45 - mmengine - INFO - Epoch(train)  [6][20/38]  lr: 1.5625e-04  eta: 0:06:11  time: 2.3425  data_time: 1.6935  memory: 3711  grad_norm: 5.6216  loss: 0.2467  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2467\n",
            "01/10 13:06:20 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_125727\n",
            "01/10 13:06:20 - mmengine - INFO - Epoch(train)  [6][38/38]  lr: 1.5625e-04  eta: 0:05:29  time: 1.9851  data_time: 1.3483  memory: 3726  grad_norm: 5.4931  loss: 0.2281  top1_acc: 0.9231  top5_acc: 1.0000  loss_cls: 0.2281\n",
            "01/10 13:07:06 - mmengine - INFO - Epoch(train)  [7][20/38]  lr: 1.5625e-04  eta: 0:04:47  time: 2.2964  data_time: 1.6437  memory: 3711  grad_norm: 5.5746  loss: 0.2122  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2122\n",
            "01/10 13:07:41 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_125727\n",
            "01/10 13:07:41 - mmengine - INFO - Epoch(train)  [7][38/38]  lr: 1.5625e-04  eta: 0:04:06  time: 1.9842  data_time: 1.3441  memory: 3711  grad_norm: 5.1919  loss: 0.1855  top1_acc: 0.8462  top5_acc: 1.0000  loss_cls: 0.1855\n",
            "01/10 13:08:25 - mmengine - INFO - Epoch(train)  [8][20/38]  lr: 1.5625e-04  eta: 0:03:23  time: 2.2120  data_time: 1.5653  memory: 3711  grad_norm: 4.9793  loss: 0.1708  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1708\n",
            "01/10 13:09:03 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_125727\n",
            "01/10 13:09:03 - mmengine - INFO - Epoch(train)  [8][38/38]  lr: 1.5625e-04  eta: 0:02:44  time: 2.1452  data_time: 1.4991  memory: 3711  grad_norm: 4.7353  loss: 0.1630  top1_acc: 0.9231  top5_acc: 1.0000  loss_cls: 0.1630\n",
            "01/10 13:09:50 - mmengine - INFO - Epoch(train)  [9][20/38]  lr: 1.5625e-04  eta: 0:02:01  time: 2.3461  data_time: 1.6946  memory: 3711  grad_norm: 3.8533  loss: 0.1223  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1223\n",
            "01/10 13:10:27 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_125727\n",
            "01/10 13:10:27 - mmengine - INFO - Epoch(train)  [9][38/38]  lr: 1.5625e-04  eta: 0:01:22  time: 2.0656  data_time: 1.4227  memory: 3711  grad_norm: 4.1985  loss: 0.1353  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1353\n",
            "01/10 13:11:13 - mmengine - INFO - Epoch(train) [10][20/38]  lr: 1.5625e-04  eta: 0:00:39  time: 2.2674  data_time: 1.6239  memory: 3711  grad_norm: 4.5518  loss: 0.1297  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1297\n",
            "01/10 13:11:49 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_125727\n",
            "01/10 13:11:49 - mmengine - INFO - Epoch(train) [10][38/38]  lr: 1.5625e-04  eta: 0:00:00  time: 2.1156  data_time: 1.4830  memory: 3711  grad_norm: 5.3775  loss: 0.1449  top1_acc: 0.7692  top5_acc: 1.0000  loss_cls: 0.1449\n",
            "01/10 13:11:49 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 13:11:53 - mmengine - INFO - Epoch(val) [10][4/4]    acc/top1: 0.9808  acc/top5: 1.0000  acc/mean1: 0.4904  data_time: 0.5145  time: 0.6124\n",
            "01/10 13:11:53 - mmengine - INFO - The previous best checkpoint /content/mmaction2/tutorial_exps/best_acc_top1_epoch_5.pth is removed\n",
            "01/10 13:11:55 - mmengine - INFO - The best checkpoint with 0.9808 acc/top1 at 10 epoch is saved to best_acc_top1_epoch_10.pth.\n",
            "01/10 13:11:59 - mmengine - INFO - Epoch(test) [ 20/146]    eta: 0:00:12  time: 0.0985  data_time: 0.0326  memory: 822  \n",
            "01/10 13:12:02 - mmengine - INFO - Epoch(test) [ 40/146]    eta: 0:00:14  time: 0.1830  data_time: 0.1393  memory: 656  \n",
            "01/10 13:12:04 - mmengine - INFO - Epoch(test) [ 60/146]    eta: 0:00:10  time: 0.1020  data_time: 0.0549  memory: 656  \n",
            "01/10 13:12:06 - mmengine - INFO - Epoch(test) [ 80/146]    eta: 0:00:07  time: 0.0823  data_time: 0.0329  memory: 656  \n",
            "01/10 13:12:08 - mmengine - INFO - Epoch(test) [100/146]    eta: 0:00:05  time: 0.0834  data_time: 0.0402  memory: 656  \n",
            "01/10 13:12:09 - mmengine - INFO - Epoch(test) [120/146]    eta: 0:00:02  time: 0.0788  data_time: 0.0269  memory: 656  \n",
            "01/10 13:12:12 - mmengine - INFO - Epoch(test) [140/146]    eta: 0:00:00  time: 0.1089  data_time: 0.0578  memory: 656  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8742)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9236)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9224)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8359)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9263)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8815)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8905)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8658)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8265)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9589)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9571)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8760)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9006)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8846)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9714)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9289)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9702)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8937)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9174)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9066)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8247)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9212)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8591)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9771)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9126)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8959)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9212)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9745)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9529)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9351)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9055)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8748)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.3609)}, {'pred': tensor(1), 'gt': tensor(0), 'pd_score': tensor(0.5536)}, {'pred': tensor(1), 'gt': tensor(0), 'pd_score': tensor(0.5434)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.2601)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.2393)}, {'pred': tensor(1), 'gt': tensor(0), 'pd_score': tensor(0.5375)}, {'pred': tensor(1), 'gt': tensor(0), 'pd_score': tensor(0.6172)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.4188)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.3719)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9360)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9389)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9700)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9041)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8768)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9044)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9144)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0214)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0160)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0142)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0115)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0099)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0121)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0084)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0083)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0082)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0113)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0088)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0098)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0127)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0103)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0105)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0083)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0163)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0227)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0295)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0269)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0233)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0154)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0135)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0217)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0150)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0187)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0223)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0234)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0154)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0214)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0212)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0207)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0223)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0151)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0183)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0192)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0222)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0278)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0075)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0073)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0089)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0056)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0089)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0109)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0136)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0128)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0153)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0153)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0122)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0124)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0117)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0114)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0110)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0106)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0151)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0157)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0162)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0149)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0157)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0163)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0174)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0180)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0166)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0145)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0158)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0161)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0151)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0273)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0116)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0128)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0168)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0195)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0267)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0219)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0088)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0129)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0128)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0115)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0076)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0081)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0056)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0070)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0133)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0086)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0068)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0083)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0066)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0142)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0171)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0394)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0198)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0141)}]\n",
            "01/10 13:12:13 - mmengine - INFO - Epoch(test) [146/146]    SelfCustomMetric/accuracy: 0.9726  SelfCustomMetric/f1_score: 0.9512  SelfCustomMetric/recall: 1.0000  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.        , 0.02564103, 1.        , 1.        ]), array([1.9770706 , 0.97707057, 0.82472414, 0.00555627], dtype=float32))  data_time: 0.0558  time: 0.1061\n",
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 13:12:13 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 2124297904\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 2124297904\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 13:12:13 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_frames'\n",
            "data_root_val = '../group_project_data/output_frames'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 13:12:14 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 13:12:14 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 13:12:45 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 13:12:45 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 13:12:45 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 13:12:45 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 13:13:22 - mmengine - INFO - Epoch(train)  [1][20/41]  lr: 1.5625e-04  eta: 0:11:57  time: 1.8393  data_time: 1.2023  memory: 3715  grad_norm: 6.8298  loss: 0.6626  top1_acc: 0.4375  top5_acc: 1.0000  loss_cls: 0.6626\n",
            "01/10 13:14:02 - mmengine - INFO - Epoch(train)  [1][40/41]  lr: 1.5625e-04  eta: 0:11:48  time: 1.9879  data_time: 1.3393  memory: 3715  grad_norm: 7.0084  loss: 0.5967  top1_acc: 0.8125  top5_acc: 1.0000  loss_cls: 0.5967\n",
            "01/10 13:14:02 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_131213\n",
            "01/10 13:14:02 - mmengine - INFO - Epoch(train)  [1][41/41]  lr: 1.5625e-04  eta: 0:11:32  time: 1.9633  data_time: 1.3248  memory: 2726  grad_norm: 7.0156  loss: 0.5909  top1_acc: 0.8182  top5_acc: 1.0000  loss_cls: 0.5909\n",
            "01/10 13:14:41 - mmengine - INFO - Epoch(train)  [2][20/41]  lr: 1.5625e-04  eta: 0:11:04  time: 1.9562  data_time: 1.3122  memory: 3715  grad_norm: 6.2924  loss: 0.5233  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.5233\n",
            "01/10 13:15:17 - mmengine - INFO - Epoch(train)  [2][40/41]  lr: 1.5625e-04  eta: 0:10:14  time: 1.7603  data_time: 1.1155  memory: 3715  grad_norm: 6.2106  loss: 0.4782  top1_acc: 0.6250  top5_acc: 1.0000  loss_cls: 0.4782\n",
            "01/10 13:15:17 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_131213\n",
            "01/10 13:15:17 - mmengine - INFO - Epoch(train)  [2][41/41]  lr: 1.5625e-04  eta: 0:10:07  time: 1.7518  data_time: 1.1168  memory: 2726  grad_norm: 6.1995  loss: 0.4754  top1_acc: 0.9091  top5_acc: 1.0000  loss_cls: 0.4754\n",
            "01/10 13:15:55 - mmengine - INFO - Epoch(train)  [3][20/41]  lr: 1.5625e-04  eta: 0:09:31  time: 1.8806  data_time: 1.2339  memory: 3715  grad_norm: 6.0355  loss: 0.4193  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.4193\n",
            "01/10 13:16:32 - mmengine - INFO - Epoch(train)  [3][40/41]  lr: 1.5625e-04  eta: 0:08:54  time: 1.8471  data_time: 1.2073  memory: 3715  grad_norm: 6.0044  loss: 0.3892  top1_acc: 0.8125  top5_acc: 1.0000  loss_cls: 0.3892\n",
            "01/10 13:16:32 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_131213\n",
            "01/10 13:16:32 - mmengine - INFO - Epoch(train)  [3][41/41]  lr: 1.5625e-04  eta: 0:08:49  time: 1.7199  data_time: 1.0924  memory: 2726  grad_norm: 6.2100  loss: 0.3935  top1_acc: 0.9091  top5_acc: 1.0000  loss_cls: 0.3935\n",
            "01/10 13:17:11 - mmengine - INFO - Epoch(train)  [4][20/41]  lr: 1.5625e-04  eta: 0:08:15  time: 1.9286  data_time: 1.2801  memory: 3715  grad_norm: 5.6242  loss: 0.3439  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.3439\n",
            "01/10 13:17:46 - mmengine - INFO - Epoch(train)  [4][40/41]  lr: 1.5625e-04  eta: 0:07:34  time: 1.7446  data_time: 1.1024  memory: 3715  grad_norm: 5.6511  loss: 0.3035  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.3035\n",
            "01/10 13:17:46 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_131213\n",
            "01/10 13:17:46 - mmengine - INFO - Epoch(train)  [4][41/41]  lr: 1.5625e-04  eta: 0:07:31  time: 1.6404  data_time: 1.0111  memory: 2726  grad_norm: 5.6680  loss: 0.3012  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3012\n",
            "01/10 13:18:26 - mmengine - INFO - Epoch(train)  [5][20/41]  lr: 1.5625e-04  eta: 0:06:58  time: 2.0017  data_time: 1.3532  memory: 3715  grad_norm: 5.4229  loss: 0.2715  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2715\n",
            "01/10 13:19:00 - mmengine - INFO - Epoch(train)  [5][40/41]  lr: 1.5625e-04  eta: 0:06:18  time: 1.7038  data_time: 1.0613  memory: 3715  grad_norm: 5.7953  loss: 0.2538  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2538\n",
            "01/10 13:19:01 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_131213\n",
            "01/10 13:19:01 - mmengine - INFO - Epoch(train)  [5][41/41]  lr: 1.5625e-04  eta: 0:06:15  time: 1.6918  data_time: 1.0585  memory: 2726  grad_norm: 6.0277  loss: 0.2639  top1_acc: 0.7273  top5_acc: 1.0000  loss_cls: 0.2639\n",
            "01/10 13:19:01 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 13:19:05 - mmengine - INFO - Epoch(val) [5][3/3]    acc/top1: 0.5556  acc/top5: 1.0000  acc/mean1: 0.2778  data_time: 0.8076  time: 0.9217\n",
            "01/10 13:19:05 - mmengine - INFO - The best checkpoint with 0.5556 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 13:19:42 - mmengine - INFO - Epoch(train)  [6][20/41]  lr: 1.5625e-04  eta: 0:05:38  time: 1.8030  data_time: 1.1560  memory: 3715  grad_norm: 5.9103  loss: 0.2400  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2400\n",
            "01/10 13:20:18 - mmengine - INFO - Epoch(train)  [6][40/41]  lr: 1.5625e-04  eta: 0:05:00  time: 1.7816  data_time: 1.1376  memory: 3715  grad_norm: 5.4376  loss: 0.2069  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.2069\n",
            "01/10 13:20:21 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_131213\n",
            "01/10 13:20:21 - mmengine - INFO - Epoch(train)  [6][41/41]  lr: 1.5625e-04  eta: 0:04:59  time: 1.8537  data_time: 1.2139  memory: 2726  grad_norm: 5.3505  loss: 0.1983  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1983\n",
            "01/10 13:20:59 - mmengine - INFO - Epoch(train)  [7][20/41]  lr: 1.5625e-04  eta: 0:04:23  time: 1.8811  data_time: 1.2335  memory: 3715  grad_norm: 5.3319  loss: 0.1933  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1933\n",
            "01/10 13:21:34 - mmengine - INFO - Epoch(train)  [7][40/41]  lr: 1.5625e-04  eta: 0:03:46  time: 1.7630  data_time: 1.1277  memory: 3715  grad_norm: 5.5075  loss: 0.1901  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1901\n",
            "01/10 13:21:35 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_131213\n",
            "01/10 13:21:35 - mmengine - INFO - Epoch(train)  [7][41/41]  lr: 1.5625e-04  eta: 0:03:44  time: 1.7376  data_time: 1.1119  memory: 2726  grad_norm: 5.5741  loss: 0.1908  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1908\n",
            "01/10 13:22:14 - mmengine - INFO - Epoch(train)  [8][20/41]  lr: 1.5625e-04  eta: 0:03:08  time: 1.9453  data_time: 1.2955  memory: 3715  grad_norm: 4.9074  loss: 0.1574  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1574\n",
            "01/10 13:22:51 - mmengine - INFO - Epoch(train)  [8][40/41]  lr: 1.5625e-04  eta: 0:02:32  time: 1.8543  data_time: 1.2033  memory: 3715  grad_norm: 4.6649  loss: 0.1501  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1501\n",
            "01/10 13:22:51 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_131213\n",
            "01/10 13:22:51 - mmengine - INFO - Epoch(train)  [8][41/41]  lr: 1.5625e-04  eta: 0:02:30  time: 1.8451  data_time: 1.2034  memory: 2726  grad_norm: 4.7939  loss: 0.1557  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1557\n",
            "01/10 13:23:29 - mmengine - INFO - Epoch(train)  [9][20/41]  lr: 1.5625e-04  eta: 0:01:53  time: 1.9172  data_time: 1.2807  memory: 3715  grad_norm: 4.3419  loss: 0.1309  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1309\n",
            "01/10 13:24:04 - mmengine - INFO - Epoch(train)  [9][40/41]  lr: 1.5625e-04  eta: 0:01:16  time: 1.7276  data_time: 1.0852  memory: 3715  grad_norm: 5.2763  loss: 0.1456  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1456\n",
            "01/10 13:24:04 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_131213\n",
            "01/10 13:24:04 - mmengine - INFO - Epoch(train)  [9][41/41]  lr: 1.5625e-04  eta: 0:01:14  time: 1.7109  data_time: 1.0781  memory: 2726  grad_norm: 5.2332  loss: 0.1465  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1465\n",
            "01/10 13:24:42 - mmengine - INFO - Epoch(train) [10][20/41]  lr: 1.5625e-04  eta: 0:00:38  time: 1.8639  data_time: 1.2192  memory: 3715  grad_norm: 3.9880  loss: 0.1094  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1094\n",
            "01/10 13:25:16 - mmengine - INFO - Epoch(train) [10][40/41]  lr: 1.5625e-04  eta: 0:00:01  time: 1.7236  data_time: 1.0809  memory: 3715  grad_norm: 5.3059  loss: 0.1489  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1489\n",
            "01/10 13:25:17 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_131213\n",
            "01/10 13:25:17 - mmengine - INFO - Epoch(train) [10][41/41]  lr: 1.5625e-04  eta: 0:00:00  time: 1.7327  data_time: 1.0996  memory: 2726  grad_norm: 5.0912  loss: 0.1399  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1399\n",
            "01/10 13:25:17 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 13:25:20 - mmengine - INFO - Epoch(val) [10][3/3]    acc/top1: 0.5278  acc/top5: 1.0000  acc/mean1: 0.2639  data_time: 0.3800  time: 0.4740\n",
            "01/10 13:25:26 - mmengine - INFO - Epoch(test) [ 20/140]    eta: 0:00:35  time: 0.2994  data_time: 0.2730  memory: 822  \n",
            "01/10 13:25:34 - mmengine - INFO - Epoch(test) [ 40/140]    eta: 0:00:34  time: 0.3812  data_time: 0.3404  memory: 656  \n",
            "01/10 13:25:38 - mmengine - INFO - Epoch(test) [ 60/140]    eta: 0:00:23  time: 0.2151  data_time: 0.1765  memory: 656  \n",
            "01/10 13:25:42 - mmengine - INFO - Epoch(test) [ 80/140]    eta: 0:00:16  time: 0.2057  data_time: 0.1648  memory: 675  \n",
            "01/10 13:25:45 - mmengine - INFO - Epoch(test) [100/140]    eta: 0:00:09  time: 0.1308  data_time: 0.0795  memory: 656  \n",
            "01/10 13:25:47 - mmengine - INFO - Epoch(test) [120/140]    eta: 0:00:04  time: 0.1330  data_time: 0.0852  memory: 656  \n",
            "01/10 13:25:49 - mmengine - INFO - Epoch(test) [140/140]    eta: 0:00:00  time: 0.0789  data_time: 0.0520  memory: 657  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9868)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9377)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9032)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9820)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9893)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7427)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8850)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9402)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9317)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8756)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9492)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9611)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9035)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9286)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9342)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9466)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9265)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9418)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9475)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9684)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9435)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8993)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9554)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9618)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9210)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9567)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9567)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9506)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9749)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9724)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9155)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9923)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9672)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8600)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9611)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9345)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9649)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9712)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9109)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8254)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9275)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9157)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9515)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9526)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8019)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8826)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8494)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8034)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9845)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8729)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8905)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7154)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6465)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6725)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6481)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7117)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7998)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7744)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8373)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9618)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9602)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5547)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7770)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8617)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8299)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8523)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8624)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8391)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8755)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9784)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9471)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0096)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0051)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0067)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0137)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0060)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0044)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0035)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0010)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0035)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0034)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0037)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0036)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0154)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0121)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0046)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0044)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0069)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0085)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0054)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0055)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0075)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0046)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0074)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0092)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0036)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0032)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0062)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0107)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0063)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0026)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0069)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0153)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0063)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0078)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0062)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0059)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0031)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0030)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0091)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0036)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0037)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0101)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0023)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0012)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0042)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0048)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0075)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0042)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0098)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0200)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0162)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0165)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0158)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0073)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0048)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0058)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0074)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0096)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0067)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0070)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0208)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0082)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0110)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0042)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0041)}]\n",
            "01/10 13:25:49 - mmengine - INFO - Epoch(test) [140/140]    SelfCustomMetric/accuracy: 1.0000  SelfCustomMetric/f1_score: 1.0000  SelfCustomMetric/recall: 1.0000  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.        , 0.01408451, 1.        , 1.        ]), array([1.9923368e+00, 9.9233681e-01, 5.5470127e-01, 9.5033651e-04],\n",
            "      dtype=float32))  data_time: 0.1673  time: 0.2063\n",
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 13:25:49 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 662660116\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 662660116\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 13:25:49 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_frames'\n",
            "data_root_val = '../group_project_data/output_frames'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 13:25:50 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 13:25:50 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 13:26:21 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 13:26:21 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 13:26:21 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 13:26:21 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 13:27:10 - mmengine - INFO - Epoch(train)  [1][20/36]  lr: 1.5625e-04  eta: 0:13:47  time: 2.4351  data_time: 1.7894  memory: 3712  grad_norm: 6.9297  loss: 0.6780  top1_acc: 0.6875  top5_acc: 1.0000  loss_cls: 0.6780\n",
            "01/10 13:27:42 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_132549\n",
            "01/10 13:27:42 - mmengine - INFO - Epoch(train)  [1][36/36]  lr: 1.5625e-04  eta: 0:12:04  time: 2.0509  data_time: 1.4080  memory: 3712  grad_norm: 6.9441  loss: 0.6536  top1_acc: 0.6250  top5_acc: 1.0000  loss_cls: 0.6536\n",
            "01/10 13:28:25 - mmengine - INFO - Epoch(train)  [2][20/36]  lr: 1.5625e-04  eta: 0:11:10  time: 2.1541  data_time: 1.5119  memory: 3712  grad_norm: 6.3233  loss: 0.5577  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.5577\n",
            "01/10 13:28:59 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_132549\n",
            "01/10 13:28:59 - mmengine - INFO - Epoch(train)  [2][36/36]  lr: 1.5625e-04  eta: 0:10:31  time: 2.1443  data_time: 1.4859  memory: 3712  grad_norm: 6.0969  loss: 0.5258  top1_acc: 0.6250  top5_acc: 1.0000  loss_cls: 0.5258\n",
            "01/10 13:29:42 - mmengine - INFO - Epoch(train)  [3][20/36]  lr: 1.5625e-04  eta: 0:09:45  time: 2.1505  data_time: 1.5087  memory: 3712  grad_norm: 6.4619  loss: 0.4865  top1_acc: 0.7500  top5_acc: 1.0000  loss_cls: 0.4865\n",
            "01/10 13:30:16 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_132549\n",
            "01/10 13:30:16 - mmengine - INFO - Epoch(train)  [3][36/36]  lr: 1.5625e-04  eta: 0:09:09  time: 2.2023  data_time: 1.5577  memory: 3712  grad_norm: 5.9715  loss: 0.4317  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4317\n",
            "01/10 13:31:00 - mmengine - INFO - Epoch(train)  [4][20/36]  lr: 1.5625e-04  eta: 0:08:25  time: 2.1880  data_time: 1.5409  memory: 3712  grad_norm: 5.8876  loss: 0.3855  top1_acc: 0.8125  top5_acc: 1.0000  loss_cls: 0.3855\n",
            "01/10 13:31:34 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_132549\n",
            "01/10 13:31:34 - mmengine - INFO - Epoch(train)  [4][36/36]  lr: 1.5625e-04  eta: 0:07:49  time: 2.0045  data_time: 1.3575  memory: 3712  grad_norm: 5.8976  loss: 0.3600  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3600\n",
            "01/10 13:32:20 - mmengine - INFO - Epoch(train)  [5][20/36]  lr: 1.5625e-04  eta: 0:07:09  time: 2.2995  data_time: 1.6493  memory: 3712  grad_norm: 5.7803  loss: 0.3253  top1_acc: 0.5000  top5_acc: 1.0000  loss_cls: 0.3253\n",
            "01/10 13:32:54 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_132549\n",
            "01/10 13:32:54 - mmengine - INFO - Epoch(train)  [5][36/36]  lr: 1.5625e-04  eta: 0:06:32  time: 2.0162  data_time: 1.3807  memory: 3712  grad_norm: 6.2355  loss: 0.3319  top1_acc: 0.8125  top5_acc: 1.0000  loss_cls: 0.3319\n",
            "01/10 13:32:54 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 13:32:57 - mmengine - INFO - Epoch(val) [5][2/2]    acc/top1: 0.9444  acc/top5: 1.0000  acc/mean1: 0.4722  data_time: 0.7360  time: 0.8367\n",
            "01/10 13:32:59 - mmengine - INFO - The best checkpoint with 0.9444 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 13:33:45 - mmengine - INFO - Epoch(train)  [6][20/36]  lr: 1.5625e-04  eta: 0:05:49  time: 2.2372  data_time: 1.5869  memory: 3712  grad_norm: 5.8619  loss: 0.2827  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2827\n",
            "01/10 13:34:18 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_132549\n",
            "01/10 13:34:18 - mmengine - INFO - Epoch(train)  [6][36/36]  lr: 1.5625e-04  eta: 0:05:13  time: 2.2191  data_time: 1.5685  memory: 3712  grad_norm: 5.4138  loss: 0.2579  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2579\n",
            "01/10 13:35:03 - mmengine - INFO - Epoch(train)  [7][20/36]  lr: 1.5625e-04  eta: 0:04:30  time: 2.2565  data_time: 1.6089  memory: 3712  grad_norm: 5.5139  loss: 0.2540  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2540\n",
            "01/10 13:35:35 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_132549\n",
            "01/10 13:35:35 - mmengine - INFO - Epoch(train)  [7][36/36]  lr: 1.5625e-04  eta: 0:03:54  time: 2.1977  data_time: 1.5469  memory: 3712  grad_norm: 5.1795  loss: 0.2341  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2341\n",
            "01/10 13:36:20 - mmengine - INFO - Epoch(train)  [8][20/36]  lr: 1.5625e-04  eta: 0:03:11  time: 2.2474  data_time: 1.5877  memory: 3712  grad_norm: 4.9906  loss: 0.1982  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.1982\n",
            "01/10 13:36:56 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_132549\n",
            "01/10 13:36:56 - mmengine - INFO - Epoch(train)  [8][36/36]  lr: 1.5625e-04  eta: 0:02:37  time: 2.2660  data_time: 1.6070  memory: 3712  grad_norm: 5.2681  loss: 0.2060  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2060\n",
            "01/10 13:37:41 - mmengine - INFO - Epoch(train)  [9][20/36]  lr: 1.5625e-04  eta: 0:01:53  time: 2.2404  data_time: 1.5691  memory: 3712  grad_norm: 5.0552  loss: 0.1826  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1826\n",
            "01/10 13:38:14 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_132549\n",
            "01/10 13:38:14 - mmengine - INFO - Epoch(train)  [9][36/36]  lr: 1.5625e-04  eta: 0:01:18  time: 2.0768  data_time: 1.4089  memory: 3712  grad_norm: 4.7577  loss: 0.1660  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1660\n",
            "01/10 13:38:59 - mmengine - INFO - Epoch(train) [10][20/36]  lr: 1.5625e-04  eta: 0:00:34  time: 2.2099  data_time: 1.5238  memory: 3712  grad_norm: 4.6164  loss: 0.1551  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.1551\n",
            "01/10 13:39:33 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_132549\n",
            "01/10 13:39:33 - mmengine - INFO - Epoch(train) [10][36/36]  lr: 1.5625e-04  eta: 0:00:00  time: 2.0599  data_time: 1.3778  memory: 3712  grad_norm: 4.9087  loss: 0.1659  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.1659\n",
            "01/10 13:39:33 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 13:39:35 - mmengine - INFO - Epoch(val) [10][2/2]    acc/top1: 1.0000  acc/top5: 1.0000  acc/mean1: 1.0000  data_time: 0.3783  time: 0.4506\n",
            "01/10 13:39:35 - mmengine - INFO - The previous best checkpoint /content/mmaction2/tutorial_exps/best_acc_top1_epoch_5.pth is removed\n",
            "01/10 13:39:36 - mmengine - INFO - The best checkpoint with 1.0000 acc/top1 at 10 epoch is saved to best_acc_top1_epoch_10.pth.\n",
            "01/10 13:39:42 - mmengine - INFO - Epoch(test) [ 20/151]    eta: 0:00:18  time: 0.1384  data_time: 0.0962  memory: 820  \n",
            "01/10 13:39:44 - mmengine - INFO - Epoch(test) [ 40/151]    eta: 0:00:13  time: 0.1098  data_time: 0.0647  memory: 654  \n",
            "01/10 13:39:47 - mmengine - INFO - Epoch(test) [ 60/151]    eta: 0:00:11  time: 0.1396  data_time: 0.0982  memory: 654  \n",
            "01/10 13:39:49 - mmengine - INFO - Epoch(test) [ 80/151]    eta: 0:00:08  time: 0.1059  data_time: 0.0582  memory: 654  \n",
            "01/10 13:39:51 - mmengine - INFO - Epoch(test) [100/151]    eta: 0:00:05  time: 0.0897  data_time: 0.0384  memory: 654  \n",
            "01/10 13:39:53 - mmengine - INFO - Epoch(test) [120/151]    eta: 0:00:03  time: 0.0867  data_time: 0.0310  memory: 654  \n",
            "01/10 13:39:55 - mmengine - INFO - Epoch(test) [140/151]    eta: 0:00:01  time: 0.0903  data_time: 0.0390  memory: 654  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9933)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9855)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9913)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9490)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9797)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9510)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0082)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0096)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0092)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0114)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0072)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0078)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0081)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0062)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0124)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0101)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0110)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0103)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0144)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0126)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0089)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0074)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0143)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0165)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0059)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0106)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0124)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0141)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0117)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0141)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0150)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0159)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0130)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0126)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0157)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0154)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0164)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0156)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0112)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0092)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0094)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0100)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0102)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0061)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0183)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0100)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0039)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0146)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0079)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0101)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0159)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0172)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0103)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0184)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0232)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0151)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0198)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0217)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0159)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0110)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0137)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0155)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0161)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0157)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0098)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0244)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0135)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0155)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0093)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0092)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6400)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6574)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6270)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6219)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5195)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6086)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6347)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0082)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0096)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0092)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0114)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0072)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0078)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0081)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0062)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0124)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0101)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0110)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0103)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0144)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0126)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0089)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0074)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0143)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0165)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0059)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0106)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0124)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0141)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0117)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0141)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0150)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0159)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0130)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0126)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0157)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0154)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0164)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0156)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0112)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0092)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0094)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0100)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0102)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0061)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0183)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0100)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0039)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0146)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0079)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0101)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0159)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0172)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0103)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0184)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0232)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0151)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0198)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0217)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0159)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0110)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0137)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0155)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0161)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0157)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0098)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0244)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0135)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0155)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0093)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0092)}]\n",
            "01/10 13:39:56 - mmengine - INFO - Epoch(test) [151/151]    SelfCustomMetric/accuracy: 1.0000  SelfCustomMetric/f1_score: 1.0000  SelfCustomMetric/recall: 1.0000  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.        , 0.07692308, 1.        , 1.        ]), array([1.9932655 , 0.9932655 , 0.5194988 , 0.00389518], dtype=float32))  data_time: 0.0590  time: 0.1072\n",
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 13:39:56 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 1836014926\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 1836014926\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 13:39:56 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_frames'\n",
            "data_root_val = '../group_project_data/output_frames'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 13:39:57 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 13:39:57 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 13:40:28 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 13:40:28 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 13:40:28 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 13:40:28 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 13:41:19 - mmengine - INFO - Epoch(train)  [1][20/28]  lr: 1.5625e-04  eta: 0:11:00  time: 2.5420  data_time: 1.8654  memory: 3713  grad_norm: 7.0838  loss: 0.6780  top1_acc: 0.5625  top5_acc: 1.0000  loss_cls: 0.6780\n",
            "01/10 13:41:39 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_133956\n",
            "01/10 13:41:39 - mmengine - INFO - Epoch(train)  [1][28/28]  lr: 1.5625e-04  eta: 0:10:37  time: 2.4714  data_time: 1.8199  memory: 3713  grad_norm: 7.2298  loss: 0.6661  top1_acc: 0.0000  top5_acc: 1.0000  loss_cls: 0.6661\n",
            "01/10 13:42:30 - mmengine - INFO - Epoch(train)  [2][20/28]  lr: 1.5625e-04  eta: 0:09:48  time: 2.5498  data_time: 1.8708  memory: 3713  grad_norm: 6.0480  loss: 0.5773  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.5773\n",
            "01/10 13:42:46 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_133956\n",
            "01/10 13:42:46 - mmengine - INFO - Epoch(train)  [2][28/28]  lr: 1.5625e-04  eta: 0:09:12  time: 2.2549  data_time: 1.6084  memory: 3713  grad_norm: 6.2015  loss: 0.5420  top1_acc: 0.5000  top5_acc: 1.0000  loss_cls: 0.5420\n",
            "01/10 13:43:41 - mmengine - INFO - Epoch(train)  [3][20/28]  lr: 1.5625e-04  eta: 0:08:37  time: 2.7425  data_time: 2.0468  memory: 3713  grad_norm: 5.7717  loss: 0.4700  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4700\n",
            "01/10 13:43:55 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_133956\n",
            "01/10 13:43:55 - mmengine - INFO - Epoch(train)  [3][28/28]  lr: 1.5625e-04  eta: 0:08:03  time: 2.3813  data_time: 1.7247  memory: 3713  grad_norm: 6.2993  loss: 0.4632  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.4632\n",
            "01/10 13:44:45 - mmengine - INFO - Epoch(train)  [4][20/28]  lr: 1.5625e-04  eta: 0:07:15  time: 2.5190  data_time: 1.8376  memory: 3713  grad_norm: 5.5656  loss: 0.4163  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4163\n",
            "01/10 13:45:02 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_133956\n",
            "01/10 13:45:02 - mmengine - INFO - Epoch(train)  [4][28/28]  lr: 1.5625e-04  eta: 0:06:51  time: 2.3199  data_time: 1.6676  memory: 3713  grad_norm: 6.4018  loss: 0.4304  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.4304\n",
            "01/10 13:45:53 - mmengine - INFO - Epoch(train)  [5][20/28]  lr: 1.5625e-04  eta: 0:06:04  time: 2.5274  data_time: 1.8446  memory: 3713  grad_norm: 5.3061  loss: 0.3563  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.3563\n",
            "01/10 13:46:10 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_133956\n",
            "01/10 13:46:10 - mmengine - INFO - Epoch(train)  [5][28/28]  lr: 1.5625e-04  eta: 0:05:42  time: 2.2463  data_time: 1.5939  memory: 3713  grad_norm: 5.4374  loss: 0.3397  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3397\n",
            "01/10 13:46:10 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 13:46:21 - mmengine - INFO - Epoch(val) [5][7/7]    acc/top1: 0.9907  acc/top5: 1.0000  acc/mean1: 0.9872  data_time: 1.2470  time: 1.4019\n",
            "01/10 13:46:21 - mmengine - INFO - The best checkpoint with 0.9907 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 13:47:14 - mmengine - INFO - Epoch(train)  [6][20/28]  lr: 1.5625e-04  eta: 0:04:55  time: 2.5596  data_time: 1.8771  memory: 3713  grad_norm: 5.2220  loss: 0.3152  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3152\n",
            "01/10 13:47:30 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_133956\n",
            "01/10 13:47:30 - mmengine - INFO - Epoch(train)  [6][28/28]  lr: 1.5625e-04  eta: 0:04:33  time: 2.3136  data_time: 1.6556  memory: 3713  grad_norm: 5.7213  loss: 0.3164  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3164\n",
            "01/10 13:48:19 - mmengine - INFO - Epoch(train)  [7][20/28]  lr: 1.5625e-04  eta: 0:03:44  time: 2.4432  data_time: 1.7638  memory: 3713  grad_norm: 5.1046  loss: 0.2704  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2704\n",
            "01/10 13:48:38 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_133956\n",
            "01/10 13:48:38 - mmengine - INFO - Epoch(train)  [7][28/28]  lr: 1.5625e-04  eta: 0:03:24  time: 2.3711  data_time: 1.7190  memory: 3713  grad_norm: 5.8511  loss: 0.2924  top1_acc: 0.5000  top5_acc: 1.0000  loss_cls: 0.2924\n",
            "01/10 13:49:29 - mmengine - INFO - Epoch(train)  [8][20/28]  lr: 1.5625e-04  eta: 0:02:36  time: 2.5456  data_time: 1.8676  memory: 3713  grad_norm: 5.6862  loss: 0.2662  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.2662\n",
            "01/10 13:49:45 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_133956\n",
            "01/10 13:49:45 - mmengine - INFO - Epoch(train)  [8][28/28]  lr: 1.5625e-04  eta: 0:02:16  time: 2.3856  data_time: 1.7337  memory: 3713  grad_norm: 5.0877  loss: 0.2348  top1_acc: 0.5000  top5_acc: 1.0000  loss_cls: 0.2348\n",
            "01/10 13:50:35 - mmengine - INFO - Epoch(train)  [9][20/28]  lr: 1.5625e-04  eta: 0:01:27  time: 2.4913  data_time: 1.8125  memory: 3713  grad_norm: 4.9943  loss: 0.2173  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2173\n",
            "01/10 13:50:52 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_133956\n",
            "01/10 13:50:52 - mmengine - INFO - Epoch(train)  [9][28/28]  lr: 1.5625e-04  eta: 0:01:08  time: 2.3239  data_time: 1.6740  memory: 3713  grad_norm: 4.7427  loss: 0.2011  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2011\n",
            "01/10 13:51:44 - mmengine - INFO - Epoch(train) [10][20/28]  lr: 1.5625e-04  eta: 0:00:19  time: 2.5867  data_time: 1.9096  memory: 3713  grad_norm: 5.7179  loss: 0.2266  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2266\n",
            "01/10 13:52:02 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_133956\n",
            "01/10 13:52:03 - mmengine - INFO - Epoch(train) [10][28/28]  lr: 1.5625e-04  eta: 0:00:00  time: 2.2377  data_time: 1.5889  memory: 3713  grad_norm: 6.5965  loss: 0.2527  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2527\n",
            "01/10 13:52:03 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 13:52:11 - mmengine - INFO - Epoch(val) [10][7/7]    acc/top1: 0.9352  acc/top5: 1.0000  acc/mean1: 0.9103  data_time: 1.0202  time: 1.1637\n",
            "01/10 13:52:14 - mmengine - INFO - Epoch(test) [ 20/132]    eta: 0:00:10  time: 0.0938  data_time: 0.0309  memory: 822  \n",
            "01/10 13:52:16 - mmengine - INFO - Epoch(test) [ 40/132]    eta: 0:00:07  time: 0.0797  data_time: 0.0031  memory: 656  \n",
            "01/10 13:52:18 - mmengine - INFO - Epoch(test) [ 60/132]    eta: 0:00:06  time: 0.1083  data_time: 0.0351  memory: 656  \n",
            "01/10 13:52:20 - mmengine - INFO - Epoch(test) [ 80/132]    eta: 0:00:05  time: 0.1046  data_time: 0.0551  memory: 656  \n",
            "01/10 13:52:22 - mmengine - INFO - Epoch(test) [100/132]    eta: 0:00:03  time: 0.0887  data_time: 0.0319  memory: 656  \n",
            "01/10 13:52:23 - mmengine - INFO - Epoch(test) [120/132]    eta: 0:00:01  time: 0.0905  data_time: 0.0421  memory: 780  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5104)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5335)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5089)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7375)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7306)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7890)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7018)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6797)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5275)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5648)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5691)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5178)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5290)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5521)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5806)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4743)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5743)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4101)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4302)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4385)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4429)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4167)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4239)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4130)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4159)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7721)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7439)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8713)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7449)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8356)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8012)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7990)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7928)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6952)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8980)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8714)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7806)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7904)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8630)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9715)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9476)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9334)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7960)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8259)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8516)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8325)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8824)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8227)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8438)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8265)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8551)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8533)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9596)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8472)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7466)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8034)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8497)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9702)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9226)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9657)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9174)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9217)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7780)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0523)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0375)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0382)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0400)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0498)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0476)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0528)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0411)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0212)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0472)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0390)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0272)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0356)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0540)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0397)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0332)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0266)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0335)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0389)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0293)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0214)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0253)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0330)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0338)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0496)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0453)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0352)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0460)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0579)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0495)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0419)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0519)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0397)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0446)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0384)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0427)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0364)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0385)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0304)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0388)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0375)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0505)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0395)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0212)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0471)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0274)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0427)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0329)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0292)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0360)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0380)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0526)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0565)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0749)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0457)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0535)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0621)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0524)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0467)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0453)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0457)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0391)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0679)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0313)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0611)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0503)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0510)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0304)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0490)}]\n",
            "01/10 13:52:25 - mmengine - INFO - Epoch(test) [132/132]    SelfCustomMetric/accuracy: 0.9318  SelfCustomMetric/f1_score: 0.9231  SelfCustomMetric/recall: 0.8571  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.        , 0.01587302, 1.        , 1.        ]), array([1.9714692 , 0.97146916, 0.41007832, 0.02119327], dtype=float32))  data_time: 0.0364  time: 0.0967\n"
          ]
        }
      ],
      "source": [
        "import os.path as osp\n",
        "import mmengine\n",
        "from mmengine.runner import Runner\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "result_list = []\n",
        "# split the datasets\n",
        "for i in range(len(random_seeds)):\n",
        "  set_seeds(random_seeds[i])\n",
        "  yaml_path = '../group_project_data/sample.yaml'\n",
        "  annotation_path = './Annotations'\n",
        "  spilt_dataset_and_create_annotations(yaml_path, annotation_path, do_oversampling=True)\n",
        "\n",
        "  # Create work_dir\n",
        "  mmengine.mkdir_or_exist(osp.abspath(cfg_raw.work_dir))\n",
        "\n",
        "  # build the runner\n",
        "  runner = Runner.from_cfg(cfg_raw)\n",
        "\n",
        "  # start training\n",
        "  runner.train()\n",
        "\n",
        "  result_list.append(runner.test())\n",
        "\n",
        "result_dict['Oversampling'] = result_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_t5hpi-fDoq"
      },
      "source": [
        "## Raw Frames without Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YVdd_CjfQek",
        "outputId": "c81e2aa8-aa81-4583-c103-40fd4fb1f1a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 14:07:47 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 1165313289\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 1165313289\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 14:07:47 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_frames'\n",
            "data_root_val = '../group_project_data/output_frames'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 14:07:48 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 14:07:48 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 14:08:19 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 14:08:19 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 14:08:19 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 14:08:19 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 14:09:03 - mmengine - INFO - Epoch(train)  [1][20/41]  lr: 1.5625e-04  eta: 0:14:14  time: 2.1902  data_time: 1.5266  memory: 3711  grad_norm: 7.4482  loss: 0.7139  top1_acc: 0.4375  top5_acc: 1.0000  loss_cls: 0.7139\n",
            "01/10 14:09:43 - mmengine - INFO - Epoch(train)  [1][40/41]  lr: 1.5625e-04  eta: 0:12:58  time: 2.0178  data_time: 1.3467  memory: 3711  grad_norm: 6.6584  loss: 0.6252  top1_acc: 0.6875  top5_acc: 1.0000  loss_cls: 0.6252\n",
            "01/10 14:09:44 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_140747\n",
            "01/10 14:09:44 - mmengine - INFO - Epoch(train)  [1][41/41]  lr: 1.5625e-04  eta: 0:12:42  time: 1.8900  data_time: 1.2269  memory: 3331  grad_norm: 6.6699  loss: 0.6204  top1_acc: 0.6429  top5_acc: 1.0000  loss_cls: 0.6204\n",
            "01/10 14:10:26 - mmengine - INFO - Epoch(train)  [2][20/41]  lr: 1.5625e-04  eta: 0:12:03  time: 2.0871  data_time: 1.4064  memory: 3711  grad_norm: 6.6392  loss: 0.5550  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.5550\n",
            "01/10 14:11:08 - mmengine - INFO - Epoch(train)  [2][40/41]  lr: 1.5625e-04  eta: 0:11:28  time: 2.1459  data_time: 1.4650  memory: 3711  grad_norm: 6.3679  loss: 0.5160  top1_acc: 0.8125  top5_acc: 1.0000  loss_cls: 0.5160\n",
            "01/10 14:11:10 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_140747\n",
            "01/10 14:11:10 - mmengine - INFO - Epoch(train)  [2][41/41]  lr: 1.5625e-04  eta: 0:11:23  time: 2.0796  data_time: 1.4055  memory: 3331  grad_norm: 6.4204  loss: 0.5165  top1_acc: 0.5714  top5_acc: 1.0000  loss_cls: 0.5165\n",
            "01/10 14:11:52 - mmengine - INFO - Epoch(train)  [3][20/41]  lr: 1.5625e-04  eta: 0:10:43  time: 2.1117  data_time: 1.4314  memory: 3711  grad_norm: 6.4698  loss: 0.4664  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4664\n",
            "01/10 14:12:33 - mmengine - INFO - Epoch(train)  [3][40/41]  lr: 1.5625e-04  eta: 0:10:00  time: 2.0580  data_time: 1.3823  memory: 3711  grad_norm: 6.2810  loss: 0.4233  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.4233\n",
            "01/10 14:12:34 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_140747\n",
            "01/10 14:12:34 - mmengine - INFO - Epoch(train)  [3][41/41]  lr: 1.5625e-04  eta: 0:09:54  time: 1.8662  data_time: 1.1988  memory: 3331  grad_norm: 6.3876  loss: 0.4238  top1_acc: 0.7857  top5_acc: 1.0000  loss_cls: 0.4238\n",
            "01/10 14:13:18 - mmengine - INFO - Epoch(train)  [4][20/41]  lr: 1.5625e-04  eta: 0:09:18  time: 2.2051  data_time: 1.5254  memory: 3711  grad_norm: 6.0890  loss: 0.3704  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.3704\n",
            "01/10 14:13:58 - mmengine - INFO - Epoch(train)  [4][40/41]  lr: 1.5625e-04  eta: 0:08:33  time: 1.9919  data_time: 1.3237  memory: 3711  grad_norm: 5.5838  loss: 0.3124  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3124\n",
            "01/10 14:13:59 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_140747\n",
            "01/10 14:13:59 - mmengine - INFO - Epoch(train)  [4][41/41]  lr: 1.5625e-04  eta: 0:08:30  time: 1.9715  data_time: 1.3076  memory: 3331  grad_norm: 5.4555  loss: 0.3027  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3027\n",
            "01/10 14:14:42 - mmengine - INFO - Epoch(train)  [5][20/41]  lr: 1.5625e-04  eta: 0:07:49  time: 2.1120  data_time: 1.4353  memory: 3711  grad_norm: 5.8071  loss: 0.2886  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.2886\n",
            "01/10 14:15:21 - mmengine - INFO - Epoch(train)  [5][40/41]  lr: 1.5625e-04  eta: 0:07:05  time: 1.9697  data_time: 1.2901  memory: 3711  grad_norm: 5.7752  loss: 0.2698  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2698\n",
            "01/10 14:15:24 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_140747\n",
            "01/10 14:15:24 - mmengine - INFO - Epoch(train)  [5][41/41]  lr: 1.5625e-04  eta: 0:07:04  time: 1.9838  data_time: 1.3085  memory: 3331  grad_norm: 5.8075  loss: 0.2689  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2689\n",
            "01/10 14:15:24 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 14:15:27 - mmengine - INFO - Epoch(val) [5][2/2]    acc/top1: 1.0000  acc/top5: 1.0000  acc/mean1: 1.0000  data_time: 1.0110  time: 1.1173\n",
            "01/10 14:15:28 - mmengine - INFO - The best checkpoint with 1.0000 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 14:16:13 - mmengine - INFO - Epoch(train)  [6][20/41]  lr: 1.5625e-04  eta: 0:06:26  time: 2.2358  data_time: 1.5537  memory: 3711  grad_norm: 5.6294  loss: 0.2542  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2542\n",
            "01/10 14:16:55 - mmengine - INFO - Epoch(train)  [6][40/41]  lr: 1.5625e-04  eta: 0:05:44  time: 2.0810  data_time: 1.4014  memory: 3711  grad_norm: 5.0313  loss: 0.2208  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2208\n",
            "01/10 14:16:56 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_140747\n",
            "01/10 14:16:56 - mmengine - INFO - Epoch(train)  [6][41/41]  lr: 1.5625e-04  eta: 0:05:41  time: 2.0752  data_time: 1.4001  memory: 3331  grad_norm: 4.9452  loss: 0.2144  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2144\n",
            "01/10 14:17:38 - mmengine - INFO - Epoch(train)  [7][20/41]  lr: 1.5625e-04  eta: 0:05:00  time: 2.1375  data_time: 1.4546  memory: 3711  grad_norm: 4.9921  loss: 0.1973  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1973\n",
            "01/10 14:18:19 - mmengine - INFO - Epoch(train)  [7][40/41]  lr: 1.5625e-04  eta: 0:04:18  time: 2.0329  data_time: 1.3578  memory: 3711  grad_norm: 5.1880  loss: 0.2063  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2063\n",
            "01/10 14:18:20 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_140747\n",
            "01/10 14:18:20 - mmengine - INFO - Epoch(train)  [7][41/41]  lr: 1.5625e-04  eta: 0:04:15  time: 1.8782  data_time: 1.2106  memory: 3331  grad_norm: 5.3557  loss: 0.2076  top1_acc: 0.8571  top5_acc: 1.0000  loss_cls: 0.2076\n",
            "01/10 14:19:03 - mmengine - INFO - Epoch(train)  [8][20/41]  lr: 1.5625e-04  eta: 0:03:34  time: 2.1462  data_time: 1.4719  memory: 3711  grad_norm: 5.0139  loss: 0.1717  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.1717\n",
            "01/10 14:19:44 - mmengine - INFO - Epoch(train)  [8][40/41]  lr: 1.5625e-04  eta: 0:02:52  time: 2.0452  data_time: 1.3648  memory: 3711  grad_norm: 4.5640  loss: 0.1597  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1597\n",
            "01/10 14:19:46 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_140747\n",
            "01/10 14:19:46 - mmengine - INFO - Epoch(train)  [8][41/41]  lr: 1.5625e-04  eta: 0:02:50  time: 1.9899  data_time: 1.3162  memory: 3331  grad_norm: 4.6187  loss: 0.1592  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1592\n",
            "01/10 14:20:28 - mmengine - INFO - Epoch(train)  [9][20/41]  lr: 1.5625e-04  eta: 0:02:09  time: 2.1150  data_time: 1.4337  memory: 3711  grad_norm: 5.5394  loss: 0.1816  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1816\n",
            "01/10 14:21:11 - mmengine - INFO - Epoch(train)  [9][40/41]  lr: 1.5625e-04  eta: 0:01:27  time: 2.1330  data_time: 1.4645  memory: 3711  grad_norm: 5.4972  loss: 0.1627  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.1627\n",
            "01/10 14:21:12 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_140747\n",
            "01/10 14:21:12 - mmengine - INFO - Epoch(train)  [9][41/41]  lr: 1.5625e-04  eta: 0:01:25  time: 2.0872  data_time: 1.4228  memory: 3331  grad_norm: 5.6202  loss: 0.1668  top1_acc: 0.7857  top5_acc: 1.0000  loss_cls: 0.1668\n",
            "01/10 14:21:53 - mmengine - INFO - Epoch(train) [10][20/41]  lr: 1.5625e-04  eta: 0:00:43  time: 2.0354  data_time: 1.3561  memory: 3711  grad_norm: 4.5558  loss: 0.1351  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1351\n",
            "01/10 14:22:35 - mmengine - INFO - Epoch(train) [10][40/41]  lr: 1.5625e-04  eta: 0:00:02  time: 2.0964  data_time: 1.4214  memory: 3711  grad_norm: 4.5178  loss: 0.1297  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1297\n",
            "01/10 14:22:36 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_140747\n",
            "01/10 14:22:36 - mmengine - INFO - Epoch(train) [10][41/41]  lr: 1.5625e-04  eta: 0:00:00  time: 2.0672  data_time: 1.3970  memory: 3331  grad_norm: 4.3006  loss: 0.1231  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1231\n",
            "01/10 14:22:36 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 14:22:39 - mmengine - INFO - Epoch(val) [10][2/2]    acc/top1: 1.0000  acc/top5: 1.0000  acc/mean1: 1.0000  data_time: 0.6520  time: 0.7335\n",
            "01/10 14:22:43 - mmengine - INFO - Epoch(test) [ 20/109]    eta: 0:00:14  time: 0.1638  data_time: 0.1149  memory: 820  \n",
            "01/10 14:22:46 - mmengine - INFO - Epoch(test) [ 40/109]    eta: 0:00:11  time: 0.1656  data_time: 0.1193  memory: 654  \n",
            "01/10 14:22:48 - mmengine - INFO - Epoch(test) [ 60/109]    eta: 0:00:06  time: 0.0841  data_time: 0.0300  memory: 654  \n",
            "01/10 14:22:50 - mmengine - INFO - Epoch(test) [ 80/109]    eta: 0:00:03  time: 0.0854  data_time: 0.0348  memory: 654  \n",
            "01/10 14:22:51 - mmengine - INFO - Epoch(test) [100/109]    eta: 0:00:01  time: 0.0897  data_time: 0.0412  memory: 654  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9911)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9839)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9861)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9780)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9715)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9632)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9776)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9708)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9712)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9651)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9827)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9752)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9848)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9866)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9773)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9905)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9956)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9900)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9966)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9915)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9979)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9859)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9761)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9497)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9964)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9946)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9964)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9942)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9921)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9850)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9836)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9796)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9905)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9849)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9770)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9624)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9829)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7589)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6641)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7004)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0059)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0023)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0044)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0058)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0096)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0036)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0032)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0032)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0020)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0045)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0028)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0023)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0018)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0062)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0049)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0028)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0026)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0039)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0051)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0020)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0032)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0031)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0048)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0026)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0035)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0061)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0029)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0031)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0038)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0080)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0062)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0016)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0048)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0045)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0032)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0033)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0040)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0059)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0025)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0020)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0070)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0038)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0018)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0076)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0021)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0014)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0036)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0024)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0034)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0048)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0036)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0065)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0175)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0148)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0217)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0191)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0040)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0018)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0036)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0047)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0053)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0058)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0042)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0242)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0049)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0067)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0034)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0031)}]\n",
            "01/10 14:22:52 - mmengine - INFO - Epoch(test) [109/109]    SelfCustomMetric/accuracy: 1.0000  SelfCustomMetric/f1_score: 1.0000  SelfCustomMetric/recall: 1.0000  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.   , 0.025, 1.   , 1.   ]), array([1.9978564e+00, 9.9785632e-01, 6.6413343e-01, 1.3815074e-03],\n",
            "      dtype=float32))  data_time: 0.0647  time: 0.1148\n",
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 14:22:52 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 1608637542\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 1608637542\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 14:22:53 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_frames'\n",
            "data_root_val = '../group_project_data/output_frames'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 14:22:53 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 14:22:53 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 14:23:24 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 14:23:24 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 14:23:24 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 14:23:24 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 14:24:08 - mmengine - INFO - Epoch(train)  [1][20/38]  lr: 1.5625e-04  eta: 0:13:15  time: 2.2103  data_time: 1.5398  memory: 3711  grad_norm: 7.3525  loss: 0.7117  top1_acc: 0.3750  top5_acc: 1.0000  loss_cls: 0.7117\n",
            "01/10 14:24:45 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_142252\n",
            "01/10 14:24:45 - mmengine - INFO - Epoch(train)  [1][38/38]  lr: 1.5625e-04  eta: 0:12:08  time: 2.1206  data_time: 1.4483  memory: 3711  grad_norm: 7.1304  loss: 0.6265  top1_acc: 0.4615  top5_acc: 1.0000  loss_cls: 0.6265\n",
            "01/10 14:25:29 - mmengine - INFO - Epoch(train)  [2][20/38]  lr: 1.5625e-04  eta: 0:11:33  time: 2.2002  data_time: 1.5275  memory: 3711  grad_norm: 6.5012  loss: 0.5611  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.5611\n",
            "01/10 14:26:09 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_142252\n",
            "01/10 14:26:09 - mmengine - INFO - Epoch(train)  [2][38/38]  lr: 1.5625e-04  eta: 0:11:00  time: 2.2028  data_time: 1.5317  memory: 3711  grad_norm: 6.2807  loss: 0.4903  top1_acc: 0.7692  top5_acc: 1.0000  loss_cls: 0.4903\n",
            "01/10 14:26:54 - mmengine - INFO - Epoch(train)  [3][20/38]  lr: 1.5625e-04  eta: 0:10:19  time: 2.2147  data_time: 1.5332  memory: 3711  grad_norm: 6.4568  loss: 0.4523  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4523\n",
            "01/10 14:27:30 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_142252\n",
            "01/10 14:27:30 - mmengine - INFO - Epoch(train)  [3][38/38]  lr: 1.5625e-04  eta: 0:09:34  time: 2.0168  data_time: 1.3441  memory: 3711  grad_norm: 5.6266  loss: 0.3735  top1_acc: 0.9231  top5_acc: 1.0000  loss_cls: 0.3735\n",
            "01/10 14:28:14 - mmengine - INFO - Epoch(train)  [4][20/38]  lr: 1.5625e-04  eta: 0:08:52  time: 2.1920  data_time: 1.5246  memory: 3711  grad_norm: 5.4323  loss: 0.3279  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3279\n",
            "01/10 14:28:53 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_142252\n",
            "01/10 14:28:53 - mmengine - INFO - Epoch(train)  [4][38/38]  lr: 1.5625e-04  eta: 0:08:13  time: 2.1132  data_time: 1.4425  memory: 3711  grad_norm: 5.9673  loss: 0.3148  top1_acc: 0.9231  top5_acc: 1.0000  loss_cls: 0.3148\n",
            "01/10 14:29:39 - mmengine - INFO - Epoch(train)  [5][20/38]  lr: 1.5625e-04  eta: 0:07:33  time: 2.2807  data_time: 1.5965  memory: 3711  grad_norm: 5.6327  loss: 0.2807  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2807\n",
            "01/10 14:30:14 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_142252\n",
            "01/10 14:30:14 - mmengine - INFO - Epoch(train)  [5][38/38]  lr: 1.5625e-04  eta: 0:06:49  time: 2.0624  data_time: 1.3934  memory: 3711  grad_norm: 5.5809  loss: 0.2443  top1_acc: 0.9231  top5_acc: 1.0000  loss_cls: 0.2443\n",
            "01/10 14:30:14 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 14:30:18 - mmengine - INFO - Epoch(val) [5][4/4]    acc/top1: 0.9038  acc/top5: 1.0000  acc/mean1: 0.4519  data_time: 0.8181  time: 0.9389\n",
            "01/10 14:30:19 - mmengine - INFO - The best checkpoint with 0.9038 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 14:31:10 - mmengine - INFO - Epoch(train)  [6][20/38]  lr: 1.5625e-04  eta: 0:06:11  time: 2.4783  data_time: 1.7932  memory: 3711  grad_norm: 5.6216  loss: 0.2467  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2467\n",
            "01/10 14:31:44 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_142252\n",
            "01/10 14:31:44 - mmengine - INFO - Epoch(train)  [6][38/38]  lr: 1.5625e-04  eta: 0:05:28  time: 1.9874  data_time: 1.3158  memory: 3711  grad_norm: 5.4931  loss: 0.2281  top1_acc: 0.9231  top5_acc: 1.0000  loss_cls: 0.2281\n",
            "01/10 14:32:30 - mmengine - INFO - Epoch(train)  [7][20/38]  lr: 1.5625e-04  eta: 0:04:46  time: 2.3054  data_time: 1.6248  memory: 3711  grad_norm: 5.5746  loss: 0.2122  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2122\n",
            "01/10 14:33:04 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_142252\n",
            "01/10 14:33:04 - mmengine - INFO - Epoch(train)  [7][38/38]  lr: 1.5625e-04  eta: 0:04:05  time: 1.9441  data_time: 1.2749  memory: 3711  grad_norm: 5.1919  loss: 0.1855  top1_acc: 0.8462  top5_acc: 1.0000  loss_cls: 0.1855\n",
            "01/10 14:33:49 - mmengine - INFO - Epoch(train)  [8][20/38]  lr: 1.5625e-04  eta: 0:03:23  time: 2.2075  data_time: 1.5269  memory: 3711  grad_norm: 4.9793  loss: 0.1708  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1708\n",
            "01/10 14:34:25 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_142252\n",
            "01/10 14:34:25 - mmengine - INFO - Epoch(train)  [8][38/38]  lr: 1.5625e-04  eta: 0:02:43  time: 2.0092  data_time: 1.3404  memory: 3711  grad_norm: 4.7353  loss: 0.1630  top1_acc: 0.9231  top5_acc: 1.0000  loss_cls: 0.1630\n",
            "01/10 14:35:11 - mmengine - INFO - Epoch(train)  [9][20/38]  lr: 1.5625e-04  eta: 0:02:01  time: 2.2955  data_time: 1.6185  memory: 3711  grad_norm: 3.8533  loss: 0.1223  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1223\n",
            "01/10 14:35:48 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_142252\n",
            "01/10 14:35:48 - mmengine - INFO - Epoch(train)  [9][38/38]  lr: 1.5625e-04  eta: 0:01:21  time: 2.0364  data_time: 1.3657  memory: 3711  grad_norm: 4.1985  loss: 0.1353  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1353\n",
            "01/10 14:36:35 - mmengine - INFO - Epoch(train) [10][20/38]  lr: 1.5625e-04  eta: 0:00:38  time: 2.3572  data_time: 1.6800  memory: 3711  grad_norm: 4.5518  loss: 0.1297  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1297\n",
            "01/10 14:37:11 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_142252\n",
            "01/10 14:37:11 - mmengine - INFO - Epoch(train) [10][38/38]  lr: 1.5625e-04  eta: 0:00:00  time: 2.0474  data_time: 1.3792  memory: 3711  grad_norm: 5.3775  loss: 0.1449  top1_acc: 0.7692  top5_acc: 1.0000  loss_cls: 0.1449\n",
            "01/10 14:37:11 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 14:37:15 - mmengine - INFO - Epoch(val) [10][4/4]    acc/top1: 0.9808  acc/top5: 1.0000  acc/mean1: 0.4904  data_time: 0.5868  time: 0.6925\n",
            "01/10 14:37:15 - mmengine - INFO - The previous best checkpoint /content/mmaction2/tutorial_exps/best_acc_top1_epoch_5.pth is removed\n",
            "01/10 14:37:16 - mmengine - INFO - The best checkpoint with 0.9808 acc/top1 at 10 epoch is saved to best_acc_top1_epoch_10.pth.\n",
            "01/10 14:37:19 - mmengine - INFO - Epoch(test) [ 20/146]    eta: 0:00:10  time: 0.0869  data_time: 0.0117  memory: 819  \n",
            "01/10 14:37:23 - mmengine - INFO - Epoch(test) [ 40/146]    eta: 0:00:13  time: 0.1678  data_time: 0.1119  memory: 653  \n",
            "01/10 14:37:25 - mmengine - INFO - Epoch(test) [ 60/146]    eta: 0:00:10  time: 0.1011  data_time: 0.0472  memory: 653  \n",
            "01/10 14:37:26 - mmengine - INFO - Epoch(test) [ 80/146]    eta: 0:00:07  time: 0.0815  data_time: 0.0206  memory: 653  \n",
            "01/10 14:37:28 - mmengine - INFO - Epoch(test) [100/146]    eta: 0:00:04  time: 0.0956  data_time: 0.0361  memory: 653  \n",
            "01/10 14:37:31 - mmengine - INFO - Epoch(test) [120/146]    eta: 0:00:02  time: 0.1218  data_time: 0.0689  memory: 653  \n",
            "01/10 14:37:33 - mmengine - INFO - Epoch(test) [140/146]    eta: 0:00:00  time: 0.1240  data_time: 0.0731  memory: 654  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8742)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9236)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9224)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8359)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9263)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8815)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8905)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8658)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8265)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9589)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9571)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8760)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9006)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8846)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9714)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9289)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9702)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8937)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9174)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9066)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8247)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9212)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8591)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9771)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9126)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8959)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9212)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9745)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9529)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9351)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9055)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8748)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.3609)}, {'pred': tensor(1), 'gt': tensor(0), 'pd_score': tensor(0.5536)}, {'pred': tensor(1), 'gt': tensor(0), 'pd_score': tensor(0.5434)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.2601)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.2393)}, {'pred': tensor(1), 'gt': tensor(0), 'pd_score': tensor(0.5375)}, {'pred': tensor(1), 'gt': tensor(0), 'pd_score': tensor(0.6172)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.4188)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.3719)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9360)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9389)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9700)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9041)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8768)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9044)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9144)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0214)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0160)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0142)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0115)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0099)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0121)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0084)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0083)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0082)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0113)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0088)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0098)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0127)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0103)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0105)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0083)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0163)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0227)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0295)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0269)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0233)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0154)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0135)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0217)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0150)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0187)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0223)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0234)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0154)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0214)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0212)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0207)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0223)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0151)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0183)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0192)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0222)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0278)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0075)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0073)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0089)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0056)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0089)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0109)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0136)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0128)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0153)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0153)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0122)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0124)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0117)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0114)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0110)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0106)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0151)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0157)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0162)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0149)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0157)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0163)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0174)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0180)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0166)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0145)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0158)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0161)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0151)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0273)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0116)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0128)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0168)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0195)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0267)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0219)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0088)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0129)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0128)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0115)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0076)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0081)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0056)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0070)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0133)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0086)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0068)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0083)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0066)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0142)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0171)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0394)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0198)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0141)}]\n",
            "01/10 14:37:34 - mmengine - INFO - Epoch(test) [146/146]    SelfCustomMetric/accuracy: 0.9726  SelfCustomMetric/f1_score: 0.9512  SelfCustomMetric/recall: 1.0000  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.        , 0.02564103, 1.        , 1.        ]), array([1.9770706 , 0.97707057, 0.82472414, 0.00555627], dtype=float32))  data_time: 0.0517  time: 0.1100\n",
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 14:37:34 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 2124297904\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 2124297904\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 14:37:34 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_frames'\n",
            "data_root_val = '../group_project_data/output_frames'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 14:37:35 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 14:37:35 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 14:38:06 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 14:38:06 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 14:38:06 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 14:38:06 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 14:38:42 - mmengine - INFO - Epoch(train)  [1][20/41]  lr: 1.5625e-04  eta: 0:11:53  time: 1.8307  data_time: 1.1666  memory: 3713  grad_norm: 6.8298  loss: 0.6626  top1_acc: 0.4375  top5_acc: 1.0000  loss_cls: 0.6626\n",
            "01/10 14:39:18 - mmengine - INFO - Epoch(train)  [1][40/41]  lr: 1.5625e-04  eta: 0:11:11  time: 1.8013  data_time: 1.1245  memory: 3713  grad_norm: 7.0084  loss: 0.5967  top1_acc: 0.8125  top5_acc: 1.0000  loss_cls: 0.5967\n",
            "01/10 14:39:19 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_143734\n",
            "01/10 14:39:19 - mmengine - INFO - Epoch(train)  [1][41/41]  lr: 1.5625e-04  eta: 0:10:57  time: 1.7182  data_time: 1.0510  memory: 2727  grad_norm: 7.0156  loss: 0.5909  top1_acc: 0.8182  top5_acc: 1.0000  loss_cls: 0.5909\n",
            "01/10 14:39:57 - mmengine - INFO - Epoch(train)  [2][20/41]  lr: 1.5625e-04  eta: 0:10:36  time: 1.9057  data_time: 1.2299  memory: 3713  grad_norm: 6.2924  loss: 0.5233  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.5233\n",
            "01/10 14:40:30 - mmengine - INFO - Epoch(train)  [2][40/41]  lr: 1.5625e-04  eta: 0:09:47  time: 1.6709  data_time: 1.0043  memory: 3713  grad_norm: 6.2106  loss: 0.4782  top1_acc: 0.6250  top5_acc: 1.0000  loss_cls: 0.4782\n",
            "01/10 14:40:31 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_143734\n",
            "01/10 14:40:31 - mmengine - INFO - Epoch(train)  [2][41/41]  lr: 1.5625e-04  eta: 0:09:41  time: 1.6687  data_time: 1.0120  memory: 2727  grad_norm: 6.1995  loss: 0.4754  top1_acc: 0.9091  top5_acc: 1.0000  loss_cls: 0.4754\n",
            "01/10 14:41:10 - mmengine - INFO - Epoch(train)  [3][20/41]  lr: 1.5625e-04  eta: 0:09:15  time: 1.9357  data_time: 1.2588  memory: 3713  grad_norm: 6.0355  loss: 0.4193  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.4193\n",
            "01/10 14:41:45 - mmengine - INFO - Epoch(train)  [3][40/41]  lr: 1.5625e-04  eta: 0:08:37  time: 1.7688  data_time: 1.1002  memory: 3713  grad_norm: 6.0044  loss: 0.3892  top1_acc: 0.8125  top5_acc: 1.0000  loss_cls: 0.3892\n",
            "01/10 14:41:46 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_143734\n",
            "01/10 14:41:46 - mmengine - INFO - Epoch(train)  [3][41/41]  lr: 1.5625e-04  eta: 0:08:32  time: 1.5880  data_time: 0.9322  memory: 2727  grad_norm: 6.2100  loss: 0.3935  top1_acc: 0.9091  top5_acc: 1.0000  loss_cls: 0.3935\n",
            "01/10 14:42:23 - mmengine - INFO - Epoch(train)  [4][20/41]  lr: 1.5625e-04  eta: 0:08:00  time: 1.8725  data_time: 1.2009  memory: 3713  grad_norm: 5.6242  loss: 0.3439  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.3439\n",
            "01/10 14:42:58 - mmengine - INFO - Epoch(train)  [4][40/41]  lr: 1.5625e-04  eta: 0:07:23  time: 1.7559  data_time: 1.0908  memory: 3713  grad_norm: 5.6511  loss: 0.3035  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.3035\n",
            "01/10 14:42:59 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_143734\n",
            "01/10 14:42:59 - mmengine - INFO - Epoch(train)  [4][41/41]  lr: 1.5625e-04  eta: 0:07:19  time: 1.5791  data_time: 0.9268  memory: 2727  grad_norm: 5.6680  loss: 0.3012  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3012\n",
            "01/10 14:43:36 - mmengine - INFO - Epoch(train)  [5][20/41]  lr: 1.5625e-04  eta: 0:06:45  time: 1.8453  data_time: 1.1719  memory: 3713  grad_norm: 5.4229  loss: 0.2715  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2715\n",
            "01/10 14:44:10 - mmengine - INFO - Epoch(train)  [5][40/41]  lr: 1.5625e-04  eta: 0:06:07  time: 1.7235  data_time: 1.0552  memory: 3713  grad_norm: 5.7953  loss: 0.2538  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2538\n",
            "01/10 14:44:11 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_143734\n",
            "01/10 14:44:11 - mmengine - INFO - Epoch(train)  [5][41/41]  lr: 1.5625e-04  eta: 0:06:04  time: 1.7129  data_time: 1.0540  memory: 2727  grad_norm: 6.0277  loss: 0.2639  top1_acc: 0.7273  top5_acc: 1.0000  loss_cls: 0.2639\n",
            "01/10 14:44:11 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 14:44:13 - mmengine - INFO - Epoch(val) [5][3/3]    acc/top1: 0.5556  acc/top5: 1.0000  acc/mean1: 0.2778  data_time: 0.5082  time: 0.6161\n",
            "01/10 14:44:14 - mmengine - INFO - The best checkpoint with 0.5556 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 14:44:51 - mmengine - INFO - Epoch(train)  [6][20/41]  lr: 1.5625e-04  eta: 0:05:28  time: 1.7343  data_time: 1.0682  memory: 3713  grad_norm: 5.9103  loss: 0.2400  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2400\n",
            "01/10 14:45:27 - mmengine - INFO - Epoch(train)  [6][40/41]  lr: 1.5625e-04  eta: 0:04:53  time: 1.8262  data_time: 1.1565  memory: 3713  grad_norm: 5.4376  loss: 0.2069  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.2069\n",
            "01/10 14:45:29 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_143734\n",
            "01/10 14:45:29 - mmengine - INFO - Epoch(train)  [6][41/41]  lr: 1.5625e-04  eta: 0:04:51  time: 1.8016  data_time: 1.1433  memory: 2727  grad_norm: 5.3505  loss: 0.1983  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1983\n",
            "01/10 14:46:06 - mmengine - INFO - Epoch(train)  [7][20/41]  lr: 1.5625e-04  eta: 0:04:16  time: 1.8494  data_time: 1.1785  memory: 3713  grad_norm: 5.3319  loss: 0.1933  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1933\n",
            "01/10 14:46:42 - mmengine - INFO - Epoch(train)  [7][40/41]  lr: 1.5625e-04  eta: 0:03:41  time: 1.7881  data_time: 1.1170  memory: 3713  grad_norm: 5.5075  loss: 0.1901  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1901\n",
            "01/10 14:46:43 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_143734\n",
            "01/10 14:46:43 - mmengine - INFO - Epoch(train)  [7][41/41]  lr: 1.5625e-04  eta: 0:03:39  time: 1.8255  data_time: 1.1643  memory: 2727  grad_norm: 5.5741  loss: 0.1908  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1908\n",
            "01/10 14:47:19 - mmengine - INFO - Epoch(train)  [8][20/41]  lr: 1.5625e-04  eta: 0:03:03  time: 1.8212  data_time: 1.1498  memory: 3713  grad_norm: 4.9074  loss: 0.1574  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1574\n",
            "01/10 14:47:54 - mmengine - INFO - Epoch(train)  [8][40/41]  lr: 1.5625e-04  eta: 0:02:27  time: 1.7283  data_time: 1.0570  memory: 3713  grad_norm: 4.6649  loss: 0.1501  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1501\n",
            "01/10 14:47:55 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_143734\n",
            "01/10 14:47:55 - mmengine - INFO - Epoch(train)  [8][41/41]  lr: 1.5625e-04  eta: 0:02:25  time: 1.7387  data_time: 1.0772  memory: 2727  grad_norm: 4.7939  loss: 0.1557  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1557\n",
            "01/10 14:48:33 - mmengine - INFO - Epoch(train)  [9][20/41]  lr: 1.5625e-04  eta: 0:01:50  time: 1.9313  data_time: 1.2545  memory: 3713  grad_norm: 4.3419  loss: 0.1309  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1309\n",
            "01/10 14:49:08 - mmengine - INFO - Epoch(train)  [9][40/41]  lr: 1.5625e-04  eta: 0:01:14  time: 1.7117  data_time: 1.0372  memory: 3713  grad_norm: 5.2763  loss: 0.1456  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1456\n",
            "01/10 14:49:08 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_143734\n",
            "01/10 14:49:08 - mmengine - INFO - Epoch(train)  [9][41/41]  lr: 1.5625e-04  eta: 0:01:12  time: 1.6998  data_time: 1.0359  memory: 2727  grad_norm: 5.2332  loss: 0.1465  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1465\n",
            "01/10 14:49:48 - mmengine - INFO - Epoch(train) [10][20/41]  lr: 1.5625e-04  eta: 0:00:37  time: 2.0108  data_time: 1.3421  memory: 3713  grad_norm: 3.9880  loss: 0.1094  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1094\n",
            "01/10 14:50:21 - mmengine - INFO - Epoch(train) [10][40/41]  lr: 1.5625e-04  eta: 0:00:01  time: 1.6322  data_time: 0.9661  memory: 3713  grad_norm: 5.3059  loss: 0.1489  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1489\n",
            "01/10 14:50:22 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_143734\n",
            "01/10 14:50:22 - mmengine - INFO - Epoch(train) [10][41/41]  lr: 1.5625e-04  eta: 0:00:00  time: 1.6222  data_time: 0.9658  memory: 2727  grad_norm: 5.0912  loss: 0.1399  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1399\n",
            "01/10 14:50:22 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 14:50:24 - mmengine - INFO - Epoch(val) [10][3/3]    acc/top1: 0.5278  acc/top5: 1.0000  acc/mean1: 0.2639  data_time: 0.3528  time: 0.4442\n",
            "01/10 14:50:31 - mmengine - INFO - Epoch(test) [ 20/140]    eta: 0:00:40  time: 0.3388  data_time: 0.3029  memory: 822  \n",
            "01/10 14:50:38 - mmengine - INFO - Epoch(test) [ 40/140]    eta: 0:00:33  time: 0.3389  data_time: 0.3083  memory: 656  \n",
            "01/10 14:50:42 - mmengine - INFO - Epoch(test) [ 60/140]    eta: 0:00:23  time: 0.2189  data_time: 0.1826  memory: 656  \n",
            "01/10 14:50:48 - mmengine - INFO - Epoch(test) [ 80/140]    eta: 0:00:17  time: 0.2588  data_time: 0.2061  memory: 656  \n",
            "01/10 14:50:50 - mmengine - INFO - Epoch(test) [100/140]    eta: 0:00:10  time: 0.1274  data_time: 0.0729  memory: 656  \n",
            "01/10 14:50:52 - mmengine - INFO - Epoch(test) [120/140]    eta: 0:00:04  time: 0.0876  data_time: 0.0364  memory: 656  \n",
            "01/10 14:50:54 - mmengine - INFO - Epoch(test) [140/140]    eta: 0:00:00  time: 0.0860  data_time: 0.0316  memory: 657  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9868)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9377)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9032)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9820)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9893)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7427)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8850)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9402)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9317)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8756)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9492)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9611)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9035)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9286)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9342)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9466)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9265)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9418)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9475)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9684)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9435)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8993)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9554)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9618)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9210)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9567)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9567)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9506)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9749)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9724)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9155)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9923)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9672)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8600)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9611)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9345)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9649)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9712)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9109)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8254)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9275)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9157)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9515)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9526)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8019)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8826)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8494)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8034)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9845)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8729)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8905)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7154)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6465)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6725)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6481)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7117)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7998)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7744)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8373)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9618)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9602)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5547)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7770)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8617)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8299)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8523)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8624)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8391)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8755)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9784)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9471)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0096)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0051)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0067)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0137)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0060)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0044)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0035)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0010)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0035)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0034)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0037)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0036)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0154)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0121)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0046)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0044)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0069)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0085)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0054)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0055)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0075)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0046)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0074)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0092)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0036)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0032)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0062)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0107)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0063)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0026)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0069)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0153)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0063)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0078)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0062)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0059)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0031)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0030)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0091)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0036)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0037)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0101)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0023)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0012)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0042)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0043)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0048)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0075)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0042)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0098)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0200)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0162)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0165)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0158)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0073)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0048)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0058)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0074)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0096)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0067)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0070)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0208)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0082)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0110)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0042)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0041)}]\n",
            "01/10 14:50:54 - mmengine - INFO - Epoch(test) [140/140]    SelfCustomMetric/accuracy: 1.0000  SelfCustomMetric/f1_score: 1.0000  SelfCustomMetric/recall: 1.0000  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.        , 0.01408451, 1.        , 1.        ]), array([1.9923368e+00, 9.9233681e-01, 5.5470127e-01, 9.5033651e-04],\n",
            "      dtype=float32))  data_time: 0.1630  time: 0.2081\n",
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 14:50:54 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 662660116\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 662660116\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 14:50:54 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_frames'\n",
            "data_root_val = '../group_project_data/output_frames'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 14:50:55 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 14:50:55 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 14:51:26 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 14:51:26 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 14:51:26 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 14:51:26 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 14:52:15 - mmengine - INFO - Epoch(train)  [1][20/36]  lr: 1.5625e-04  eta: 0:14:01  time: 2.4745  data_time: 1.7993  memory: 3707  grad_norm: 6.9297  loss: 0.6780  top1_acc: 0.6875  top5_acc: 1.0000  loss_cls: 0.6780\n",
            "01/10 14:52:46 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_145054\n",
            "01/10 14:52:46 - mmengine - INFO - Epoch(train)  [1][36/36]  lr: 1.5625e-04  eta: 0:11:56  time: 2.0134  data_time: 1.3372  memory: 3707  grad_norm: 6.9441  loss: 0.6536  top1_acc: 0.6250  top5_acc: 1.0000  loss_cls: 0.6536\n",
            "01/10 14:53:30 - mmengine - INFO - Epoch(train)  [2][20/36]  lr: 1.5625e-04  eta: 0:11:13  time: 2.2274  data_time: 1.5504  memory: 3707  grad_norm: 6.3233  loss: 0.5577  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.5577\n",
            "01/10 14:54:04 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_145054\n",
            "01/10 14:54:04 - mmengine - INFO - Epoch(train)  [2][36/36]  lr: 1.5625e-04  eta: 0:10:32  time: 2.1535  data_time: 1.4727  memory: 3707  grad_norm: 6.0969  loss: 0.5258  top1_acc: 0.6250  top5_acc: 1.0000  loss_cls: 0.5258\n",
            "01/10 14:54:48 - mmengine - INFO - Epoch(train)  [3][20/36]  lr: 1.5625e-04  eta: 0:09:47  time: 2.1772  data_time: 1.4903  memory: 3707  grad_norm: 6.4619  loss: 0.4865  top1_acc: 0.7500  top5_acc: 1.0000  loss_cls: 0.4865\n",
            "01/10 14:55:22 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_145054\n",
            "01/10 14:55:22 - mmengine - INFO - Epoch(train)  [3][36/36]  lr: 1.5625e-04  eta: 0:09:11  time: 2.2392  data_time: 1.5582  memory: 3707  grad_norm: 5.9715  loss: 0.4317  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4317\n",
            "01/10 14:56:06 - mmengine - INFO - Epoch(train)  [4][20/36]  lr: 1.5625e-04  eta: 0:08:27  time: 2.1951  data_time: 1.5136  memory: 3707  grad_norm: 5.8876  loss: 0.3855  top1_acc: 0.8125  top5_acc: 1.0000  loss_cls: 0.3855\n",
            "01/10 14:56:44 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_145054\n",
            "01/10 14:56:44 - mmengine - INFO - Epoch(train)  [4][36/36]  lr: 1.5625e-04  eta: 0:07:56  time: 2.1387  data_time: 1.4623  memory: 3707  grad_norm: 5.8976  loss: 0.3600  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3600\n",
            "01/10 14:57:28 - mmengine - INFO - Epoch(train)  [5][20/36]  lr: 1.5625e-04  eta: 0:07:13  time: 2.2296  data_time: 1.5537  memory: 3707  grad_norm: 5.7803  loss: 0.3253  top1_acc: 0.5000  top5_acc: 1.0000  loss_cls: 0.3253\n",
            "01/10 14:58:02 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_145054\n",
            "01/10 14:58:02 - mmengine - INFO - Epoch(train)  [5][36/36]  lr: 1.5625e-04  eta: 0:06:35  time: 2.0927  data_time: 1.4184  memory: 3707  grad_norm: 6.2355  loss: 0.3319  top1_acc: 0.8125  top5_acc: 1.0000  loss_cls: 0.3319\n",
            "01/10 14:58:02 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 14:58:04 - mmengine - INFO - Epoch(val) [5][2/2]    acc/top1: 0.9444  acc/top5: 1.0000  acc/mean1: 0.4722  data_time: 0.6201  time: 0.7183\n",
            "01/10 14:58:05 - mmengine - INFO - The best checkpoint with 0.9444 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 14:58:52 - mmengine - INFO - Epoch(train)  [6][20/36]  lr: 1.5625e-04  eta: 0:05:52  time: 2.2228  data_time: 1.5462  memory: 3707  grad_norm: 5.8619  loss: 0.2827  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2827\n",
            "01/10 14:59:26 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_145054\n",
            "01/10 14:59:26 - mmengine - INFO - Epoch(train)  [6][36/36]  lr: 1.5625e-04  eta: 0:05:16  time: 2.2162  data_time: 1.5421  memory: 3707  grad_norm: 5.4138  loss: 0.2579  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2579\n",
            "01/10 15:00:12 - mmengine - INFO - Epoch(train)  [7][20/36]  lr: 1.5625e-04  eta: 0:04:33  time: 2.2820  data_time: 1.6106  memory: 3707  grad_norm: 5.5139  loss: 0.2540  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2540\n",
            "01/10 15:00:44 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_145054\n",
            "01/10 15:00:45 - mmengine - INFO - Epoch(train)  [7][36/36]  lr: 1.5625e-04  eta: 0:03:56  time: 2.2100  data_time: 1.5282  memory: 3707  grad_norm: 5.1795  loss: 0.2341  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2341\n",
            "01/10 15:01:31 - mmengine - INFO - Epoch(train)  [8][20/36]  lr: 1.5625e-04  eta: 0:03:13  time: 2.3308  data_time: 1.6497  memory: 3707  grad_norm: 4.9906  loss: 0.1982  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.1982\n",
            "01/10 15:02:05 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_145054\n",
            "01/10 15:02:05 - mmengine - INFO - Epoch(train)  [8][36/36]  lr: 1.5625e-04  eta: 0:02:38  time: 2.2720  data_time: 1.6038  memory: 3707  grad_norm: 5.2681  loss: 0.2060  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2060\n",
            "01/10 15:02:50 - mmengine - INFO - Epoch(train)  [9][20/36]  lr: 1.5625e-04  eta: 0:01:54  time: 2.2398  data_time: 1.5587  memory: 3707  grad_norm: 5.0552  loss: 0.1826  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.1826\n",
            "01/10 15:03:24 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_145054\n",
            "01/10 15:03:24 - mmengine - INFO - Epoch(train)  [9][36/36]  lr: 1.5625e-04  eta: 0:01:19  time: 2.1510  data_time: 1.4691  memory: 3707  grad_norm: 4.7577  loss: 0.1660  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.1660\n",
            "01/10 15:04:08 - mmengine - INFO - Epoch(train) [10][20/36]  lr: 1.5625e-04  eta: 0:00:35  time: 2.2462  data_time: 1.5635  memory: 3707  grad_norm: 4.6164  loss: 0.1551  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.1551\n",
            "01/10 15:04:42 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_145054\n",
            "01/10 15:04:42 - mmengine - INFO - Epoch(train) [10][36/36]  lr: 1.5625e-04  eta: 0:00:00  time: 2.0484  data_time: 1.3696  memory: 3707  grad_norm: 4.9087  loss: 0.1659  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.1659\n",
            "01/10 15:04:42 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 15:04:45 - mmengine - INFO - Epoch(val) [10][2/2]    acc/top1: 1.0000  acc/top5: 1.0000  acc/mean1: 1.0000  data_time: 0.4070  time: 0.4804\n",
            "01/10 15:04:45 - mmengine - INFO - The previous best checkpoint /content/mmaction2/tutorial_exps/best_acc_top1_epoch_5.pth is removed\n",
            "01/10 15:04:46 - mmengine - INFO - The best checkpoint with 1.0000 acc/top1 at 10 epoch is saved to best_acc_top1_epoch_10.pth.\n",
            "01/10 15:04:51 - mmengine - INFO - Epoch(test) [ 20/151]    eta: 0:00:18  time: 0.1448  data_time: 0.0983  memory: 818  \n",
            "01/10 15:04:53 - mmengine - INFO - Epoch(test) [ 40/151]    eta: 0:00:12  time: 0.0862  data_time: 0.0338  memory: 652  \n",
            "01/10 15:04:55 - mmengine - INFO - Epoch(test) [ 60/151]    eta: 0:00:10  time: 0.1021  data_time: 0.0502  memory: 652  \n",
            "01/10 15:04:58 - mmengine - INFO - Epoch(test) [ 80/151]    eta: 0:00:08  time: 0.1361  data_time: 0.0835  memory: 652  \n",
            "01/10 15:05:00 - mmengine - INFO - Epoch(test) [100/151]    eta: 0:00:05  time: 0.1145  data_time: 0.0648  memory: 652  \n",
            "01/10 15:05:02 - mmengine - INFO - Epoch(test) [120/151]    eta: 0:00:03  time: 0.0849  data_time: 0.0372  memory: 652  \n",
            "01/10 15:05:04 - mmengine - INFO - Epoch(test) [140/151]    eta: 0:00:01  time: 0.0919  data_time: 0.0468  memory: 653  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9933)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9855)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9913)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9490)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9797)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9510)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0082)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0096)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0092)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0114)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0072)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0078)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0081)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0062)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0124)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0101)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0110)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0103)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0144)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0126)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0089)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0074)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0143)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0165)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0059)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0106)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0124)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0141)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0117)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0141)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0150)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0159)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0130)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0126)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0157)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0154)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0164)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0156)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0112)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0092)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0094)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0100)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0102)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0061)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0183)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0100)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0039)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0146)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0079)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0101)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0159)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0172)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0103)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0184)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0232)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0151)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0198)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0217)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0159)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0110)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0137)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0155)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0161)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0157)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0098)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0244)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0135)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0155)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0093)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0092)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6400)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6574)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6270)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6219)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5195)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6086)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6347)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0082)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0096)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0092)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0114)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0072)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0078)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0081)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0062)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0124)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0101)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0110)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0103)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0144)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0126)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0089)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0074)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0143)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0165)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0059)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0106)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0124)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0141)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0117)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0141)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0150)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0159)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0130)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0126)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0157)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0154)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0164)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0156)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0112)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0092)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0094)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0100)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0102)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0061)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0183)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0100)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0039)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0146)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0095)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0079)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0097)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0101)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0159)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0172)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0103)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0184)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0232)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0151)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0198)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0217)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0159)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0110)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0137)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0155)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0161)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0157)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0098)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0244)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0135)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0155)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0093)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0092)}]\n",
            "01/10 15:05:05 - mmengine - INFO - Epoch(test) [151/151]    SelfCustomMetric/accuracy: 1.0000  SelfCustomMetric/f1_score: 1.0000  SelfCustomMetric/recall: 1.0000  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.        , 0.07692308, 1.        , 1.        ]), array([1.9932655 , 0.9932655 , 0.5194988 , 0.00389518], dtype=float32))  data_time: 0.0574  time: 0.1071\n",
            "{'1': {'frame_folders': ['1_1_0', '1_1_1', '1_1_2', '1_1_3', '1_2_0', '1_2_1', '1_3_0', '1_3_1', '1_3_2', '1_4_0', '1_4_1', '1_4_2', '1_4_3', '1_5_0', '1_5_1', '1_5_2', '1_5_3', '1_5_4', '1_6_0', '1_7_0', '1_7_1', '1_7_2'], 'video_type': 1}, '10': {'frame_folders': ['10_34_0', '10_34_1', '10_34_2', '10_34_3', '10_34_4', '10_35_0', '10_35_1', '10_35_2', '10_36_0', '10_36_1', '10_37_0', '10_37_1', '10_38_0', '10_38_1', '10_38_2', '10_39_0', '10_39_1', '10_39_2', '10_39_3', '10_39_4', '10_40_0', '10_40_1', '10_41_0', '10_41_1'], 'video_type': 1}, '11': {'frame_folders': ['11_42_0'], 'video_type': 1}, '12': {'frame_folders': ['12_43_0', '12_44_0', '12_44_1', '12_44_2', '12_44_3', '12_45_0', '12_45_1', '12_45_2', '12_45_3'], 'video_type': 0}, '13': {'frame_folders': ['13_46_0', '13_49_0', '13_49_1', '13_49_2', '13_49_3', '13_49_4', '13_50_0'], 'video_type': 1}, '14': {'frame_folders': ['14_51_0', '14_52_0', '14_52_1', '14_52_2', '14_52_3', '14_53_0', '14_53_1', '14_53_2', '14_53_3', '14_53_4', '14_53_5', '14_54_0', '14_54_1', '14_54_2', '14_54_3', '14_54_4', '14_55_0', '14_55_1', '14_55_2', '14_55_3', '14_55_4', '14_55_5', '14_55_6', '14_56_0', '14_56_1', '14_56_2', '14_56_3', '14_56_4', '14_56_5', '14_57_0', '14_57_1', '14_57_2', '14_57_3', '14_57_4', '14_57_5', '14_57_6', '14_57_7', '14_57_8', '14_57_9', '14_58_0', '14_58_1', '14_58_2', '14_58_3', '14_59_0', '14_59_1', '14_60_0', '14_60_1', '14_61_0', '14_62_0', '14_62_1', '14_63_0', '14_63_1', '14_64_0', '14_64_1', '14_64_2', '14_64_3', '14_64_4', '14_65_0', '14_65_1', '14_65_2', '14_65_3', '14_65_4', '14_66_0', '14_67_0', '14_67_1', '14_68_0', '14_69_0', '14_70_0', '14_70_1'], 'video_type': 0}, '15': {'frame_folders': ['15_72_0', '15_72_1', '15_73_0', '15_73_1', '15_73_2', '15_74_0', '15_74_1', '15_74_2', '15_74_3', '15_74_4', '15_74_5', '15_75_0', '15_75_1', '15_77_0', '15_78_0', '15_78_1', '15_79_0', '15_80_0', '15_80_1', '15_80_2', '15_81_0', '15_81_1', '15_81_2', '15_83_0', '15_83_1', '15_83_2', '15_83_3', '15_84_0', '15_84_1', '15_84_2', '15_84_3', '15_85_0'], 'video_type': 1}, '17': {'frame_folders': ['17_86_0', '17_87_0', '17_87_1', '17_87_2', '17_87_3', '17_87_4', '17_87_5', '17_87_6'], 'video_type': 0}, '18': {'frame_folders': ['18_88_0', '18_88_1', '18_88_2', '18_88_3', '18_88_4', '18_89_0', '18_89_1', '18_89_2', '18_89_3'], 'video_type': 1}, '19': {'frame_folders': ['19_90_0', '19_90_1', '19_90_2', '19_90_3', '19_90_4', '19_90_5', '19_90_6', '19_90_7', '19_90_8', '19_91_0', '19_91_1', '19_91_2', '19_91_3', '19_91_4', '19_91_5'], 'video_type': 1}, '20': {'frame_folders': ['20_92_0', '20_92_1', '20_92_2', '20_92_3', '20_93_0', '20_93_1', '20_94_0'], 'video_type': 1}, '22': {'frame_folders': ['22_96_0', '22_96_1', '22_97_0', '22_97_1', '22_97_2'], 'video_type': 1}, '23': {'frame_folders': ['23_98_0', '23_98_1', '23_99_0', '23_99_1', '23_100_0', '23_100_1'], 'video_type': 1}, '3': {'frame_folders': ['3_8_0', '3_8_1', '3_8_2', '3_9_0', '3_9_1', '3_9_2', '3_9_3', '3_10_0', '3_10_1', '3_11_0', '3_11_1', '3_12_0', '3_12_1', '3_12_2', '3_12_3', '3_14_0', '3_14_1', '3_14_2', '3_14_3', '3_14_4', '3_14_5', '3_14_6', '3_14_7', '3_14_8', '3_14_9'], 'video_type': 1}, '4': {'frame_folders': ['4_16_0', '4_16_1', '4_16_2', '4_16_3', '4_16_4', '4_16_5'], 'video_type': 1}, '5': {'frame_folders': ['5_17_0', '5_17_1', '5_17_2', '5_17_3', '5_17_4', '5_17_5', '5_17_6', '5_17_7', '5_17_8'], 'video_type': 1}, '6': {'frame_folders': ['6_18_0', '6_18_1', '6_20_0', '6_21_0', '6_21_1', '6_24_0', '6_25_0'], 'video_type': 1}, '7': {'frame_folders': ['7_27_0', '7_27_1', '7_27_2', '7_28_0', '7_29_0', '7_29_1'], 'video_type': 1}, '8': {'frame_folders': ['8_30_0', '8_30_1', '8_30_2', '8_30_3', '8_30_4', '8_31_0', '8_31_1', '8_31_2', '8_31_3', '8_31_4'], 'video_type': 1}, '9': {'frame_folders': ['9_32_0', '9_32_1', '9_32_2', '9_32_3', '9_33_0', '9_33_1', '9_33_2', '9_33_3', '9_33_4', '9_33_5', '9_33_6'], 'video_type': 1}, 'ArmFlapping_10': {'frame_folders': ['ArmFlapping_10_135_0', 'ArmFlapping_10_135_1', 'ArmFlapping_10_135_2', 'ArmFlapping_10_135_3', 'ArmFlapping_10_135_4', 'ArmFlapping_10_135_5', 'ArmFlapping_10_136_0', 'ArmFlapping_10_137_0', 'ArmFlapping_10_137_1', 'ArmFlapping_10_137_2', 'ArmFlapping_10_137_3', 'ArmFlapping_10_137_4', 'ArmFlapping_10_137_5', 'ArmFlapping_10_137_6', 'ArmFlapping_10_137_7', 'ArmFlapping_10_137_8', 'ArmFlapping_10_137_9', 'ArmFlapping_10_137_10', 'ArmFlapping_10_137_11', 'ArmFlapping_10_137_12', 'ArmFlapping_10_138_0', 'ArmFlapping_10_138_1', 'ArmFlapping_10_138_2', 'ArmFlapping_10_139_0', 'ArmFlapping_10_139_1', 'ArmFlapping_10_139_2', 'ArmFlapping_10_140_0', 'ArmFlapping_10_140_1', 'ArmFlapping_10_140_2', 'ArmFlapping_10_140_3', 'ArmFlapping_10_140_4', 'ArmFlapping_10_140_5', 'ArmFlapping_10_140_6', 'ArmFlapping_10_141_0', 'ArmFlapping_10_141_1', 'ArmFlapping_10_142_0', 'ArmFlapping_10_142_1', 'ArmFlapping_10_143_0', 'ArmFlapping_10_143_1', 'ArmFlapping_10_144_0', 'ArmFlapping_10_144_1', 'ArmFlapping_10_144_2', 'ArmFlapping_10_145_0', 'ArmFlapping_10_145_1', 'ArmFlapping_10_145_2', 'ArmFlapping_10_145_3', 'ArmFlapping_10_145_4', 'ArmFlapping_10_146_0', 'ArmFlapping_10_146_1', 'ArmFlapping_10_146_2', 'ArmFlapping_10_146_3', 'ArmFlapping_10_146_4', 'ArmFlapping_10_146_5', 'ArmFlapping_10_146_6', 'ArmFlapping_10_146_7', 'ArmFlapping_10_146_8', 'ArmFlapping_10_146_9', 'ArmFlapping_10_146_10', 'ArmFlapping_10_147_0', 'ArmFlapping_10_147_1', 'ArmFlapping_10_147_2', 'ArmFlapping_10_147_3', 'ArmFlapping_10_147_4', 'ArmFlapping_10_147_5', 'ArmFlapping_10_147_6', 'ArmFlapping_10_147_7', 'ArmFlapping_10_147_8', 'ArmFlapping_10_147_9', 'ArmFlapping_10_147_10', 'ArmFlapping_10_147_11', 'ArmFlapping_10_147_12', 'ArmFlapping_10_147_13', 'ArmFlapping_10_147_14', 'ArmFlapping_10_147_15', 'ArmFlapping_10_147_16', 'ArmFlapping_10_147_17', 'ArmFlapping_10_147_18', 'ArmFlapping_10_147_19', 'ArmFlapping_10_147_20', 'ArmFlapping_10_147_21', 'ArmFlapping_10_147_22', 'ArmFlapping_10_147_23', 'ArmFlapping_10_147_24', 'ArmFlapping_10_147_25', 'ArmFlapping_10_147_26', 'ArmFlapping_10_147_27', 'ArmFlapping_10_147_28', 'ArmFlapping_10_147_29', 'ArmFlapping_10_147_30', 'ArmFlapping_10_147_31', 'ArmFlapping_10_147_32', 'ArmFlapping_10_147_33', 'ArmFlapping_10_147_34', 'ArmFlapping_10_147_35', 'ArmFlapping_10_147_36', 'ArmFlapping_10_148_0', 'ArmFlapping_10_149_0', 'ArmFlapping_10_149_1'], 'video_type': 0}, 'ArmFlapping_12': {'frame_folders': ['ArmFlapping_12_151_0', 'ArmFlapping_12_151_1', 'ArmFlapping_12_151_2', 'ArmFlapping_12_151_3', 'ArmFlapping_12_151_4', 'ArmFlapping_12_152_0', 'ArmFlapping_12_153_0'], 'video_type': 1}, 'ArmFlapping_13': {'frame_folders': ['ArmFlapping_13_154_0', 'ArmFlapping_13_154_1', 'ArmFlapping_13_154_2'], 'video_type': 1}, 'ArmFlapping_14': {'frame_folders': ['ArmFlapping_14_155_0', 'ArmFlapping_14_155_1', 'ArmFlapping_14_157_0', 'ArmFlapping_14_158_0', 'ArmFlapping_14_159_0', 'ArmFlapping_14_159_1', 'ArmFlapping_14_159_2', 'ArmFlapping_14_160_0', 'ArmFlapping_14_160_1', 'ArmFlapping_14_161_0', 'ArmFlapping_14_162_0', 'ArmFlapping_14_164_0', 'ArmFlapping_14_165_0', 'ArmFlapping_14_166_0', 'ArmFlapping_14_166_1', 'ArmFlapping_14_167_0', 'ArmFlapping_14_168_0', 'ArmFlapping_14_168_1', 'ArmFlapping_14_168_2', 'ArmFlapping_14_170_0', 'ArmFlapping_14_171_0', 'ArmFlapping_14_171_1'], 'video_type': 1}, 'ArmFlapping_16': {'frame_folders': ['ArmFlapping_16_172_0', 'ArmFlapping_16_172_1', 'ArmFlapping_16_172_2', 'ArmFlapping_16_173_0', 'ArmFlapping_16_173_1', 'ArmFlapping_16_173_2', 'ArmFlapping_16_173_3', 'ArmFlapping_16_173_4', 'ArmFlapping_16_173_5', 'ArmFlapping_16_174_0', 'ArmFlapping_16_174_1', 'ArmFlapping_16_174_2', 'ArmFlapping_16_174_3'], 'video_type': 1}, 'ArmFlapping_3': {'frame_folders': ['ArmFlapping_3_101_0', 'ArmFlapping_3_101_1', 'ArmFlapping_3_101_2', 'ArmFlapping_3_101_3', 'ArmFlapping_3_101_4', 'ArmFlapping_3_101_5', 'ArmFlapping_3_101_6', 'ArmFlapping_3_101_7', 'ArmFlapping_3_101_8', 'ArmFlapping_3_101_9', 'ArmFlapping_3_101_10', 'ArmFlapping_3_101_11', 'ArmFlapping_3_101_12', 'ArmFlapping_3_101_13', 'ArmFlapping_3_101_14', 'ArmFlapping_3_101_15', 'ArmFlapping_3_101_16', 'ArmFlapping_3_101_17', 'ArmFlapping_3_102_0', 'ArmFlapping_3_102_1', 'ArmFlapping_3_102_2', 'ArmFlapping_3_102_3', 'ArmFlapping_3_102_4', 'ArmFlapping_3_102_5', 'ArmFlapping_3_102_6', 'ArmFlapping_3_102_7', 'ArmFlapping_3_102_8', 'ArmFlapping_3_102_9', 'ArmFlapping_3_102_10', 'ArmFlapping_3_102_11', 'ArmFlapping_3_102_12', 'ArmFlapping_3_102_13'], 'video_type': 1}, 'ArmFlapping_6': {'frame_folders': ['ArmFlapping_6_105_0', 'ArmFlapping_6_105_1', 'ArmFlapping_6_106_0', 'ArmFlapping_6_107_0', 'ArmFlapping_6_107_1', 'ArmFlapping_6_108_0', 'ArmFlapping_6_108_1', 'ArmFlapping_6_108_2', 'ArmFlapping_6_109_0', 'ArmFlapping_6_109_1', 'ArmFlapping_6_110_0', 'ArmFlapping_6_110_1', 'ArmFlapping_6_110_2', 'ArmFlapping_6_110_3', 'ArmFlapping_6_110_4', 'ArmFlapping_6_111_0', 'ArmFlapping_6_111_1', 'ArmFlapping_6_111_2', 'ArmFlapping_6_112_0', 'ArmFlapping_6_112_1', 'ArmFlapping_6_113_0', 'ArmFlapping_6_113_1', 'ArmFlapping_6_113_2', 'ArmFlapping_6_113_3', 'ArmFlapping_6_114_0', 'ArmFlapping_6_114_1', 'ArmFlapping_6_114_2', 'ArmFlapping_6_115_0', 'ArmFlapping_6_116_0', 'ArmFlapping_6_116_1', 'ArmFlapping_6_117_0', 'ArmFlapping_6_118_0', 'ArmFlapping_6_118_1', 'ArmFlapping_6_119_0', 'ArmFlapping_6_119_1', 'ArmFlapping_6_120_0', 'ArmFlapping_6_120_1', 'ArmFlapping_6_121_0', 'ArmFlapping_6_121_1', 'ArmFlapping_6_121_2', 'ArmFlapping_6_121_3', 'ArmFlapping_6_122_0', 'ArmFlapping_6_123_0', 'ArmFlapping_6_124_0', 'ArmFlapping_6_125_0', 'ArmFlapping_6_125_1', 'ArmFlapping_6_125_2', 'ArmFlapping_6_125_3', 'ArmFlapping_6_126_0', 'ArmFlapping_6_127_0', 'ArmFlapping_6_127_1'], 'video_type': 1}, 'ArmFlapping_8': {'frame_folders': ['ArmFlapping_8_129_0', 'ArmFlapping_8_130_0', 'ArmFlapping_8_130_1', 'ArmFlapping_8_130_2', 'ArmFlapping_8_130_3', 'ArmFlapping_8_131_0', 'ArmFlapping_8_131_1', 'ArmFlapping_8_131_2', 'ArmFlapping_8_131_3', 'ArmFlapping_8_132_0', 'ArmFlapping_8_132_1', 'ArmFlapping_8_132_2', 'ArmFlapping_8_132_3'], 'video_type': 1}, 'ArmFlapping_9': {'frame_folders': ['ArmFlapping_9_133_0', 'ArmFlapping_9_133_1', 'ArmFlapping_9_133_2', 'ArmFlapping_9_133_3', 'ArmFlapping_9_133_4', 'ArmFlapping_9_133_5'], 'video_type': 1}}\n",
            "01/10 15:05:05 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 1836014926\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 1836014926\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 15:05:05 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_frames'\n",
            "data_root_val = '../group_project_data/output_frames'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_frames'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 15:05:06 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 15:05:06 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n",
            "01/10 15:05:37 - mmengine - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n",
            "Loads checkpoint by local backend from path: ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
            "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
            "01/10 15:05:37 - mmengine - INFO - Load checkpoint from ./checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
            "01/10 15:05:37 - mmengine - WARNING - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/10 15:05:37 - mmengine - INFO - Checkpoints will be saved to /content/mmaction2/tutorial_exps.\n",
            "01/10 15:06:30 - mmengine - INFO - Epoch(train)  [1][20/28]  lr: 1.5625e-04  eta: 0:11:24  time: 2.6332  data_time: 1.9523  memory: 3713  grad_norm: 7.0838  loss: 0.6780  top1_acc: 0.5625  top5_acc: 1.0000  loss_cls: 0.6780\n",
            "01/10 15:06:47 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_150505\n",
            "01/10 15:06:47 - mmengine - INFO - Epoch(train)  [1][28/28]  lr: 1.5625e-04  eta: 0:10:32  time: 2.4468  data_time: 1.7921  memory: 3713  grad_norm: 7.2298  loss: 0.6661  top1_acc: 0.0000  top5_acc: 1.0000  loss_cls: 0.6661\n",
            "01/10 15:07:39 - mmengine - INFO - Epoch(train)  [2][20/28]  lr: 1.5625e-04  eta: 0:09:52  time: 2.6110  data_time: 1.9291  memory: 3713  grad_norm: 6.0480  loss: 0.5773  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.5773\n",
            "01/10 15:07:55 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_150505\n",
            "01/10 15:07:55 - mmengine - INFO - Epoch(train)  [2][28/28]  lr: 1.5625e-04  eta: 0:09:13  time: 2.2739  data_time: 1.6239  memory: 3713  grad_norm: 6.2015  loss: 0.5420  top1_acc: 0.5000  top5_acc: 1.0000  loss_cls: 0.5420\n",
            "01/10 15:08:49 - mmengine - INFO - Epoch(train)  [3][20/28]  lr: 1.5625e-04  eta: 0:08:35  time: 2.6912  data_time: 2.0000  memory: 3713  grad_norm: 5.7717  loss: 0.4700  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4700\n",
            "01/10 15:09:03 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_150505\n",
            "01/10 15:09:03 - mmengine - INFO - Epoch(train)  [3][28/28]  lr: 1.5625e-04  eta: 0:08:01  time: 2.3542  data_time: 1.6994  memory: 3713  grad_norm: 6.2993  loss: 0.4632  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.4632\n",
            "01/10 15:09:54 - mmengine - INFO - Epoch(train)  [4][20/28]  lr: 1.5625e-04  eta: 0:07:15  time: 2.5356  data_time: 1.8639  memory: 3713  grad_norm: 5.5656  loss: 0.4163  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.4163\n",
            "01/10 15:10:11 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_150505\n",
            "01/10 15:10:11 - mmengine - INFO - Epoch(train)  [4][28/28]  lr: 1.5625e-04  eta: 0:06:50  time: 2.3270  data_time: 1.6785  memory: 3713  grad_norm: 6.4018  loss: 0.4304  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.4304\n",
            "01/10 15:11:01 - mmengine - INFO - Epoch(train)  [5][20/28]  lr: 1.5625e-04  eta: 0:06:03  time: 2.5336  data_time: 1.8543  memory: 3713  grad_norm: 5.3061  loss: 0.3563  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.3563\n",
            "01/10 15:11:18 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_150505\n",
            "01/10 15:11:18 - mmengine - INFO - Epoch(train)  [5][28/28]  lr: 1.5625e-04  eta: 0:05:41  time: 2.3117  data_time: 1.6584  memory: 3713  grad_norm: 5.4374  loss: 0.3397  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3397\n",
            "01/10 15:11:18 - mmengine - INFO - Saving checkpoint at 5 epochs\n",
            "01/10 15:11:27 - mmengine - INFO - Epoch(val) [5][7/7]    acc/top1: 0.9907  acc/top5: 1.0000  acc/mean1: 0.9872  data_time: 0.8872  time: 1.0289\n",
            "01/10 15:11:27 - mmengine - INFO - The best checkpoint with 0.9907 acc/top1 at 5 epoch is saved to best_acc_top1_epoch_5.pth.\n",
            "01/10 15:12:22 - mmengine - INFO - Epoch(train)  [6][20/28]  lr: 1.5625e-04  eta: 0:04:56  time: 2.6947  data_time: 2.0049  memory: 3713  grad_norm: 5.2220  loss: 0.3152  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3152\n",
            "01/10 15:12:39 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_150505\n",
            "01/10 15:12:39 - mmengine - INFO - Epoch(train)  [6][28/28]  lr: 1.5625e-04  eta: 0:04:34  time: 2.3105  data_time: 1.6538  memory: 3713  grad_norm: 5.7213  loss: 0.3164  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.3164\n",
            "01/10 15:13:28 - mmengine - INFO - Epoch(train)  [7][20/28]  lr: 1.5625e-04  eta: 0:03:45  time: 2.4491  data_time: 1.7666  memory: 3713  grad_norm: 5.1046  loss: 0.2704  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2704\n",
            "01/10 15:13:46 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_150505\n",
            "01/10 15:13:46 - mmengine - INFO - Epoch(train)  [7][28/28]  lr: 1.5625e-04  eta: 0:03:25  time: 2.3653  data_time: 1.7102  memory: 3713  grad_norm: 5.8511  loss: 0.2924  top1_acc: 0.5000  top5_acc: 1.0000  loss_cls: 0.2924\n",
            "01/10 15:14:39 - mmengine - INFO - Epoch(train)  [8][20/28]  lr: 1.5625e-04  eta: 0:02:37  time: 2.6230  data_time: 1.9440  memory: 3713  grad_norm: 5.6862  loss: 0.2662  top1_acc: 0.8750  top5_acc: 1.0000  loss_cls: 0.2662\n",
            "01/10 15:14:55 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_150505\n",
            "01/10 15:14:55 - mmengine - INFO - Epoch(train)  [8][28/28]  lr: 1.5625e-04  eta: 0:02:17  time: 2.4600  data_time: 1.8063  memory: 3713  grad_norm: 5.0877  loss: 0.2348  top1_acc: 0.5000  top5_acc: 1.0000  loss_cls: 0.2348\n",
            "01/10 15:15:45 - mmengine - INFO - Epoch(train)  [9][20/28]  lr: 1.5625e-04  eta: 0:01:28  time: 2.5049  data_time: 1.8174  memory: 3713  grad_norm: 4.9943  loss: 0.2173  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2173\n",
            "01/10 15:16:02 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_150505\n",
            "01/10 15:16:02 - mmengine - INFO - Epoch(train)  [9][28/28]  lr: 1.5625e-04  eta: 0:01:08  time: 2.3324  data_time: 1.6782  memory: 3713  grad_norm: 4.7427  loss: 0.2011  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2011\n",
            "01/10 15:16:55 - mmengine - INFO - Epoch(train) [10][20/28]  lr: 1.5625e-04  eta: 0:00:19  time: 2.6308  data_time: 1.9448  memory: 3713  grad_norm: 5.7179  loss: 0.2266  top1_acc: 0.9375  top5_acc: 1.0000  loss_cls: 0.2266\n",
            "01/10 15:17:13 - mmengine - INFO - Exp name: tsn_imagenet-pretrained-r50_8xb32_5x1x3-110e_kinetics400-flow_20240110_150505\n",
            "01/10 15:17:13 - mmengine - INFO - Epoch(train) [10][28/28]  lr: 1.5625e-04  eta: 0:00:00  time: 2.3781  data_time: 1.7224  memory: 3713  grad_norm: 6.5965  loss: 0.2527  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.2527\n",
            "01/10 15:17:13 - mmengine - INFO - Saving checkpoint at 10 epochs\n",
            "01/10 15:17:21 - mmengine - INFO - Epoch(val) [10][7/7]    acc/top1: 0.9352  acc/top5: 1.0000  acc/mean1: 0.9103  data_time: 0.8470  time: 0.9740\n",
            "01/10 15:17:23 - mmengine - INFO - Epoch(test) [ 20/132]    eta: 0:00:09  time: 0.0875  data_time: 0.0189  memory: 822  \n",
            "01/10 15:17:25 - mmengine - INFO - Epoch(test) [ 40/132]    eta: 0:00:07  time: 0.0828  data_time: 0.0168  memory: 656  \n",
            "01/10 15:17:28 - mmengine - INFO - Epoch(test) [ 60/132]    eta: 0:00:07  time: 0.1364  data_time: 0.0738  memory: 656  \n",
            "01/10 15:17:30 - mmengine - INFO - Epoch(test) [ 80/132]    eta: 0:00:05  time: 0.1004  data_time: 0.0468  memory: 656  \n",
            "01/10 15:17:31 - mmengine - INFO - Epoch(test) [100/132]    eta: 0:00:03  time: 0.0869  data_time: 0.0470  memory: 656  \n",
            "01/10 15:17:33 - mmengine - INFO - Epoch(test) [120/132]    eta: 0:00:01  time: 0.0905  data_time: 0.0412  memory: 656  \n",
            "[{'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5104)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5335)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5089)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7375)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7306)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7890)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7018)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6797)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5275)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5648)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5691)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5178)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5290)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5521)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5806)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4743)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.5743)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4101)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4302)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4385)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4429)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4167)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4239)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4130)}, {'pred': tensor(0), 'gt': tensor(1), 'pd_score': tensor(0.4159)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7721)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7439)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8713)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7449)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8356)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8012)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7990)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7928)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.6952)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8980)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8714)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7806)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7904)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8630)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9715)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9476)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9334)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7960)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8259)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8516)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8325)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8824)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8227)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8438)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8265)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8551)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8533)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9596)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8472)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7466)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8034)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.8497)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9702)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9226)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9657)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9174)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.9217)}, {'pred': tensor(1), 'gt': tensor(1), 'pd_score': tensor(0.7780)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0523)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0375)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0382)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0400)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0498)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0476)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0528)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0411)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0212)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0472)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0390)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0272)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0356)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0540)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0397)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0332)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0266)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0335)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0389)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0293)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0214)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0253)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0330)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0338)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0496)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0453)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0352)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0460)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0579)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0495)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0419)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0519)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0397)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0446)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0384)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0427)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0364)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0385)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0304)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0388)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0375)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0505)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0395)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0212)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0471)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0274)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0427)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0329)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0292)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0360)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0380)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0526)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0565)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0749)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0457)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0535)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0621)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0524)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0467)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0453)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0457)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0391)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0679)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0313)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0611)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0503)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0510)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0304)}, {'pred': tensor(0), 'gt': tensor(0), 'pd_score': tensor(0.0490)}]\n",
            "01/10 15:17:34 - mmengine - INFO - Epoch(test) [132/132]    SelfCustomMetric/accuracy: 0.9318  SelfCustomMetric/f1_score: 0.9231  SelfCustomMetric/recall: 0.8571  SelfCustomMetric/auc: 1.0000  SelfCustomMetric/roc_curve: (array([0., 0., 0., 1.]), array([0.        , 0.01587302, 1.        , 1.        ]), array([1.9714692 , 0.97146916, 0.41007832, 0.02119327], dtype=float32))  data_time: 0.0401  time: 0.0962\n"
          ]
        }
      ],
      "source": [
        "import os.path as osp\n",
        "import mmengine\n",
        "from mmengine.runner import Runner\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "result_list = []\n",
        "# split the datasets\n",
        "for i in range(len(random_seeds)):\n",
        "  set_seeds(random_seeds[i])\n",
        "  yaml_path = '../group_project_data/sample.yaml'\n",
        "  annotation_path = './Annotations'\n",
        "  spilt_dataset_and_create_annotations(yaml_path, annotation_path, do_oversampling=False)\n",
        "\n",
        "  # Create work_dir\n",
        "  mmengine.mkdir_or_exist(osp.abspath(cfg_raw.work_dir))\n",
        "\n",
        "  # build the runner\n",
        "  runner = Runner.from_cfg(cfg_raw)\n",
        "\n",
        "  # start training\n",
        "  runner.train()\n",
        "\n",
        "  result_list.append(runner.test())\n",
        "\n",
        "result_dict['Baseline'] = result_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT6iwSC_2d9E"
      },
      "source": [
        "# Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbjgMW01yXBe"
      },
      "source": [
        "## Model Complexity\n",
        "- FLOPs\n",
        "- Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iYhQtb1SED_M",
        "outputId": "3f393efe-da4a-44ac-b422-110dd7412f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/10 15:39:30 - mmengine - INFO - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 1135159399\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 12.2, V12.2.140\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "    PyTorch: 2.1.0+cu121\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.16.0+cu121\n",
            "    OpenCV: 4.8.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 1135159399\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/10 15:39:31 - mmengine - INFO - Config:\n",
            "ann_file_test = './Annotations/test.txt'\n",
            "ann_file_train = './Annotations/train.txt'\n",
            "ann_file_val = './Annotations/val.txt'\n",
            "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
            "clip_len = 30\n",
            "data_root = '../group_project_data/output_yolo3'\n",
            "data_root_val = '../group_project_data/output_yolo3'\n",
            "dataset_type = 'RawframeDataset'\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=5, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
            "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    runtime_info=dict(type='RuntimeInfoHook'),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    sync_buffers=dict(type='SyncBuffersHook'),\n",
            "    timer=dict(type='IterTimerHook'))\n",
            "default_scope = 'mmaction'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "file_client_args = dict(io_backend='disk')\n",
            "load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        depth=50,\n",
            "        in_channels=3,\n",
            "        norm_eval=False,\n",
            "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
            "        type='ResNet'),\n",
            "    cls_head=dict(\n",
            "        average_clips='prob',\n",
            "        consensus=dict(dim=1, type='AvgConsensus'),\n",
            "        dropout_ratio=0.4,\n",
            "        in_channels=2048,\n",
            "        init_std=0.01,\n",
            "        num_classes=2,\n",
            "        spatial_type='avg',\n",
            "        type='TSNHead'),\n",
            "    data_preprocessor=dict(\n",
            "        format_shape='NCHW',\n",
            "        mean=[\n",
            "            0.485,\n",
            "            0.456,\n",
            "            0.406,\n",
            "        ],\n",
            "        std=[\n",
            "            0.229,\n",
            "            0.224,\n",
            "            0.225,\n",
            "        ],\n",
            "        type='ActionDataPreprocessor'),\n",
            "    type='Recognizer2D')\n",
            "optim_wrapper = dict(\n",
            "    clip_grad=dict(max_norm=40, norm_type=2),\n",
            "    optimizer=dict(\n",
            "        lr=0.00015625, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        begin=0,\n",
            "        by_epoch=True,\n",
            "        end=110,\n",
            "        gamma=0.1,\n",
            "        milestones=[\n",
            "            70,\n",
            "            100,\n",
            "        ],\n",
            "        type='MultiStepLR'),\n",
            "]\n",
            "resume = False\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/test.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='TenCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = [\n",
            "    dict(type='MySelfCustomMetric'),\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='TenCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "train_cfg = dict(\n",
            "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=5)\n",
            "train_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/train.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                num_clips=1,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(type='RandomResizedCrop'),\n",
            "            dict(keep_ratio=False, scale=(\n",
            "                64,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(flip_ratio=0.5, type='Flip'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, num_clips=1, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(type='RandomResizedCrop'),\n",
            "    dict(keep_ratio=False, scale=(\n",
            "        64,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(flip_ratio=0.5, type='Flip'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_dataloader = dict(\n",
            "    batch_size=16,\n",
            "    dataset=dict(\n",
            "        ann_file='./Annotations/val.txt',\n",
            "        data_prefix=dict(img='../group_project_data/output_yolo3'),\n",
            "        filename_tmpl='img_{:05d}.jpg',\n",
            "        modality='RGB',\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                clip_len=30,\n",
            "                frame_interval=1,\n",
            "                test_mode=True,\n",
            "                type='SampleFrames'),\n",
            "            dict(io_backend='disk', type='RawFrameDecode'),\n",
            "            dict(scale=(\n",
            "                -1,\n",
            "                64,\n",
            "            ), type='Resize'),\n",
            "            dict(crop_size=64, type='CenterCrop'),\n",
            "            dict(input_format='NCHW', type='FormatShape'),\n",
            "            dict(type='PackActionInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='RawframeDataset'),\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(type='AccMetric')\n",
            "val_pipeline = [\n",
            "    dict(clip_len=30, frame_interval=1, test_mode=True, type='SampleFrames'),\n",
            "    dict(io_backend='disk', type='RawFrameDecode'),\n",
            "    dict(scale=(\n",
            "        -1,\n",
            "        64,\n",
            "    ), type='Resize'),\n",
            "    dict(crop_size=64, type='CenterCrop'),\n",
            "    dict(input_format='NCHW', type='FormatShape'),\n",
            "    dict(type='PackActionInputs'),\n",
            "]\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    type='ActionVisualizer', vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "work_dir = './tutorial_exps'\n",
            "\n",
            "01/10 15:39:31 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/10 15:39:31 - mmengine - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) SyncBuffersHook                    \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "01/10 15:39:32 - mmengine - WARNING - Unsupported operator aten::add_ encountered 53 time(s)\n",
            "01/10 15:39:32 - mmengine - WARNING - Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "01/10 15:39:32 - mmengine - WARNING - Unsupported operator aten::add encountered 16 time(s)\n",
            "01/10 15:39:32 - mmengine - WARNING - The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "cls_head, cls_head.avg_pool, cls_head.consensus, cls_head.dropout, cls_head.fc_cls, cls_head.loss_cls, data_preprocessor\n",
            "01/10 15:39:33 - mmengine - WARNING - Unsupported operator aten::batch_norm encountered 53 time(s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import yaml\n",
        "from mmengine.analysis import get_model_complexity_info\n",
        "\n",
        "with open('output.yaml', 'w') as file:\n",
        "    yaml.dump(result_dict, file, default_flow_style=False)\n",
        "\n",
        "runner = Runner.from_cfg(cfg)\n",
        "input_shape = (1,3,64,64)\n",
        "analysis_results = get_model_complexity_info(runner.model, input_shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB155gYwye8P",
        "outputId": "17756351-8c23-444f-b5b9-19d47da1a90c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Flops:0.338G\n",
            "Model Parameters:23.512M\n"
          ]
        }
      ],
      "source": [
        "print(\"Model Flops:{}\".format(analysis_results['flops_str']))\n",
        "print(\"Model Parameters:{}\".format(analysis_results['params_str']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtiKeK0J3wpQ"
      },
      "source": [
        "## Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAGFELWG_w0v",
        "outputId": "615f02d0-d2b0-43cf-a8ad-f34280206557"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'OverSampling+Cropped': None,\n",
              " 'Cropped': [{'SelfCustomMetric/accuracy': 0.981651376146789,\n",
              "   'SelfCustomMetric/f1_score': 0.9743589743589743,\n",
              "   'SelfCustomMetric/recall': 0.95,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.   , 0.025, 1.   , 1.   ]),\n",
              "    array([1.9920324e+00, 9.9203241e-01, 4.4566023e-01, 8.2085689e-04],\n",
              "          dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 1.0,\n",
              "   'SelfCustomMetric/f1_score': 1.0,\n",
              "   'SelfCustomMetric/recall': 1.0,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.02564103, 1.        , 1.        ]),\n",
              "    array([1.9787033 , 0.9787033 , 0.7749559 , 0.00270668], dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 0.9928571428571429,\n",
              "   'SelfCustomMetric/f1_score': 0.9929078014184397,\n",
              "   'SelfCustomMetric/recall': 0.9859154929577465,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.01408451, 1.        , 1.        ]),\n",
              "    array([1.9988961e+00, 9.9889612e-01, 4.9098918e-01, 5.0193752e-04],\n",
              "          dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 0.9933774834437086,\n",
              "   'SelfCustomMetric/f1_score': 0.9600000000000001,\n",
              "   'SelfCustomMetric/recall': 0.9230769230769231,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.07692308, 1.        , 1.        ]),\n",
              "    array([1.9934627 , 0.9934627 , 0.49318725, 0.00323809], dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 0.9545454545454546,\n",
              "   'SelfCustomMetric/f1_score': 0.9500000000000001,\n",
              "   'SelfCustomMetric/recall': 0.9047619047619048,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.01587302, 1.        , 1.        ]),\n",
              "    array([1.9891509 , 0.9891509 , 0.35687304, 0.01818558], dtype=float32))}],\n",
              " 'Oversampling': [{'SelfCustomMetric/accuracy': 1.0,\n",
              "   'SelfCustomMetric/f1_score': 1.0,\n",
              "   'SelfCustomMetric/recall': 1.0,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.   , 0.025, 1.   , 1.   ]),\n",
              "    array([1.9978564e+00, 9.9785632e-01, 6.6413343e-01, 1.3815074e-03],\n",
              "          dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 0.9726027397260274,\n",
              "   'SelfCustomMetric/f1_score': 0.951219512195122,\n",
              "   'SelfCustomMetric/recall': 1.0,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.02564103, 1.        , 1.        ]),\n",
              "    array([1.9770706 , 0.97707057, 0.82472414, 0.00555627], dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 1.0,\n",
              "   'SelfCustomMetric/f1_score': 1.0,\n",
              "   'SelfCustomMetric/recall': 1.0,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.01408451, 1.        , 1.        ]),\n",
              "    array([1.9923368e+00, 9.9233681e-01, 5.5470127e-01, 9.5033651e-04],\n",
              "          dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 1.0,\n",
              "   'SelfCustomMetric/f1_score': 1.0,\n",
              "   'SelfCustomMetric/recall': 1.0,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.07692308, 1.        , 1.        ]),\n",
              "    array([1.9932655 , 0.9932655 , 0.5194988 , 0.00389518], dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 0.9318181818181818,\n",
              "   'SelfCustomMetric/f1_score': 0.923076923076923,\n",
              "   'SelfCustomMetric/recall': 0.8571428571428571,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.01587302, 1.        , 1.        ]),\n",
              "    array([1.9714692 , 0.97146916, 0.41007832, 0.02119327], dtype=float32))}],\n",
              " 'Baseline': [{'SelfCustomMetric/accuracy': 1.0,\n",
              "   'SelfCustomMetric/f1_score': 1.0,\n",
              "   'SelfCustomMetric/recall': 1.0,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.   , 0.025, 1.   , 1.   ]),\n",
              "    array([1.9978564e+00, 9.9785632e-01, 6.6413343e-01, 1.3815074e-03],\n",
              "          dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 0.9726027397260274,\n",
              "   'SelfCustomMetric/f1_score': 0.951219512195122,\n",
              "   'SelfCustomMetric/recall': 1.0,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.02564103, 1.        , 1.        ]),\n",
              "    array([1.9770706 , 0.97707057, 0.82472414, 0.00555627], dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 1.0,\n",
              "   'SelfCustomMetric/f1_score': 1.0,\n",
              "   'SelfCustomMetric/recall': 1.0,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.01408451, 1.        , 1.        ]),\n",
              "    array([1.9923368e+00, 9.9233681e-01, 5.5470127e-01, 9.5033651e-04],\n",
              "          dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 1.0,\n",
              "   'SelfCustomMetric/f1_score': 1.0,\n",
              "   'SelfCustomMetric/recall': 1.0,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.07692308, 1.        , 1.        ]),\n",
              "    array([1.9932655 , 0.9932655 , 0.5194988 , 0.00389518], dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 0.9318181818181818,\n",
              "   'SelfCustomMetric/f1_score': 0.923076923076923,\n",
              "   'SelfCustomMetric/recall': 0.8571428571428571,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.01587302, 1.        , 1.        ]),\n",
              "    array([1.9714692 , 0.97146916, 0.41007832, 0.02119327], dtype=float32))}],\n",
              " 'Oversampling+Cropped': [{'SelfCustomMetric/accuracy': 0.981651376146789,\n",
              "   'SelfCustomMetric/f1_score': 0.9743589743589743,\n",
              "   'SelfCustomMetric/recall': 0.95,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.   , 0.025, 1.   , 1.   ]),\n",
              "    array([1.9920324e+00, 9.9203241e-01, 4.4566023e-01, 8.2085689e-04],\n",
              "          dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 1.0,\n",
              "   'SelfCustomMetric/f1_score': 1.0,\n",
              "   'SelfCustomMetric/recall': 1.0,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.02564103, 1.        , 1.        ]),\n",
              "    array([1.9787033 , 0.9787033 , 0.7749559 , 0.00270668], dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 0.9928571428571429,\n",
              "   'SelfCustomMetric/f1_score': 0.9929078014184397,\n",
              "   'SelfCustomMetric/recall': 0.9859154929577465,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.01408451, 1.        , 1.        ]),\n",
              "    array([1.9988961e+00, 9.9889612e-01, 4.9098918e-01, 5.0193752e-04],\n",
              "          dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 0.9933774834437086,\n",
              "   'SelfCustomMetric/f1_score': 0.9600000000000001,\n",
              "   'SelfCustomMetric/recall': 0.9230769230769231,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.07692308, 1.        , 1.        ]),\n",
              "    array([1.9934627 , 0.9934627 , 0.49318725, 0.00323809], dtype=float32))},\n",
              "  {'SelfCustomMetric/accuracy': 0.9545454545454546,\n",
              "   'SelfCustomMetric/f1_score': 0.9500000000000001,\n",
              "   'SelfCustomMetric/recall': 0.9047619047619048,\n",
              "   'SelfCustomMetric/auc': 1.0,\n",
              "   'SelfCustomMetric/roc_curve': (array([0., 0., 0., 1.]),\n",
              "    array([0.        , 0.01587302, 1.        , 1.        ]),\n",
              "    array([1.9891509 , 0.9891509 , 0.35687304, 0.01818558], dtype=float32))}]}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "result_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Calculate the average for each metric\n",
        "results = {}\n",
        "\n",
        "for experiment, metrics in result_dict.items():\n",
        "    df = pd.DataFrame(metrics)\n",
        "    averages = df.mean()\n",
        "    variances = df.var()\n",
        "    results[experiment] = pd.concat([averages, variances], keys=['Average', 'Variance'])\n",
        "\n",
        "# Create a dataframe to display the results in tabular form\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.transpose()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "P66NPbp0ELL1",
        "outputId": "a40af4ba-d48e-4f1b-dba4-6e0fae70864a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-71e9807e2c47>:7: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  averages = df.mean()\n",
            "<ipython-input-23-71e9807e2c47>:8: FutureWarning: The default value of numeric_only in DataFrame.var is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  variances = df.var()\n",
            "<ipython-input-23-71e9807e2c47>:7: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  averages = df.mean()\n",
            "<ipython-input-23-71e9807e2c47>:8: FutureWarning: The default value of numeric_only in DataFrame.var is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  variances = df.var()\n",
            "<ipython-input-23-71e9807e2c47>:7: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  averages = df.mean()\n",
            "<ipython-input-23-71e9807e2c47>:8: FutureWarning: The default value of numeric_only in DataFrame.var is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  variances = df.var()\n",
            "<ipython-input-23-71e9807e2c47>:7: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  averages = df.mean()\n",
            "<ipython-input-23-71e9807e2c47>:8: FutureWarning: The default value of numeric_only in DataFrame.var is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  variances = df.var()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       Average                            \\\n",
              "                     SelfCustomMetric/accuracy SelfCustomMetric/f1_score   \n",
              "OverSampling+Cropped                       NaN                       NaN   \n",
              "Cropped                               0.984486                  0.975453   \n",
              "Oversampling                          0.980884                  0.974859   \n",
              "Baseline                              0.980884                  0.974859   \n",
              "Oversampling+Cropped                  0.984486                  0.975453   \n",
              "\n",
              "                                                                   \\\n",
              "                     SelfCustomMetric/recall SelfCustomMetric/auc   \n",
              "OverSampling+Cropped                     NaN                  NaN   \n",
              "Cropped                             0.952751                  1.0   \n",
              "Oversampling                        0.971429                  1.0   \n",
              "Baseline                            0.971429                  1.0   \n",
              "Oversampling+Cropped                0.952751                  1.0   \n",
              "\n",
              "                                      Variance                            \\\n",
              "                     SelfCustomMetric/accuracy SelfCustomMetric/f1_score   \n",
              "OverSampling+Cropped                       NaN                       NaN   \n",
              "Cropped                               0.000324                  0.000449   \n",
              "Oversampling                          0.000893                  0.001284   \n",
              "Baseline                              0.000893                  0.001284   \n",
              "Oversampling+Cropped                  0.000324                  0.000449   \n",
              "\n",
              "                                                                   \n",
              "                     SelfCustomMetric/recall SelfCustomMetric/auc  \n",
              "OverSampling+Cropped                     NaN                  NaN  \n",
              "Cropped                             0.001631                  0.0  \n",
              "Oversampling                        0.004082                  0.0  \n",
              "Baseline                            0.004082                  0.0  \n",
              "Oversampling+Cropped                0.001631                  0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8f0599e-327a-43e6-a48e-34b604b90eb5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"4\" halign=\"left\">Average</th>\n",
              "      <th colspan=\"4\" halign=\"left\">Variance</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>SelfCustomMetric/accuracy</th>\n",
              "      <th>SelfCustomMetric/f1_score</th>\n",
              "      <th>SelfCustomMetric/recall</th>\n",
              "      <th>SelfCustomMetric/auc</th>\n",
              "      <th>SelfCustomMetric/accuracy</th>\n",
              "      <th>SelfCustomMetric/f1_score</th>\n",
              "      <th>SelfCustomMetric/recall</th>\n",
              "      <th>SelfCustomMetric/auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>OverSampling+Cropped</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cropped</th>\n",
              "      <td>0.984486</td>\n",
              "      <td>0.975453</td>\n",
              "      <td>0.952751</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.001631</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Oversampling</th>\n",
              "      <td>0.980884</td>\n",
              "      <td>0.974859</td>\n",
              "      <td>0.971429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>0.001284</td>\n",
              "      <td>0.004082</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Baseline</th>\n",
              "      <td>0.980884</td>\n",
              "      <td>0.974859</td>\n",
              "      <td>0.971429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>0.001284</td>\n",
              "      <td>0.004082</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Oversampling+Cropped</th>\n",
              "      <td>0.984486</td>\n",
              "      <td>0.975453</td>\n",
              "      <td>0.952751</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.001631</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8f0599e-327a-43e6-a48e-34b604b90eb5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c8f0599e-327a-43e6-a48e-34b604b90eb5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c8f0599e-327a-43e6-a48e-34b604b90eb5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-31389d64-ad77-4c55-aecd-859f51352ec2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-31389d64-ad77-4c55-aecd-859f51352ec2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-31389d64-ad77-4c55-aecd-859f51352ec2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}